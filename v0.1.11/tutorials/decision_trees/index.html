<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Classification problems with DecisionTree.jl · MathOptAI.jl</title><meta name="title" content="Classification problems with DecisionTree.jl · MathOptAI.jl"/><meta property="og:title" content="Classification problems with DecisionTree.jl · MathOptAI.jl"/><meta property="twitter:title" content="Classification problems with DecisionTree.jl · MathOptAI.jl"/><meta name="description" content="Documentation for MathOptAI.jl."/><meta property="og:description" content="Documentation for MathOptAI.jl."/><meta property="twitter:description" content="Documentation for MathOptAI.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MathOptAI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">MathOptAI.jl</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox"/><label class="tocitem" for="menuitem-2"><span class="docs-label">Manual</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../manual/predictors/">Predictors</a></li><li><a class="tocitem" href="../../manual/AbstractGPs/">AbstractGPs.jl</a></li><li><a class="tocitem" href="../../manual/DecisionTree/">DecisionTree.jl</a></li><li><a class="tocitem" href="../../manual/EvoTrees/">EvoTrees.jl</a></li><li><a class="tocitem" href="../../manual/Flux/">Flux.jl</a></li><li><a class="tocitem" href="../../manual/GLM/">GLM.jl</a></li><li><a class="tocitem" href="../../manual/Lux/">Lux.jl</a></li><li><a class="tocitem" href="../../manual/PyTorch/">PyTorch</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox" checked/><label class="tocitem" for="menuitem-3"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../student_enrollment/">Logistic regression with GLM.jl</a></li><li class="is-active"><a class="tocitem" href>Classification problems with DecisionTree.jl</a><ul class="internal"><li><a class="tocitem" href="#Required-packages"><span>Required packages</span></a></li><li><a class="tocitem" href="#Data"><span>Data</span></a></li><li><a class="tocitem" href="#Prediction-model"><span>Prediction model</span></a></li><li><a class="tocitem" href="#Decision-model"><span>Decision model</span></a></li><li><a class="tocitem" href="#Solution-analysis"><span>Solution analysis</span></a></li></ul></li><li><a class="tocitem" href="../mnist/">Adversarial machine learning with Flux.jl</a></li><li><a class="tocitem" href="../mnist_lux/">Adversarial machine learning with Lux.jl</a></li><li><a class="tocitem" href="../pytorch/">Function fitting with PyTorch</a></li><li><a class="tocitem" href="../gaussian/">Function fitting with AbstractGPs</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Developers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../developers/design_principles/">Design principles</a></li></ul></li><li><a class="tocitem" href="../../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Classification problems with DecisionTree.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Classification problems with DecisionTree.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/lanl-ansi/MathOptAI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/lanl-ansi/MathOptAI.jl/blob/main/docs/src/tutorials/decision_trees.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Classification-problems-with-DecisionTree.jl"><a class="docs-heading-anchor" href="#Classification-problems-with-DecisionTree.jl">Classification problems with DecisionTree.jl</a><a id="Classification-problems-with-DecisionTree.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Classification-problems-with-DecisionTree.jl" title="Permalink"></a></h1><p>The purpose of this tutorial is to explain how to embed a decision tree model from <a href="https://github.com/JuliaAI/DecisionTree.jl">DecisionTree.jl</a> into JuMP.</p><p>The data and example in this tutorial comes from the paper: David Bergman, Teng Huang, Philip Brooks, Andrea Lodi, Arvind U. Raghunathan (2021) JANOS: An Integrated Predictive and Prescriptive Modeling Framework. INFORMS Journal on Computing 34(2):807-816. <a href="https://doi.org/10.1287/ijoc.2020.1023">https://doi.org/10.1287/ijoc.2020.1023</a></p><h2 id="Required-packages"><a class="docs-heading-anchor" href="#Required-packages">Required packages</a><a id="Required-packages-1"></a><a class="docs-heading-anchor-permalink" href="#Required-packages" title="Permalink"></a></h2><p>This tutorial uses the following packages.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using JuMP</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import CSV</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import DataFrames</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import Downloads</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import DecisionTree</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import HiGHS</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import MathOptAI</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import Statistics</code></pre><h2 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h2><p>Here is a function to load the data directly from the JANOS repository:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function read_df(filename)
           url = &quot;https://raw.githubusercontent.com/INFORMSJoC/2020.1023/master/data/&quot;
           data = Downloads.download(url * filename)
           return CSV.read(data, DataFrames.DataFrame)
       end</code><code class="nohighlight hljs ansi" style="display:block;">read_df (generic function with 1 method)</code></pre><p>There are two important files. The first, <code>college_student_enroll-s1-1.csv</code>, contains historical admissions data on anonymized students, their SAT score, their GPA, their merit scholarships, and whether the enrolled in the college.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; train_df = read_df(&quot;college_student_enroll-s1-1.csv&quot;)</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">20000×6 DataFrame</span>
<span class="sgr1">   Row </span>│<span class="sgr1"> Column1  StudentID  SAT    GPA      merit    enroll </span>
       │<span class="sgr90"> Int64    Int64      Int64  Float64  Float64  Int64  </span>
───────┼─────────────────────────────────────────────────────
     1 │       1          1   1507     3.72     1.64       0
     2 │       2          2   1532     3.93     0.52       0
     3 │       3          3   1487     3.77     1.67       0
     4 │       4          4   1259     3.05     1.21       1
     5 │       5          5   1354     3.39     1.65       1
     6 │       6          6   1334     3.22     0.0        0
     7 │       7          7   1125     2.73     1.68       1
     8 │       8          8   1180     2.82     0.0        1
   ⋮   │    ⋮         ⋮        ⋮       ⋮        ⋮       ⋮
 19994 │   19994      19994   1185     3.09     1.16       1
 19995 │   19995      19995   1471     3.7      1.05       0
 19996 │   19996      19996   1139     3.03     1.21       1
 19997 │   19997      19997   1371     3.39     1.26       0
 19998 │   19998      19998   1424     3.72     0.85       0
 19999 │   19999      19999   1170     3.01     0.73       1
 20000 │   20000      20000   1389     3.57     0.55       0
<span class="sgr36">                                           19985 rows omitted</span></code></pre><p>The second, <code>college_applications6000.csv</code>, contains the SAT and GPA data of students who are currently applying:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df = read_df(&quot;college_applications6000.csv&quot;)</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">6000×3 DataFrame</span>
<span class="sgr1">  Row </span>│<span class="sgr1"> StudentID  SAT    GPA     </span>
      │<span class="sgr90"> Int64      Int64  Float64 </span>
──────┼───────────────────────────
    1 │         1   1240     3.22
    2 │         2   1206     2.87
    3 │         3   1520     3.74
    4 │         4   1238     3.11
    5 │         5   1142     2.74
    6 │         6   1086     2.77
    7 │         7   1367     3.28
    8 │         8   1034     2.41
  ⋮   │     ⋮        ⋮       ⋮
 5994 │      5994   1121     2.96
 5995 │      5995   1564     4.1
 5996 │      5996   1332     3.14
 5997 │      5997   1228     2.95
 5998 │      5998   1165     2.81
 5999 │      5999   1400     3.43
 6000 │      6000   1097     2.65
<span class="sgr36">                 5985 rows omitted</span></code></pre><p>There are 6,000 prospective students:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; n_students = size(evaluate_df, 1)</code><code class="nohighlight hljs ansi" style="display:block;">6000</code></pre><h2 id="Prediction-model"><a class="docs-heading-anchor" href="#Prediction-model">Prediction model</a><a id="Prediction-model-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction-model" title="Permalink"></a></h2><p>The first step is to train a logistic regression model to predict the Boolean <code>enroll</code> column based on the <code>SAT</code>, <code>GPA</code>, and <code>merit</code> columns.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; train_features = Matrix(train_df[:, [:SAT, :GPA, :merit]])</code><code class="nohighlight hljs ansi" style="display:block;">20000×3 Matrix{Float64}:
 1507.0  3.72  1.64
 1532.0  3.93  0.52
 1487.0  3.77  1.67
 1259.0  3.05  1.21
 1354.0  3.39  1.65
 1334.0  3.22  0.0
 1125.0  2.73  1.68
 1180.0  2.82  0.0
 1098.0  2.91  0.0
 1116.0  2.95  0.0
    ⋮
 1048.0  2.51  0.73
 1157.0  2.79  2.11
 1185.0  3.09  1.16
 1471.0  3.7   1.05
 1139.0  3.03  1.21
 1371.0  3.39  1.26
 1424.0  3.72  0.85
 1170.0  3.01  0.73
 1389.0  3.57  0.55</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; train_labels = train_df[:, :enroll]</code><code class="nohighlight hljs ansi" style="display:block;">20000-element Vector{Int64}:
 0
 0
 0
 1
 1
 0
 1
 1
 1
 1
 ⋮
 1
 1
 1
 0
 1
 0
 0
 1
 0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; predictor = DecisionTree.DecisionTreeClassifier(; max_depth = 3)</code><code class="nohighlight hljs ansi" style="display:block;">DecisionTreeClassifier
max_depth:                3
min_samples_leaf:         1
min_samples_split:        2
min_purity_increase:      0.0
pruning_purity_threshold: 1.0
n_subfeatures:            0
classes:                  nothing
root:                     nothing</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; DecisionTree.fit!(predictor, train_features, train_labels)</code><code class="nohighlight hljs ansi" style="display:block;">DecisionTreeClassifier
max_depth:                3
min_samples_leaf:         1
min_samples_split:        2
min_purity_increase:      0.0
pruning_purity_threshold: 1.0
n_subfeatures:            0
classes:                  [0, 1]
root:                     Decision Tree
Leaves: 8
Depth:  3</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; DecisionTree.print_tree(predictor)</code><code class="nohighlight hljs ansi" style="display:block;">Feature 1 &lt; 1228.0 ?
├─ Feature 2 &lt; 3.125 ?
    ├─ Feature 1 &lt; 1214.0 ?
        ├─ 1 : 7336/7336
        └─ 1 : 334/361
    └─ Feature 3 &lt; 0.255 ?
        ├─ 0 : 161/220
        └─ 1 : 121/121
└─ Feature 1 &lt; 1412.0 ?
    ├─ Feature 3 &lt; 1.005 ?
        ├─ 0 : 3600/4046
        └─ 1 : 1603/2338
    └─ Feature 3 &lt; 1.865 ?
        ├─ 0 : 4530/4530
        └─ 0 : 947/1048</code></pre><h2 id="Decision-model"><a class="docs-heading-anchor" href="#Decision-model">Decision model</a><a id="Decision-model-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-model" title="Permalink"></a></h2><p>Now that we have a trained decision tree, we want a decision model that chooses the optimal merit scholarship for each student in:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">6000×3 DataFrame</span>
<span class="sgr1">  Row </span>│<span class="sgr1"> StudentID  SAT    GPA     </span>
      │<span class="sgr90"> Int64      Int64  Float64 </span>
──────┼───────────────────────────
    1 │         1   1240     3.22
    2 │         2   1206     2.87
    3 │         3   1520     3.74
    4 │         4   1238     3.11
    5 │         5   1142     2.74
    6 │         6   1086     2.77
    7 │         7   1367     3.28
    8 │         8   1034     2.41
  ⋮   │     ⋮        ⋮       ⋮
 5994 │      5994   1121     2.96
 5995 │      5995   1564     4.1
 5996 │      5996   1332     3.14
 5997 │      5997   1228     2.95
 5998 │      5998   1165     2.81
 5999 │      5999   1400     3.43
 6000 │      6000   1097     2.65
<span class="sgr36">                 5985 rows omitted</span></code></pre><p>Here&#39;s an empty JuMP model to start:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; model = Model()</code><code class="nohighlight hljs ansi" style="display:block;">A JuMP Model
├ solver: none
├ objective_sense: FEASIBILITY_SENSE
├ num_variables: 0
├ num_constraints: 0
└ Names registered in the model: none</code></pre><p>First, we add a new column to <code>evaluate_df</code>, with one JuMP decision variable for each row. It is important the <code>.merit</code> column name in <code>evaluate_df</code> matches the name in <code>train_df</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df.merit = @variable(model, 0 &lt;= x_merit[1:n_students] &lt;= 2.5);</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">6000×4 DataFrame</span>
<span class="sgr1">  Row </span>│<span class="sgr1"> StudentID  SAT    GPA      merit         </span>
      │<span class="sgr90"> Int64      Int64  Float64  GenericV…     </span>
──────┼──────────────────────────────────────────
    1 │         1   1240     3.22  x_merit[1]
    2 │         2   1206     2.87  x_merit[2]
    3 │         3   1520     3.74  x_merit[3]
    4 │         4   1238     3.11  x_merit[4]
    5 │         5   1142     2.74  x_merit[5]
    6 │         6   1086     2.77  x_merit[6]
    7 │         7   1367     3.28  x_merit[7]
    8 │         8   1034     2.41  x_merit[8]
  ⋮   │     ⋮        ⋮       ⋮           ⋮
 5994 │      5994   1121     2.96  x_merit[5994]
 5995 │      5995   1564     4.1   x_merit[5995]
 5996 │      5996   1332     3.14  x_merit[5996]
 5997 │      5997   1228     2.95  x_merit[5997]
 5998 │      5998   1165     2.81  x_merit[5998]
 5999 │      5999   1400     3.43  x_merit[5999]
 6000 │      6000   1097     2.65  x_merit[6000]
<span class="sgr36">                                5985 rows omitted</span></code></pre><p>Then, we use <a href="../../api/#MathOptAI.add_predictor"><code>MathOptAI.add_predictor</code></a> to embed <code>model_ml</code> into the JuMP <code>model</code>. <a href="../../api/#MathOptAI.add_predictor"><code>MathOptAI.add_predictor</code></a> returns a vector of variables, one for each row in <code>evaluate_df</code>, corresponding to the output <code>enroll</code> of our logistic regression.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_features = Matrix(evaluate_df[:, [:SAT, :GPA, :merit]])</code><code class="nohighlight hljs ansi" style="display:block;">6000×3 Matrix{JuMP.AffExpr}:
 1240  3.22  x_merit[1]
 1206  2.87  x_merit[2]
 1520  3.74  x_merit[3]
 1238  3.11  x_merit[4]
 1142  2.74  x_merit[5]
 1086  2.77  x_merit[6]
 1367  3.28  x_merit[7]
 1034  2.41  x_merit[8]
 1547  3.82  x_merit[9]
 1453  3.65  x_merit[10]
 ⋮
 1299  3.16  x_merit[5992]
 1400  3.59  x_merit[5993]
 1121  2.96  x_merit[5994]
 1564  4.1   x_merit[5995]
 1332  3.14  x_merit[5996]
 1228  2.95  x_merit[5997]
 1165  2.81  x_merit[5998]
 1400  3.43  x_merit[5999]
 1097  2.65  x_merit[6000]</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df.enroll = mapreduce(vcat, 1:size(evaluate_features, 1)) do i
           y, _ = MathOptAI.add_predictor(model, predictor, evaluate_features[i, :])
           return y
       end</code><code class="nohighlight hljs ansi" style="display:block;">6000-element Vector{JuMP.VariableRef}:
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 ⋮
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value
 moai_BinaryDecisionTree_value</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">6000×5 DataFrame</span>
<span class="sgr1">  Row </span>│<span class="sgr1"> StudentID  SAT    GPA      merit          enroll                      </span> ⋯
      │<span class="sgr90"> Int64      Int64  Float64  GenericV…      GenericV…                   </span> ⋯
──────┼─────────────────────────────────────────────────────────────────────────
    1 │         1   1240     3.22  x_merit[1]     moai_BinaryDecisionTree_valu ⋯
    2 │         2   1206     2.87  x_merit[2]     moai_BinaryDecisionTree_valu
    3 │         3   1520     3.74  x_merit[3]     moai_BinaryDecisionTree_valu
    4 │         4   1238     3.11  x_merit[4]     moai_BinaryDecisionTree_valu
    5 │         5   1142     2.74  x_merit[5]     moai_BinaryDecisionTree_valu ⋯
    6 │         6   1086     2.77  x_merit[6]     moai_BinaryDecisionTree_valu
    7 │         7   1367     3.28  x_merit[7]     moai_BinaryDecisionTree_valu
    8 │         8   1034     2.41  x_merit[8]     moai_BinaryDecisionTree_valu
  ⋮   │     ⋮        ⋮       ⋮           ⋮                      ⋮              ⋱
 5994 │      5994   1121     2.96  x_merit[5994]  moai_BinaryDecisionTree_valu ⋯
 5995 │      5995   1564     4.1   x_merit[5995]  moai_BinaryDecisionTree_valu
 5996 │      5996   1332     3.14  x_merit[5996]  moai_BinaryDecisionTree_valu
 5997 │      5997   1228     2.95  x_merit[5997]  moai_BinaryDecisionTree_valu
 5998 │      5998   1165     2.81  x_merit[5998]  moai_BinaryDecisionTree_valu ⋯
 5999 │      5999   1400     3.43  x_merit[5999]  moai_BinaryDecisionTree_valu
 6000 │      6000   1097     2.65  x_merit[6000]  moai_BinaryDecisionTree_valu
<span class="sgr36">                                                  1 column and 5985 rows omitted</span></code></pre><p>The <code>.enroll</code> column name in <code>evaluate_df</code> is just a name. It doesn&#39;t have to match the name in <code>train_df</code>.</p><p>The objective of our problem is to maximize the expected number of students who enroll:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; @objective(model, Max, sum(evaluate_df.enroll))</code><code class="nohighlight hljs ansi" style="display:block;">moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + [[...5940 terms omitted...]] + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value + moai_BinaryDecisionTree_value</code></pre><p>Subject to the constraint that we can spend at most <code>0.2 * n_students</code> on merit scholarships:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; @constraint(model, sum(evaluate_df.merit) &lt;= 0.2 * n_students)</code><code class="nohighlight hljs ansi" style="display:block;">x_merit[1] + x_merit[2] + x_merit[3] + x_merit[4] + x_merit[5] + x_merit[6] + x_merit[7] + x_merit[8] + x_merit[9] + x_merit[10] + x_merit[11] + x_merit[12] + x_merit[13] + x_merit[14] + x_merit[15] + x_merit[16] + x_merit[17] + x_merit[18] + x_merit[19] + x_merit[20] + x_merit[21] + x_merit[22] + x_merit[23] + x_merit[24] + x_merit[25] + x_merit[26] + x_merit[27] + x_merit[28] + x_merit[29] + x_merit[30] + [[...5940 terms omitted...]] + x_merit[5971] + x_merit[5972] + x_merit[5973] + x_merit[5974] + x_merit[5975] + x_merit[5976] + x_merit[5977] + x_merit[5978] + x_merit[5979] + x_merit[5980] + x_merit[5981] + x_merit[5982] + x_merit[5983] + x_merit[5984] + x_merit[5985] + x_merit[5986] + x_merit[5987] + x_merit[5988] + x_merit[5989] + x_merit[5990] + x_merit[5991] + x_merit[5992] + x_merit[5993] + x_merit[5994] + x_merit[5995] + x_merit[5996] + x_merit[5997] + x_merit[5998] + x_merit[5999] + x_merit[6000] ≤ 1200</code></pre><p>Because logistic regression involves a <a href="../../api/#Sigmoid"><code>Sigmoid</code></a> layer, we need to use a smooth nonlinear optimizer. A common choice is Ipopt. Solve and check the optimizer found a feasible solution:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; set_optimizer(model, HiGHS.Optimizer)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; set_silent(model)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; optimize!(model)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; @assert is_solved_and_feasible(model)</code></pre><p>Let&#39;s store the solution in <code>evaluate_df</code> for analysis:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df.merit_sol = value.(evaluate_df.merit);</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df.enroll_sol = value.(evaluate_df.enroll);</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; evaluate_df</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">6000×7 DataFrame</span>
<span class="sgr1">  Row </span>│<span class="sgr1"> StudentID  SAT    GPA      merit          enroll                      </span> ⋯
      │<span class="sgr90"> Int64      Int64  Float64  GenericV…      GenericV…                   </span> ⋯
──────┼─────────────────────────────────────────────────────────────────────────
    1 │         1   1240     3.22  x_merit[1]     moai_BinaryDecisionTree_valu ⋯
    2 │         2   1206     2.87  x_merit[2]     moai_BinaryDecisionTree_valu
    3 │         3   1520     3.74  x_merit[3]     moai_BinaryDecisionTree_valu
    4 │         4   1238     3.11  x_merit[4]     moai_BinaryDecisionTree_valu
    5 │         5   1142     2.74  x_merit[5]     moai_BinaryDecisionTree_valu ⋯
    6 │         6   1086     2.77  x_merit[6]     moai_BinaryDecisionTree_valu
    7 │         7   1367     3.28  x_merit[7]     moai_BinaryDecisionTree_valu
    8 │         8   1034     2.41  x_merit[8]     moai_BinaryDecisionTree_valu
  ⋮   │     ⋮        ⋮       ⋮           ⋮                      ⋮              ⋱
 5994 │      5994   1121     2.96  x_merit[5994]  moai_BinaryDecisionTree_valu ⋯
 5995 │      5995   1564     4.1   x_merit[5995]  moai_BinaryDecisionTree_valu
 5996 │      5996   1332     3.14  x_merit[5996]  moai_BinaryDecisionTree_valu
 5997 │      5997   1228     2.95  x_merit[5997]  moai_BinaryDecisionTree_valu
 5998 │      5998   1165     2.81  x_merit[5998]  moai_BinaryDecisionTree_valu ⋯
 5999 │      5999   1400     3.43  x_merit[5999]  moai_BinaryDecisionTree_valu
 6000 │      6000   1097     2.65  x_merit[6000]  moai_BinaryDecisionTree_valu
<span class="sgr36">                                                 3 columns and 5985 rows omitted</span></code></pre><h2 id="Solution-analysis"><a class="docs-heading-anchor" href="#Solution-analysis">Solution analysis</a><a id="Solution-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Solution-analysis" title="Permalink"></a></h2><p>We expect that just under 2,500 students will enroll:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; sum(evaluate_df.enroll_sol)</code><code class="nohighlight hljs ansi" style="display:block;">3530.0</code></pre><p>We awarded merit scholarships to approximately 1 in 6 students:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; count(evaluate_df.merit_sol .&gt; 1e-5)</code><code class="nohighlight hljs ansi" style="display:block;">1278</code></pre><p>The average merit scholarship was worth just under <span>$</span>1,000:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; 1_000 * Statistics.mean(evaluate_df.merit_sol[evaluate_df.merit_sol.&gt;1e-5])</code><code class="nohighlight hljs ansi" style="display:block;">938.6854460093895</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../student_enrollment/">« Logistic regression with GLM.jl</a><a class="docs-footer-nextpage" href="../mnist/">Adversarial machine learning with Flux.jl »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Monday 2 June 2025 03:31">Monday 2 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
