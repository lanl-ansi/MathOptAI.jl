var documenterSearchIndex = {"docs":
[{"location":"manual/Flux/#Flux.jl","page":"Flux.jl","title":"Flux.jl","text":"Flux.jl is a library for machine learning in Julia.\n\nThe upstream documentation is available at https://fluxml.ai/Flux.jl/stable/.","category":"section"},{"location":"manual/Flux/#Supported-layers","page":"Flux.jl","title":"Supported layers","text":"MathOptAI supports embedding a Flux model into JuMP if it is a Flux.Chain composed of:\n\nFlux.Conv\nFlux.Dense\nFlux.flatten\nFlux.LayerNorm\nFlux.MaxPool\nFlux.MeanPool\nFlux.Scale\nFlux.relu\nFlux.sigmoid\nFlux.softmax\nFlux.softplus\nFlux.tanh","category":"section"},{"location":"manual/Flux/#Basic-example","page":"Flux.jl","title":"Basic example","text":"Use MathOptAI.add_predictor to embed a Flux.Chain into a JuMP model:\n\nusing JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/Flux/#Reduced-space","page":"Flux.jl","title":"Reduced-space","text":"Use the reduced_space = true keyword to formulate a reduced-space model:\n\nusing JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; reduced_space = true);\ny\nformulation","category":"section"},{"location":"manual/Flux/#Gray-box","page":"Flux.jl","title":"Gray-box","text":"Use the gray_box = true keyword to embed the network as a vector nonlinear operator:\n\nusing JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; gray_box = true);\ny\nformulation","category":"section"},{"location":"manual/Flux/#Change-how-layers-are-formulated","page":"Flux.jl","title":"Change how layers are formulated","text":"Pass a dictionary to the config keyword that maps Flux activation functions to a MathOptAI predictor:\n\nusing JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    config = Dict(Flux.relu => MathOptAI.ReLUSOS1),\n);\ny\nformulation","category":"section"},{"location":"manual/Flux/#Custom-layers","page":"Flux.jl","title":"Custom layers","text":"If your Flux model contains a custom layer, define new methods for build_predictor and add_predictor:\n\nusing JuMP, Flux, MathOptAI\nstruct CustomLayer{T<:Flux.Chain}\n  chain::T\nend\n(model::CustomLayer)(x) = model.chain(x) + x\nstruct CustomPredictor <: MathOptAI.AbstractPredictor\n    p::MathOptAI.Pipeline\nend\nfunction MathOptAI.build_predictor(model::CustomLayer)\n    predictor = MathOptAI.build_predictor(model.chain)\n    return CustomPredictor(predictor)\nend\nfunction MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::CustomPredictor,\n    x::Vector;\n    kwargs...,\n)\n    y, formulation = MathOptAI.add_predictor(model, predictor.p, x; kwargs...)\n    @assert length(x) == length(y)\n    return y .+ x, formulation\nend\nmodel = Model();\n@variable(model, x[i in 1:3]);\npredictor =\n    Flux.Chain(CustomLayer(Flux.Chain(Flux.Dense(3 => 3, Flux.relu))))\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/PythonCall/#Python-integration","page":"Python integration","title":"Python integration","text":"MathOptAI uses PythonCall.jl to call from Julia into Python.\n\nTo use PytorchModel your code must load the PythonCall package:\n\nimport PythonCall\n\nPythonCall uses CondaPkg.jl to manage Python dependencies. See CondaPkg.jl for more control over how to link Julia to an existing Python environment.\n\nIf you have an existing Python installation (with PyTorch installed), and it is available in the current Conda environment, do:\n\nENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Current\"\nimport PythonCall\n\nIf the Python installation can be found on the path and it is not in a Conda environment, do:\n\nENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Null\"\nimport PythonCall\n\nIf python is not on your path, you may additionally need to set JULIA_PYTHONCALL_EXE, for example, do:\n\nENV[\"JULIA_PYTHONCALL_EXE\"] = \"python3\"\nENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Null\"\nimport PythonCall","category":"section"},{"location":"tutorials/graph_neural_networks/#Graph-neural-networks","page":"Graph neural networks","title":"Graph neural networks","text":"The purpose of this tutorial is to explain how to embed graph neural network models from PyTorch Geometric into JuMP.\n\ninfo: Info\nTo use PyTorch from MathOptAI, you must first follow the Python integration instructions.","category":"section"},{"location":"tutorials/graph_neural_networks/#Required-packages","page":"Graph neural networks","title":"Required packages","text":"This tutorial requires the following packages\n\nusing JuMP\nusing Test\nimport Ipopt\nimport MathOptAI\nimport PythonCall","category":"section"},{"location":"tutorials/graph_neural_networks/#Setting-up-the-Python-side","page":"Graph neural networks","title":"Setting up the Python side","text":"We have written a custom PyTorch module, which is stored in the file my_gnn.py:\n\nprint(read(joinpath(@__DIR__, \"my_gnn.py\"), String))\n\nTo make it easy to load this Python file into the documentation, we add the current directory to the Python path:\n\ndir = @__DIR__\nPythonCall.@pyexec(dir => \"import sys; sys.path.insert(0, dir)\")\n\nThen, we can import torch and the my_gnn file into Julia:\n\nmy_gnn = PythonCall.pyimport(\"my_gnn\");\nnothing #hide\n\nNow, we can load our GNN into Julia:\n\npredictor = my_gnn.MyGNN()\n\nIn practice, you should train the GNN in Python, and write it to a file using torch.save; see the Function fitting with PyTorch for details. You could then replace the predictor with an appropriate PytorchModel.","category":"section"},{"location":"tutorials/graph_neural_networks/#Creating-the-graph","page":"Graph neural networks","title":"Creating the graph","text":"Before we can embed predictor into a JuMP model, we need to specialize it to a particular graph structure, and we need to provide a callback function that MathOptAI can use to turn an instance of MyGNN into an AbstractPredictor.\n\nFirst, we need the graph structure. Rather than providing a 2-by-n tensor, we provide the edges as a list of i => j pairs:\n\nedge_index = [1 => 2, 2 => 1, 2 => 3, 3 => 2, 3 => 4, 4 => 3]\n\nNote that the nodes are 1-indexed.\n\nThe callback takes in a PythonCall.Py layer, and returns an AbstractPredictor that matches the forward pass of the GNN:\n\nfunction MyGNN_callback(layer::PythonCall.Py; kwargs...)\n    return MathOptAI.Pipeline(\n        MathOptAI.GCNConv(layer.conv; edge_index),\n        MathOptAI.Sigmoid(),\n    )\nend\n\nNote how the callback uses edge_index.","category":"section"},{"location":"tutorials/graph_neural_networks/#Creating-the-JuMP-model","page":"Graph neural networks","title":"Creating the JuMP model","text":"Now we can embed predictor into a JuMP model.\n\nmodel = Model(Ipopt.Optimizer)\nset_silent(model)\n\nOur input x has four rows, one for each node, and two columns, one for each input attribute. The fixed bounds are so we can easily compare solutions between Python and Julia; replace them with your real constraints in practice.\n\n@variable(model, x[i in 1:4, j in 1:2] == sin(i) + cos(j))\n\nThen, we add predictor with the config argument mapping gnn.MyGNN (the Python class) to our callback MyGNN_callback:\n\ny, _ = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    config = Dict(my_gnn.MyGNN => MyGNN_callback),\n);\nnothing #hide\n\nNow we can solve the model:\n\noptimize!(model)\nassert_is_solved_and_feasible(model)\n\nand look at the solution of y:\n\nY = reshape(value(y), 4, 3)\n\nWhich is identical to what we obtain when we evaluate the GNN in Python:\n\ntorch = PythonCall.pyimport(\"torch\")\npy_x = torch.tensor([value.(x[i, :]) for i in 1:4])\n# `py_edge_index` is the same graph as `edge_index`, just in a different form.\n# Note also that this is 0-indexed.\npy_edge_index = torch.tensor([[0, 1, 1, 2, 2, 3], [1, 0, 2, 1, 3, 2]])\npredictor(py_x, py_edge_index)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorials/mnist/#Adversarial-machine-learning-with-Flux.jl","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"The purpose of this tutorial is to explain how to embed a neural network model from Flux.jl into JuMP.","category":"section"},{"location":"tutorials/mnist/#Required-packages","page":"Adversarial machine learning with Flux.jl","title":"Required packages","text":"This tutorial requires the following packages\n\nusing JuMP\nimport Flux\nimport Ipopt\nimport MathOptAI\nimport MLDatasets\nimport Plots","category":"section"},{"location":"tutorials/mnist/#Data","page":"Adversarial machine learning with Flux.jl","title":"Data","text":"This tutorial uses images from the MNIST dataset.\n\nWe load the predefined train and test splits:\n\ntrain_data = MLDatasets.MNIST(; split = :train)\n\ntest_data = MLDatasets.MNIST(; split = :test)\n\nSince the data are images, it is helpful to plot them. (This requires a transpose and reversing the rows to get the orientation correct.)\n\nfunction plot_image(x::Matrix; kwargs...)\n    return Plots.heatmap(\n        x'[size(x, 1):-1:1, :];\n        xlims = (1, size(x, 2)),\n        ylims = (1, size(x, 1)),\n        aspect_ratio = true,\n        legend = false,\n        xaxis = false,\n        yaxis = false,\n        kwargs...,\n    )\nend\n\nfunction plot_image(instance::NamedTuple)\n    return plot_image(instance.features; title = \"Label = $(instance.targets)\")\nend\n\nPlots.plot([plot_image(train_data[i]) for i in 1:6]...; layout = (2, 3))","category":"section"},{"location":"tutorials/mnist/#Training","page":"Adversarial machine learning with Flux.jl","title":"Training","text":"We use a simple neural network with one hidden layer and a sigmoid activation function. (There are better performing networks; try experimenting.)\n\npredictor = Flux.Chain(\n    Flux.Dense(28^2 => 32, Flux.sigmoid),\n    Flux.Dense(32 => 10),\n    Flux.softmax,\n)\n\nHere is a function to load our data into the format that predictor expects:\n\nfunction data_loader(data; batchsize, shuffle = false)\n    x = reshape(data.features, 28^2, :)\n    y = Flux.onehotbatch(data.targets, 0:9)\n    return Flux.DataLoader((x, y); batchsize, shuffle)\nend\n\nand here is a function to score the percentage of correct labels, where we assign a label by choosing the label of the highest softmax in the final layer.\n\nfunction score_model(predictor, data)\n    x, y = only(data_loader(data; batchsize = length(data)))\n    y_hat = predictor(x)\n    is_correct = Flux.onecold(y) .== Flux.onecold(y_hat)\n    p = round(100 * sum(is_correct) / length(is_correct); digits = 2)\n    println(\"Accuracy = $p %\")\n    return\nend\n\nThe accuracy of our model is only around 10% before training:\n\nscore_model(predictor, train_data)\nscore_model(predictor, test_data)\n\nLet's improve that by training our model.\n\nnote: Note\nIt is not the purpose of this tutorial to explain how Flux works; see the documentation at https://fluxml.ai for more details. Changing the number of epochs or the learning rate can improve the loss.\n\nbegin\n    train_loader = data_loader(train_data; batchsize = 256, shuffle = true)\n    optimizer_state = Flux.setup(Flux.Adam(3e-4), predictor)\n    for epoch in 1:30\n        loss = 0.0\n        for (x, y) in train_loader\n            loss_batch, gradient = Flux.withgradient(predictor) do model\n                return Flux.crossentropy(model(x), y)\n            end\n            Flux.update!(optimizer_state, predictor, only(gradient))\n            loss += loss_batch\n        end\n        loss = round(loss / length(train_loader); digits = 4)\n        print(\"Epoch $epoch: loss = $loss\\t\")\n        score_model(predictor, test_data)\n    end\nend\n\nHere are the first eight predictions of the test data:\n\nfunction plot_image(predictor, x::Matrix)\n    score, index = findmax(predictor(vec(x)))\n    title = \"Predicted: $(index - 1) ($(round(Int, 100 * score))%)\"\n    return plot_image(x; title)\nend\n\nplots = [plot_image(predictor, test_data[i].features) for i in 1:8]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))\n\nWe can also look at the best and worst four predictions:\n\nx, y = only(data_loader(test_data; batchsize = length(test_data)))\nlosses = Flux.crossentropy(predictor(x), y; agg = identity)\nindices = sortperm(losses; dims = 2)[[1:4; (end-3):end]]\nplots = [plot_image(predictor, test_data[i].features) for i in indices]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))\n\nThere are still some fairly bad mistakes. Can you change the model or training parameters improve to improve things?","category":"section"},{"location":"tutorials/mnist/#JuMP","page":"Adversarial machine learning with Flux.jl","title":"JuMP","text":"Now that we have a trained machine learning model, we can embed it in a JuMP model.\n\nHere's a function which takes a test case and returns an example that maximizes the probability of the adversarial example.\n\nfunction find_adversarial_image(test_case; adversary_label, δ = 0.05)\n    model = Model(Ipopt.Optimizer)\n    set_silent(model)\n    @variable(model, 0 <= x[1:28, 1:28] <= 1)\n    @constraint(model, -δ .<= x .- test_case.features .<= δ)\n    # Note: we need to use `vec` here because `x` is a 28-by-28 Matrix, but our\n    # neural network expects a 28^2 length vector.\n    y, _ = MathOptAI.add_predictor(model, predictor, vec(x))\n    @objective(model, Max, y[adversary_label+1] - y[test_case.targets+1])\n    optimize!(model)\n    @assert is_solved_and_feasible(model)\n    return value.(x)\nend\n\nLet's try finding an adversarial example to the third test image. The image on the left is our input image. The network thinks this is a 1 with probability 99%. The image on the right is the adversarial image. The network thinks this is a 7, although it is less confident.\n\nx_adversary = find_adversarial_image(test_data[3]; adversary_label = 7);\nPlots.plot(\n    plot_image(predictor, test_data[3].features),\n    plot_image(predictor, Float32.(x_adversary)),\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorials/student_enrollment/#Logistic-regression-with-GLM.jl","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The purpose of this tutorial is to explain how to embed a logistic regression model from GLM.jl into JuMP.\n\nThe data and example in this tutorial comes from the paper: David Bergman, Teng Huang, Philip Brooks, Andrea Lodi, Arvind U. Raghunathan (2021) JANOS: An Integrated Predictive and Prescriptive Modeling Framework. INFORMS Journal on Computing 34(2):807-816. https://doi.org/10.1287/ijoc.2020.1023","category":"section"},{"location":"tutorials/student_enrollment/#Required-packages","page":"Logistic regression with GLM.jl","title":"Required packages","text":"This tutorial uses the following packages.\n\nusing JuMP\nimport CSV\nimport DataFrames\nimport Downloads\nimport GLM\nimport Ipopt\nimport MathOptAI\nimport Statistics","category":"section"},{"location":"tutorials/student_enrollment/#Data","page":"Logistic regression with GLM.jl","title":"Data","text":"Here is a function to load the data directly from the JANOS repository:\n\nfunction read_df(filename)\n    url = \"https://raw.githubusercontent.com/INFORMSJoC/2020.1023/master/data/\"\n    data = Downloads.download(url * filename)\n    return CSV.read(data, DataFrames.DataFrame)\nend\n\nThere are two important files. The first, college_student_enroll-s1-1.csv, contains historical admissions data on anonymized students, their SAT score, their GPA, their merit scholarships, and whether the enrolled in the college.\n\ntrain_df = read_df(\"college_student_enroll-s1-1.csv\")\n\nThe second, college_applications6000.csv, contains the SAT and GPA data of students who are currently applying:\n\nevaluate_df = read_df(\"college_applications6000.csv\")\n\nThere are 6,000 prospective students:\n\nn_students = size(evaluate_df, 1)","category":"section"},{"location":"tutorials/student_enrollment/#Prediction-model","page":"Logistic regression with GLM.jl","title":"Prediction model","text":"The first step is to train a logistic regression model to predict the Boolean enroll column based on the SAT, GPA, and merit columns.\n\npredictor = GLM.glm(\n    GLM.@formula(enroll ~ 0 + SAT + GPA + merit),\n    train_df,\n    GLM.Bernoulli(),\n)","category":"section"},{"location":"tutorials/student_enrollment/#Decision-model","page":"Logistic regression with GLM.jl","title":"Decision model","text":"Now that we have a trained logistic regression model, we want a decision model that chooses the optimal merit scholarship for each student in\n\nevaluate_df\n\nHere's an empty JuMP model to start:\n\nmodel = Model()\n\nFirst, we add a new column to evaluate_df, with one JuMP decision variable for each row. It is important the .merit column name in evaluate_df matches the name in train_df.\n\nevaluate_df.merit = @variable(model, 0 <= x_merit[1:n_students] <= 2.5);\nevaluate_df\n\nThen, we use MathOptAI.add_predictor to embed predictor into the JuMP model. MathOptAI.add_predictor returns a vector of variables, one for each row inn evaluate_df, corresponding to the output enroll of our logistic regression.\n\nevaluate_df.enroll, _ = MathOptAI.add_predictor(model, predictor, evaluate_df);\nevaluate_df\n\nThe .enroll column name in evaluate_df is just a name. It doesn't have to match the name in train_df.\n\nThe objective of our problem is to maximize the expected number of students who enroll:\n\n@objective(model, Max, sum(evaluate_df.enroll))\n\nSubject to the constraint that we can spend at most 0.2 * n_students on merit scholarships:\n\n@constraint(model, sum(evaluate_df.merit) <= 0.2 * n_students)\n\nBecause logistic regression involves a Sigmoid layer, we need to use a smooth nonlinear optimizer. A common choice is Ipopt. Solve and check the optimizer found a feasible solution:\n\nset_optimizer(model, Ipopt.Optimizer)\nset_silent(model)\noptimize!(model)\n@assert is_solved_and_feasible(model)\nsolution_summary(model)\n\nLet's store the solution in evaluate_df for analysis:\n\nevaluate_df.merit_sol = value.(evaluate_df.merit);\nevaluate_df.enroll_sol = value.(evaluate_df.enroll);\nevaluate_df","category":"section"},{"location":"tutorials/student_enrollment/#Solution-analysis","page":"Logistic regression with GLM.jl","title":"Solution analysis","text":"We expect that just under 2,500 students will enroll:\n\nsum(evaluate_df.enroll_sol)\n\nWe awarded merit scholarships to approximately 1 in 6 students:\n\ncount(evaluate_df.merit_sol .> 1e-5)\n\nThe average merit scholarship was worth just over $1,000:\n\n1_000 * Statistics.mean(evaluate_df.merit_sol[evaluate_df.merit_sol .> 1e-5])\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"manual/GLM/#GLM.jl","page":"GLM.jl","title":"GLM.jl","text":"GLM.jl is a library for fitting generalized linear models in Julia.\n\nMathOptAI.jl supports embedding two types of regression models from GLM:\n\nGLM.lm(X, Y)\nGLM.glm(X, Y, GLM.Bernoulli())","category":"section"},{"location":"manual/GLM/#Linear-regression","page":"GLM.jl","title":"Linear regression","text":"The input x to add_predictor must be a vector with the same number of elements as columns in the training matrix. The return is a vector of JuMP variables with a single element.\n\nusing GLM, JuMP, MathOptAI\nX, Y = rand(10, 2), rand(10);\npredictor = GLM.lm(X, Y);\nmodel = Model();\n@variable(model, x[1:2]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/GLM/#Logistic-regression","page":"GLM.jl","title":"Logistic regression","text":"The input x to add_predictor must be a vector with the same number of elements as columns in the training matrix. The return is a vector of JuMP variables with a single element.\n\nusing GLM, JuMP, MathOptAI\nX, Y = rand(10, 2), rand(Bool, 10);\npredictor = GLM.glm(X, Y, GLM.Bernoulli());\nmodel = Model();\n@variable(model, x[1:2]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/GLM/#DataFrames","page":"GLM.jl","title":"DataFrames","text":"DataFrames.jl can be used with GLM.jl.\n\nThe input x to add_predictor must be a DataFrame with the same feature columns as the training DataFrame. The return is a vector of JuMP variables, with one element for each row in the DataFrame.\n\nusing DataFrames, GLM, JuMP, MathOptAI\ntrain_df = DataFrames.DataFrame(x1 = rand(10), x2 = rand(10));\ntrain_df.y = 1.0 .* train_df.x1 + 2.0 .* train_df.x2 .+ rand(10);\npredictor = GLM.lm(GLM.@formula(y ~ x1 + x2), train_df);\nmodel = Model();\ntest_df = DataFrames.DataFrame(\n    x1 = rand(6),\n    x2 = @variable(model, [1:6]),\n);\ntest_df.y, _ = MathOptAI.add_predictor(model, predictor, test_df);\ntest_df.y","category":"section"},{"location":"developers/design_principles/#Design-principles","page":"Design principles","title":"Design principles","text":"This project is inspired by two existing projects:\n\nOMLT\ngurobi-machinelearning\n\nOMLT is a framework built around Pyomo, and gurobi-machinelearning is a framework build around gurobipy.\n\nThese projects served as inspiration, but we also departed from them in some carefully considered ways.\n\nAll of our design decisions were guided by two principles:\n\nTo be simple\nTo leverage Julia's Pkg extensions and multiple dispatch.","category":"section"},{"location":"developers/design_principles/#Terminology","page":"Design principles","title":"Terminology","text":"Because the field is relatively new, there is no settled choice of terminology.\n\nMathOptAI chooses to use \"predictor\" as the synonym for the machine learning model. Hence, we have AbstractPredictor, add_predictor, and build_predictor.\n\nIn contrast, gurobi-machinelearning tends to use \"regression model\" and OMLT uses \"formulation.\"\n\nWe choose \"predictor\" because all models we implement are of the form y = f(x).\n\nWe do not use \"machine learning model\" because we have support for the linear and logistic regression models of classical statistical fitting. We could have used \"regression model,\" but we find that models like neural networks and binary decision trees are not commonly thought of as regression models.","category":"section"},{"location":"developers/design_principles/#Inputs-are-vectors","page":"Design principles","title":"Inputs are vectors","text":"MathOptAI assumes that all inputs x and outputs y to y = predictor(x) are Base.Vectors.\n\nWe make this choice for simplicity.\n\nIn our opinion, Julia libraries often take a laissez-faire approach to the types that they support. In the optimistic case, this can lead to novel behavior by combining two packages that the package author had previously not considered or tested. In the pessimistic case, this can lead to incorrect results or cryptic error messages.\n\nWe choose to make y a Vector, even for scalar outputs, to simplify code that works generically for many different predictors. Without this principle, there will inevitably be cases where a scalar and length-1 vector are confused.\n\nIf you want to use a predictor that does not take Vector input (for example, it is an image as input to a neural network), the first preprocessing step should be to vec the input into a single Vector.\n\nExceptions to the Vector rule will be carefully considered and tested.\n\nCurrently, there are two exceptions:\n\nThe StatsModels extension allows x to be a DataFrames.DataFrame, if the predictor is a StatsModels.TableRegressionModel.\nThere is a fallback that, when given x::Array as input, calls vec(x) and sets the input_size keyword argument. This fallback is used by some extensions to simplify inputs for array-based neural network layers.","category":"section"},{"location":"developers/design_principles/#Inputs-are-provided,-outputs-are-returned","page":"Design principles","title":"Inputs are provided, outputs are returned","text":"The main job of MathOptAI is to embed models of the form y = predictor(x) into a JuMP model. A key design decision is how to represent the input x and output y.","category":"section"},{"location":"developers/design_principles/#gurobi-machinelearning","page":"Design principles","title":"gurobi-machinelearning","text":"gurobi-machinelearning implements an API of the following general form:\n\npred_constr = add_predictor_constr(model, predictor, x, y)\n\nHere, both the input x and the output y must be created and provided by the user, and a new object pred_constr is returned.\n\nThe benefit of this design is that pred_constr can contain statistics about the reformulation (for example, the number of variables that were added), and it can be used to delete a predictor from model.\n\nThe downside is that the user must ensure that the shape and size of y is correct.","category":"section"},{"location":"developers/design_principles/#OMLT","page":"Design principles","title":"OMLT","text":"OMLT implements an API of the following general form:\n\nmodel.pred_constr = OmltBlock()\nmodel.pred_constr.build_formulation(predictor)\nx, y = model.pred_constr.inputs, model.pred_constr.outputs\n\nFirst, a new OmltBlock() is created. Then the formulation is built inside the block, and both the input and output are provided to the user.\n\nThe benefit of this design is that pred_constr can contain statistics about the reformulation (for example, the number of variables that were added), and it can be used to delete a predictor from model.\n\nThe downside is that the user must often write additional constraints to connect the input and output of the OmltBlock to their existing decision variables:\n\n#connect pyomo model input and output to the neural network\n@model.Constraint()\ndef connect_input(mdl):\n    return mdl.input == mdl.nn.inputs[0]\n\n@model.Constraint()\ndef connect_output(mdl):\n    return mdl.output == mdl.nn.outputs[0]\n\nA second downside is that the predictor must describe the input and output dimension; these cannot be inferred automatically. As one example, this means that it cannot do the following:\n\n# Not possible because dimension not given\nmodel.pred_constr.build_formulation(ReLU())\n\nIn the context of MathOptAI, something like ReLU() is useful so that we can map generic layers like Flux.relu => MathOptAI.ReLU(), and so that we do not duplicate required dimension information in input and predictor (see the MathOptAI section below).","category":"section"},{"location":"developers/design_principles/#MathOptAI","page":"Design principles","title":"MathOptAI","text":"The main entry-point to MathOptAI is add_predictor:\n\ny, formulation = MathOptAI.add_predictor(model, predictor, x)\n\nThe user provides the input x, and the output y is returned.\n\nThe main benefit of this approach is simplicity.\n\nFirst, the user probably already has the input x as decision variables or an expression in the model, so we do not need the connect_input constraint, and because we use a full-space formulation by default, the output y will always be a vector of decision variables, which avoids the need for a connect_output constraint.\n\nSecond, predictors do not need to store dimension information, so we can have:\n\ny, formulation = MathOptAI.add_predictor(model, MathOptAI.ReLU(), x)\n\nfor any size of x.","category":"section"},{"location":"developers/design_principles/#Activations-are-predictors","page":"Design principles","title":"Activations are predictors","text":"OMLT makes a distinction between layers, like full_space_dense_layer, and elementwise activation functions, like sigmoid_activation_function.\n\nThe downside to this approach is that it treats activation functions as special, leading to issues such as OMLT#125.\n\nIn contrast, MathOptAI treats activation functions as a vector-valued predictor like any other:\n\ny, formulation = MathOptAI.add_predictor(model, MathOptAI.ReLU(), x)\n\nThis means that we can pipeline them to create predictors such as:\n\nfunction LogisticRegression(A)\n    return MathOptAI.Pipeline(MathOptAI.Affine(A), MathOptAI.Sigmoid())\nend","category":"section"},{"location":"developers/design_principles/#Controlling-transformations","page":"Design principles","title":"Controlling transformations","text":"Many predictors have multiple ways that they can be formulated in an optimization model. For example, ReLU implements the non-smooth nonlinear formulation y = maxx 0, while ReLUQuadratic implements a the complementarity formulation x = y - slack y slack ge 0 y * slack = 0.\n\nChoosing the appropriate formulation for the combination of model and solver can have a large impact on the performance.\n\nBecause gurobi-machinelearning is specific to the Gurobi solver, they have a limited ability for the user to choose and implement different formulations.\n\nOMLT is more general, in that it has multiple ways of formulating layers such as ReLU. However, these are hard-coded into complete formulations such as omlt.neuralnet.nn_formulation.ReluBigMFormulation or omlt.neuralnet.nn_formulation.ReluComplementarityFormulation.\n\nIn contrast, MathOptAI tries to take a maximally modular approach, where the user can control how the layers are formulated at runtime, including using a custom formulation that is not defined in MathOptAI.jl.\n\nCurrently, we achieve this with a config dictionary, which maps the various neural network layers to an AbstractPredictor. For example:\n\nchain = Flux.Chain(Flux.Dense(1 => 16, Flux.relu), Flux.Dense(16 => 1));\nconfig = Dict(Flux.relu => MathOptAI.ReLU)\npredictor = MathOptAI.build_predictor(chain; config)\n\nPlease open a GitHub issue if you have a suggestion for a better API.","category":"section"},{"location":"developers/design_principles/#Full-space-or-reduced-space","page":"Design principles","title":"Full-space or reduced-space","text":"OMLT has two ways that it can formulate neural networks: full-space and reduced-space.\n\nThe full-space formulations add intermediate variables to represent the output of all layers.\n\nFor example, in a Flux.Dense(2, 3, Flux.relu) layer, a full-space formulation will add an intermediate y_tmp variable to represent the output of the affine layer prior to the ReLU:\n\nlayer = Flux.Dense(2, 3, Flux.relu)\nmodel_full_space = Model()\n@variable(model_full_space, x[1:2])\n@variable(model_full_space, y_tmp[1:3])\n@variable(model_full_space, y[1:3])\n@constraint(model_full_space, y_tmp == layer.A * x + layer.b)\n@constraint(model_full_space, y .== max.(0, y_tmp))\n\nIn contrast, a reduced-space formulation encodes the input-output relationship as a single nonlinear constraint:\n\nlayer = Flux.Dense(2, 3, Flux.relu)\nmodel_reduced_space = Model()\n@variable(model_reduced_space, x[1:2])\n@variable(model_reduced_space, y[1:3])\n@constraint(model_reduced_space, y .== max.(0, layer.A * x + layer.b))\n\nIn general, the full-space formulations have more variables and constraints but simpler nonlinear expressions, whereas the reduced-space formulations have fewer variables and constraints but more complicated nonlinear expressions.\n\nMathOptAI.jl implements the full-space formulation by default, but some layers support the reduced-space formulation with the ReducedSpace wrapper.","category":"section"},{"location":"manual/DecisionTree/#DecisionTree.jl","page":"DecisionTree.jl","title":"DecisionTree.jl","text":"DecisionTree.jl is a library for fitting decision trees in Julia.","category":"section"},{"location":"manual/DecisionTree/#Binary-decision-tree-regression","page":"DecisionTree.jl","title":"Binary decision tree regression","text":"Here is an example:\n\nusing JuMP, MathOptAI, DecisionTree\ntruth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\nfeatures = abs.(sin.((1:10) .* (3:4)'));\nsize(features)\nlabels = truth.(Vector.(eachrow(features)));\npredictor = DecisionTree.build_tree(labels, features)\nmodel = Model();\n@variable(model, 0 <= x[1:2] <= 1);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/DecisionTree/#Random-forest-regression","page":"DecisionTree.jl","title":"Random forest regression","text":"using JuMP, MathOptAI, DecisionTree\ntruth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\nfeatures = abs.(sin.((1:10) .* (3:4)'));\nsize(features)\nlabels = truth.(Vector.(eachrow(features)));\npredictor = DecisionTree.build_forest(labels, features)\nmodel = Model();\n@variable(model, 0 <= x[1:2] <= 1);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/predictors/#Predictors","page":"Predictors","title":"Predictors","text":"The main entry point for embedding prediction models into JuMP is add_predictor.\n\nAll methods use the form y, formulation = MathOptAI.add_predictor(model, predictor, x) to add the relationship y = predictor(x) to model.","category":"section"},{"location":"manual/predictors/#Supported-predictors","page":"Predictors","title":"Supported predictors","text":"The following predictors are supported. See their docstrings for details:\n\nPredictor Problem class Dimensions\nAffine Linear M rightarrow N\nAffineCombination Linear M rightarrow N\nAvgPool2d Linear M rightarrow N\nBinaryDecisionTree Mixed-integer linear M rightarrow 1\nConv2d. Linear M rightarrow N\nGCNConv Linear M rightarrow N\nGELU Global nonlinear M rightarrow M\nGrayBox Local nonlinear M rightarrow N\nLayerNorm Global nonlinear M rightarrow M\nLeakyReLU Depends on inner ReLU M rightarrow M\nMaxPool2d Global nonlinear M rightarrow N\nMaxPool2dBigM Mixed-integer linear M rightarrow N\nPipeline  M rightarrow N\nQuantile Local nonlinear M rightarrow N\nReLU Global nonlinear M rightarrow M\nReLUBigM Mixed-integer linear M rightarrow M\nReLUQuadratic Non-convex quadratic M rightarrow M\nReLUSOS1 Mixed-integer linear M rightarrow M\nScale Linear M rightarrow M\nSigmoid Global nonlinear M rightarrow M\nSoftMax Global nonlinear M rightarrow M\nSoftPlus Global nonlinear M rightarrow M\nTAGConv Linear M rightarrow N\nTanh Global nonlinear M rightarrow M","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page lists the public API of MathOptAI.\n\ninfo: Info\nThis page is an unstructured list of the MathOptAI API. For a more structured overview, read the Manual or Tutorial parts of this documentation.\n\nLoad all of the public the API into the current scope with:\n\nusing MathOptAI\n\nAlternatively, load only the module with:\n\nimport MathOptAI\n\nand then prefix all calls with MathOptAI. to create MathOptAI.<NAME>.","category":"section"},{"location":"api/#AbstractPredictor","page":"API Reference","title":"AbstractPredictor","text":"","category":"section"},{"location":"api/#add_predictor","page":"API Reference","title":"add_predictor","text":"","category":"section"},{"location":"api/#build_predictor","page":"API Reference","title":"build_predictor","text":"","category":"section"},{"location":"api/#output_size","page":"API Reference","title":"output_size","text":"","category":"section"},{"location":"api/#Affine","page":"API Reference","title":"Affine","text":"","category":"section"},{"location":"api/#AffineCombination","page":"API Reference","title":"AffineCombination","text":"","category":"section"},{"location":"api/#AvgPool2d","page":"API Reference","title":"AvgPool2d","text":"","category":"section"},{"location":"api/#BinaryDecisionTree","page":"API Reference","title":"BinaryDecisionTree","text":"","category":"section"},{"location":"api/#Conv2d","page":"API Reference","title":"Conv2d","text":"","category":"section"},{"location":"api/#GCNConv","page":"API Reference","title":"GCNConv","text":"","category":"section"},{"location":"api/#GELU","page":"API Reference","title":"GELU","text":"","category":"section"},{"location":"api/#GrayBox","page":"API Reference","title":"GrayBox","text":"","category":"section"},{"location":"api/#LayerNorm","page":"API Reference","title":"LayerNorm","text":"","category":"section"},{"location":"api/#LeakyReLU","page":"API Reference","title":"LeakyReLU","text":"","category":"section"},{"location":"api/#MaxPool2d","page":"API Reference","title":"MaxPool2d","text":"","category":"section"},{"location":"api/#MaxPool2dBigM","page":"API Reference","title":"MaxPool2dBigM","text":"","category":"section"},{"location":"api/#Permutation","page":"API Reference","title":"Permutation","text":"","category":"section"},{"location":"api/#Pipeline","page":"API Reference","title":"Pipeline","text":"","category":"section"},{"location":"api/#PytorchModel","page":"API Reference","title":"PytorchModel","text":"","category":"section"},{"location":"api/#Quantile","page":"API Reference","title":"Quantile","text":"","category":"section"},{"location":"api/#ReducedSpace","page":"API Reference","title":"ReducedSpace","text":"","category":"section"},{"location":"api/#ReLU","page":"API Reference","title":"ReLU","text":"","category":"section"},{"location":"api/#ReLUBigM","page":"API Reference","title":"ReLUBigM","text":"","category":"section"},{"location":"api/#ReLUQuadratic","page":"API Reference","title":"ReLUQuadratic","text":"","category":"section"},{"location":"api/#ReLUSOS1","page":"API Reference","title":"ReLUSOS1","text":"","category":"section"},{"location":"api/#Scale","page":"API Reference","title":"Scale","text":"","category":"section"},{"location":"api/#Sigmoid","page":"API Reference","title":"Sigmoid","text":"","category":"section"},{"location":"api/#SoftMax","page":"API Reference","title":"SoftMax","text":"","category":"section"},{"location":"api/#SoftPlus","page":"API Reference","title":"SoftPlus","text":"","category":"section"},{"location":"api/#TAGConv","page":"API Reference","title":"TAGConv","text":"","category":"section"},{"location":"api/#Tanh","page":"API Reference","title":"Tanh","text":"","category":"section"},{"location":"api/#AbstractFormulation","page":"API Reference","title":"AbstractFormulation","text":"","category":"section"},{"location":"api/#Formulation","page":"API Reference","title":"Formulation","text":"","category":"section"},{"location":"api/#PipelineFormulation","page":"API Reference","title":"PipelineFormulation","text":"","category":"section"},{"location":"api/#AbstractGPs","page":"API Reference","title":"AbstractGPs","text":"","category":"section"},{"location":"api/#DecisionTree","page":"API Reference","title":"DecisionTree","text":"","category":"section"},{"location":"api/#EvoTrees","page":"API Reference","title":"EvoTrees","text":"","category":"section"},{"location":"api/#Flux","page":"API Reference","title":"Flux","text":"","category":"section"},{"location":"api/#GLM","page":"API Reference","title":"GLM","text":"","category":"section"},{"location":"api/#Lux","page":"API Reference","title":"Lux","text":"","category":"section"},{"location":"api/#PythonCall","page":"API Reference","title":"PythonCall","text":"","category":"section"},{"location":"api/#StatsModels","page":"API Reference","title":"StatsModels","text":"","category":"section"},{"location":"api/#Extensions","page":"API Reference","title":"Extensions","text":"","category":"section"},{"location":"api/#replace_weights_with_variables","page":"API Reference","title":"replace_weights_with_variables","text":"","category":"section"},{"location":"api/#MathOptAI.AbstractPredictor","page":"API Reference","title":"MathOptAI.AbstractPredictor","text":"abstract type AbstractPredictor end\n\nAn abstract type representing different types of prediction models.\n\nMethods\n\nAll subtypes must implement:\n\nadd_predictor\nbuild_predictor\n\nThe following methods are optional, but encouraged:\n\noutput_size\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.add_predictor","page":"API Reference","title":"MathOptAI.add_predictor","text":"MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::MathOptAI.Quantile{<:AbstractGPs.PosteriorGP},\n    x::Vector,\n)\n\nAdd the quantiles of a trained Gaussian Process from AbstractGPs.jl to model.\n\nExample\n\njulia> using JuMP, MathOptAI, AbstractGPs\n\njulia> x_data = 2π .* (0.0:0.1:1.0);\n\njulia> y_data = sin.(x_data);\n\njulia> fx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.1);\n\njulia> p_fx = AbstractGPs.posterior(fx, y_data);\n\njulia> model = Model();\n\njulia> @variable(model, 1 <= x[1:1] <= 6, start = 3);\n\njulia> predictor = MathOptAI.Quantile(p_fx, [0.1, 0.9]);\n\njulia> y, _ = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_quantile[1]\n moai_quantile[2]\n\njulia> @objective(model, Max, y[2] - y[1])\nmoai_quantile[2] - moai_quantile[1]\n\n\n\n\n\nMathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::StatsModels.TableRegressionModel,\n    x::DataFrames.DataFrame;\n    kwargs...,\n)\n\nAdd a trained regression model from StatsModels.jl to model, using the DataFrame x as input.\n\nIn most cases, predictor should be a GLM.jl predictor supported by MathOptAI, but trained using @formula and a DataFrame instead of the raw matrix input.\n\nIn general, x may have some columns that are constant (Float64) and some columns that are JuMP decision variables.\n\nKeyword arguments\n\nAll keyword arguments are passed to the corresponding add_predictor of the GLM extension.\n\nExample\n\njulia> using DataFrames, GLM, JuMP, MathOptAI\n\njulia> train_df = DataFrames.DataFrame(x1 = rand(10), x2 = rand(10));\n\njulia> train_df.y = 1.0 .* train_df.x1 + 2.0 .* train_df.x2 .+ rand(10);\n\njulia> predictor = GLM.lm(GLM.@formula(y ~ x1 + x2), train_df);\n\njulia> model = Model();\n\njulia> test_df = DataFrames.DataFrame(\n           x1 = rand(6),\n           x2 = @variable(model, [1:6]),\n       );\n\njulia> test_df.y, _ = MathOptAI.add_predictor(model, predictor, test_df);\n\njulia> test_df.y\n6-element Vector{VariableRef}:\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n\n\n\n\n\nadd_predictor(\n    model::JuMP.AbstractModel,\n    predictor::Any,\n    x::Vector;\n    reduced_space::Bool = false,\n    kwargs...,\n)::Tuple{<:Vector,<:AbstractFormulation}\n\nReturn a Vector representing y such that y = predictor(x) and an AbstractFormulation containing the variables and constraints that were added to the model.\n\nThe element type of x is deliberately unspecified. The vector x may contain any mix of scalar constants, JuMP decision variables, and scalar JuMP functions like AffExpr, QuadExpr, or NonlinearExpr.\n\nKeyword arguments\n\nreduced_space: if true, wrap predictor in ReducedSpace before adding to the model.\n\nAll other keyword arguments are passed to build_predictor.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.Affine([2.0, 3.0])\nAffine(A, b) [input: 2, output: 1]\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> formulation\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [1]\n  └ 2 x[1] + 3 x[2] - moai_Affine[1] = 0\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x; reduced_space = true);\n\njulia> y\n1-element Vector{AffExpr}:\n 2 x[1] + 3 x[2]\n\njulia> formulation\nReducedSpace(Affine(A, b) [input: 2, output: 1])\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\nadd_predictor(model::JuMP.AbstractModel, predictor, x::Array; kwargs...)\n\nThis method is a helper function for adding predictor to model when the input x is a multi-dimensional array.\n\nIt is equivalent to passing vec(x) with the keyword input_size = size(x).\n\nExample\n\njulia> using JuMP, MathOptAI, Flux\n\njulia> model = Model();\n\njulia> @variable(model, x[1:4, 1:4]);\n\njulia> predictor = Flux.Chain(Flux.MaxPool((2, 2)), Flux.flatten);\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n4-element Vector{VariableRef}:\n moai_MaxPool2d[1]\n moai_MaxPool2d[2]\n moai_MaxPool2d[3]\n moai_MaxPool2d[4]\n\njulia> formulation\nMaxPool2d((4, 4, 1), (2, 2), (2, 2), (0, 0))\n├ variables [4]\n│ ├ moai_MaxPool2d[1]\n│ ├ moai_MaxPool2d[2]\n│ ├ moai_MaxPool2d[3]\n│ └ moai_MaxPool2d[4]\n└ constraints [4]\n  ├ moai_MaxPool2d[1] - max(max(max(x[1,1], x[2,1]), x[1,2]), x[2,2]) = 0\n  ├ moai_MaxPool2d[2] - max(max(max(x[3,1], x[4,1]), x[3,2]), x[4,2]) = 0\n  ├ moai_MaxPool2d[3] - max(max(max(x[1,3], x[2,3]), x[1,4]), x[2,4]) = 0\n  └ moai_MaxPool2d[4] - max(max(max(x[3,3], x[4,3]), x[3,4]), x[4,4]) = 0\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.build_predictor-Tuple{MathOptAI.AbstractPredictor}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(predictor::EvoTrees.EvoTree{L,1}) where {L}\n\nConvert a boosted tree from EvoTrees.jl to an AffineCombination of BinaryDecisionTree.\n\nExample\n\njulia> using JuMP, MathOptAI, EvoTrees\n\njulia> truth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\ntruth (generic function with 1 method)\n\njulia> x_train = abs.(sin.((1:10) .* (3:4)'));\n\njulia> size(x_train)\n(10, 2)\n\njulia> y_train = truth.(Vector.(eachrow(x_train)));\n\njulia> config = EvoTrees.EvoTreeRegressor(; nrounds = 3);\n\njulia> tree = EvoTrees.fit(config; x_train, y_train);\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[1:2] <= 1);\n\njulia> y, _ = MathOptAI.add_predictor(model, tree, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_AffineCombination[1]\n\njulia> MathOptAI.build_predictor(tree)\nAffineCombination\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n└ 1.0 * [2.0]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.output_size","page":"API Reference","title":"MathOptAI.output_size","text":"output_size(predictor::AbstractPredictor, input_size::Nothing)\noutput_size(predictor::AbstractPredictor, input_size::NTuple{N,Int}) where {N}\n\nReturn the output size of predictor with an input with shape input_size.\n\nIf input_size === nothing, no information about the input is known. This function returns an NTuple{N,Int} if a static output size based on the predictor, otherwise returns nothing.\n\nExample\n\njulia> using MathOptAI\n\njulia> output_size(ReLU(), nothing)\n\njulia> output_size(ReLU(), (2,))\n(2,)\n\njulia> output_size(MaxPool2d((3, 3); input_size = (6, 9, 1)), (6, 9, 1))\n(2, 3, 1)\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.Affine","page":"API Reference","title":"MathOptAI.Affine","text":"Affine(\n    A::Matrix{T},\n    b::Vector{T} = zeros(T, size(A, 1)),\n) where {T} <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = A x + b\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Affine([2.0 3.0], [4.0])\nAffine(A, b) [input: 2, output: 1]\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> formulation\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [3]\n  ├ moai_Affine[1] ≥ 4\n  ├ moai_Affine[1] ≤ 12\n  └ 2 x[1] + 3 x[2] - moai_Affine[1] = -4\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n1-element Vector{AffExpr}:\n 2 x[1] + 3 x[2] + 4\n\njulia> formulation\nReducedSpace(Affine(A, b) [input: 2, output: 1])\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.AffineCombination","page":"API Reference","title":"MathOptAI.AffineCombination","text":"AffineCombination(\n    predictors::Vector{<:AbstractPredictor},\n    weights::Vector{Float64},\n    constant::Vector{Float64},\n)\n\nAn AbstractPredictor that represents the linear combination of other predictors.\n\nThe main purpose of this predictor is to model random forests and gradient boosted trees.\n\nA random forest is the mean a set of BinaryDecisionTree\nA gradient boosted tree is the sum of a set of BinaryDecisionTree\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> rhs = MathOptAI.BinaryDecisionTree(1, 1.0, 0, 1)\nBinaryDecisionTree{Float64,Int64} [leaves=2, depth=2]\n\njulia> lhs = MathOptAI.BinaryDecisionTree(1, -0.1, -1, 0)\nBinaryDecisionTree{Float64,Int64} [leaves=2, depth=2]\n\njulia> tree_1 = MathOptAI.BinaryDecisionTree(1, 0.0, -1, rhs);\n\njulia> tree_2 = MathOptAI.BinaryDecisionTree(1, 0.9, lhs, 1);\n\njulia> random_forest = MathOptAI.AffineCombination(\n           [tree_1, tree_2],\n           [0.5, 0.5],\n           [0.0],\n       )\nAffineCombination\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n└ 1.0 * [0.0]\n\njulia> model = Model();\n\njulia> @variable(model, -3 <= x[1:1] <= 5);\n\njulia> y, formulation = MathOptAI.add_predictor(model, random_forest, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_AffineCombination[1]\n\njulia> formulation\nAffineCombination\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n└ 1.0 * [0.0]\n├ variables [1]\n│ └ moai_AffineCombination[1]\n└ constraints [1]\n  └ 0.5 moai_BinaryDecisionTree_value[1] + 0.5 moai_BinaryDecisionTree_value[1] - moai_AffineCombination[1] = 0\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ variables [4]\n│ ├ moai_BinaryDecisionTree_value[1]\n│ ├ moai_BinaryDecisionTree_z[1]\n│ ├ moai_BinaryDecisionTree_z[2]\n│ └ moai_BinaryDecisionTree_z[3]\n└ constraints [7]\n  ├ moai_BinaryDecisionTree_z[1] + moai_BinaryDecisionTree_z[2] + moai_BinaryDecisionTree_z[3] = 1\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ -1.0e-6}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≤ 0.999999}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 1}\n  └ moai_BinaryDecisionTree_z[1] - moai_BinaryDecisionTree_z[3] + moai_BinaryDecisionTree_value[1] = 0\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ variables [4]\n│ ├ moai_BinaryDecisionTree_value[1]\n│ ├ moai_BinaryDecisionTree_z[1]\n│ ├ moai_BinaryDecisionTree_z[2]\n│ └ moai_BinaryDecisionTree_z[3]\n└ constraints [7]\n  ├ moai_BinaryDecisionTree_z[1] + moai_BinaryDecisionTree_z[2] + moai_BinaryDecisionTree_z[3] = 1\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ 0.899999}\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ -0.100001}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≤ 0.899999}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≥ -0.1}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 0.9}\n  └ moai_BinaryDecisionTree_z[1] - moai_BinaryDecisionTree_z[3] + moai_BinaryDecisionTree_value[1] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.AvgPool2d","page":"API Reference","title":"MathOptAI.AvgPool2d","text":"AvgPool2d(\n    kernel_size::Tuple{Int,Int};\n    input_size::Tuple{Int,Int,Int},\n    stride::Tuple{Int,Int} = (1, 1),\n    padding::Tuple{Int,Int} = (0, 0),\n) <: AbstractPredictor\n\nAn AbstractPredictor that represents a mean pooling layer.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[h in 1:2, w in 1:4])\n2×4 Matrix{VariableRef}:\n x[1,1]  x[1,2]  x[1,3]  x[1,4]\n x[2,1]  x[2,2]  x[2,3]  x[2,4]\n\njulia> predictor = MathOptAI.AvgPool2d((2, 2); input_size = (2, 4, 1))\nAvgPool2d((2, 4, 1), (2, 2), (2, 2), (0, 0))\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, vec(x));\n\njulia> y\n2-element Vector{VariableRef}:\n moai_AvgPool2d[1]\n moai_AvgPool2d[2]\n\njulia> formulation\nAvgPool2d((2, 4, 1), (2, 2), (2, 2), (0, 0))\n├ variables [2]\n│ ├ moai_AvgPool2d[1]\n│ └ moai_AvgPool2d[2]\n└ constraints [2]\n  ├ -0.25 x[1,1] - 0.25 x[2,1] - 0.25 x[1,2] - 0.25 x[2,2] + moai_AvgPool2d[1] = 0\n  └ -0.25 x[1,3] - 0.25 x[2,3] - 0.25 x[1,4] - 0.25 x[2,4] + moai_AvgPool2d[2] = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, predictor, vec(x); reduced_space = true);\n\njulia> y\n2-element Vector{AffExpr}:\n 0.25 x[1,1] + 0.25 x[1,2] + 0.25 x[2,1] + 0.25 x[2,2]\n 0.25 x[1,3] + 0.25 x[1,4] + 0.25 x[2,3] + 0.25 x[2,4]\n\njulia> formulation\nReducedSpace(AvgPool2d((2, 4, 1), (2, 2), (2, 2), (0, 0)))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.BinaryDecisionTree","page":"API Reference","title":"MathOptAI.BinaryDecisionTree","text":"BinaryDecisionTree{K,V}(\n    feat_id::Int,\n    feat_value::K,\n    lhs::Union{V,BinaryDecisionTree{K,V}},\n    rhs::Union{V,BinaryDecisionTree{K,V}},\n    atol::Float64 = 1e-6,\n)\n\nAn AbstractPredictor that represents a binary decision tree.\n\nIf x[feat_id] <= feat_value - atol, then return lhs\nIf x[feat_id] >= feat_value, then return rhs\n\nExample\n\nTo represent the tree x[1] <= 0.0 ? -1 : (x[1] <= 1.0 ? 0 : 1), do:\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:1]);\n\njulia> f = MathOptAI.BinaryDecisionTree{Float64,Int}(\n           1,\n           0.0,\n           -1,\n           MathOptAI.BinaryDecisionTree{Float64,Int}(1, 1.0, 0, 1),\n       )\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_BinaryDecisionTree_value[1]\n\njulia> formulation\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ variables [4]\n│ ├ moai_BinaryDecisionTree_value[1]\n│ ├ moai_BinaryDecisionTree_z[1]\n│ ├ moai_BinaryDecisionTree_z[2]\n│ └ moai_BinaryDecisionTree_z[3]\n└ constraints [7]\n  ├ moai_BinaryDecisionTree_z[1] + moai_BinaryDecisionTree_z[2] + moai_BinaryDecisionTree_z[3] = 1\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ -1.0e-6}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≤ 0.999999}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 1}\n  └ moai_BinaryDecisionTree_z[1] - moai_BinaryDecisionTree_z[3] + moai_BinaryDecisionTree_value[1] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Conv2d","page":"API Reference","title":"MathOptAI.Conv2d","text":"Conv2d(\n    weight::Array{T,4},\n    bias::Vector{T};\n    input_size::Tuple{Int,Int,Int},\n    stride::Tuple{Int,Int} = (1, 1),\n    padding::Tuple{Int,Int} = (0, 0),\n) where {T} <: AbstractPredictor\n\nAn AbstractPredictor that represents a mean pooling layer a 2-dimensional convolutional layer.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[h in 1:2, w in 1:3])\n2×3 Matrix{VariableRef}:\n x[1,1]  x[1,2]  x[1,3]\n x[2,1]  x[2,2]  x[2,3]\n\njulia> weight = reshape(collect(1.0:8.0), 2, 2, 1, 2);\n\njulia> bias = [-1.0, -2.0];\n\njulia> predictor = MathOptAI.Conv2d(weight, bias; input_size = (2, 3, 1))\nConv2d{Float64}((2, 3, 1), [1.0 3.0; 2.0 4.0;;;; 5.0 7.0; 6.0 8.0], [-1.0, -2.0], (1, 1), (0, 0))\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, vec(x));\n\njulia> y\n4-element Vector{VariableRef}:\n moai_Conv2d[1]\n moai_Conv2d[2]\n moai_Conv2d[3]\n moai_Conv2d[4]\n\njulia> formulation\nConv2d{Float64}((2, 3, 1), [1.0 3.0; 2.0 4.0;;;; 5.0 7.0; 6.0 8.0], [-1.0, -2.0], (1, 1), (0, 0))\n├ variables [4]\n│ ├ moai_Conv2d[1]\n│ ├ moai_Conv2d[2]\n│ ├ moai_Conv2d[3]\n│ └ moai_Conv2d[4]\n└ constraints [4]\n  ├ -4 x[1,1] - 3 x[2,1] - 2 x[1,2] - x[2,2] + moai_Conv2d[1] = -1\n  ├ -4 x[1,2] - 3 x[2,2] - 2 x[1,3] - x[2,3] + moai_Conv2d[2] = -1\n  ├ -8 x[1,1] - 7 x[2,1] - 6 x[1,2] - 5 x[2,2] + moai_Conv2d[3] = -2\n  └ -8 x[1,2] - 7 x[2,2] - 6 x[1,3] - 5 x[2,3] + moai_Conv2d[4] = -2\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, predictor, vec(x); reduced_space = true);\n\njulia> y\n4-element Vector{AffExpr}:\n 4 x[1,1] + 2 x[1,2] + 3 x[2,1] + x[2,2] - 1\n 4 x[1,2] + 2 x[1,3] + 3 x[2,2] + x[2,3] - 1\n 8 x[1,1] + 6 x[1,2] + 7 x[2,1] + 5 x[2,2] - 2\n 8 x[1,2] + 6 x[1,3] + 7 x[2,2] + 5 x[2,3] - 2\n\njulia> formulation\nReducedSpace(Conv2d{Float64}((2, 3, 1), [1.0 3.0; 2.0 4.0;;;; 5.0 7.0; 6.0 8.0], [-1.0, -2.0], (1, 1), (0, 0)))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.GCNConv","page":"API Reference","title":"MathOptAI.GCNConv","text":"GCNConv(;\n    weights::Matrix{T},\n    bias::Vector{T},\n    edge_index::Vector{Pair{Int,Int}},\n)\n\nAn AbstractPredictor that represents a graph convolutional network operator:\n\nY = D^-12 A D^-12 X W + b\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:3, 1:2]);\n\njulia> f = MathOptAI.GCNConv(;\n           weights = [1.0; 2.0;;],\n           bias = [7.0],\n           edge_index = [1 => 2, 2 => 1, 2 => 3, 3 => 2],\n       );\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, vec(x));\n\njulia> y\n3-element Vector{VariableRef}:\n moai_GCNConv[1]\n moai_GCNConv[2]\n moai_GCNConv[3]\n\njulia> formulation\nGCNConv{Float64}([1.0; 2.0;;], [7.0], [0.4999999999999999 0.40824829046386296 0.0; 0.40824829046386296 0.33333333333333337 0.40824829046386296; 0.0 0.40824829046386296 0.4999999999999999])\n├ variables [3]\n│ ├ moai_GCNConv[1]\n│ ├ moai_GCNConv[2]\n│ └ moai_GCNConv[3]\n└ constraints [3]\n  ├ -0.4999999999999999 x[1,1] - 0.40824829046386296 x[2,1] - x[1,2] - 0.8164965809277259 x[2,2] + moai_GCNConv[1] = 7\n  ├ -0.40824829046386296 x[1,1] - 0.33333333333333337 x[2,1] - 0.40824829046386296 x[3,1] - 0.8164965809277259 x[1,2] - 0.6666666666666667 x[2,2] - 0.8164965809277259 x[3,2] + moai_GCNConv[2] = 7\n  └ -0.40824829046386296 x[2,1] - 0.4999999999999999 x[3,1] - 0.8164965809277259 x[2,2] - x[3,2] + moai_GCNConv[3] = 7\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.GELU","page":"API Reference","title":"MathOptAI.GELU","text":"GeLU() <: AbstractPredictor\n\nAn AbstractPredictor representing the Gaussian Error Linear Units function:\n\ny approx x * (1 + tanh(sqrt(2  pi) * (x + 0044715 x^3)))  2\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.GELU()\nGELU()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_GELU[1]\n moai_GELU[2]\n\njulia> formulation\nGELU()\n├ variables [2]\n│ ├ moai_GELU[1]\n│ └ moai_GELU[2]\n└ constraints [6]\n  ├ moai_GELU[1] ≥ -0.17\n  ├ moai_GELU[1] ≤ 0.8411919906082768\n  ├ moai_GELU[1] - ((0.5 x[1]) * (1 + tanh(0.7978845608028654 * (x[1] + (0.044715 * (x[1] ^ 3)))))) = 0\n  ├ moai_GELU[2] ≥ -0.17\n  ├ moai_GELU[2] ≤ 1.954597694087775\n  └ moai_GELU[2] - ((0.5 x[2]) * (1 + tanh(0.7978845608028654 * (x[2] + (0.044715 * (x[2] ^ 3)))))) = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.GrayBox","page":"API Reference","title":"MathOptAI.GrayBox","text":" GrayBox(\n    predictor::P;\n    device::String = \"cpu\",\n    hessian::Bool = true,\n) where {P}\n\nAn AbstractPredictor that represents the relationship:\n\ny = f(x)\n\nas a vector nonlinear operator.\n\nThis predictor should not be used directly; it is intended to be used by extensions like Flux and PyTorch.\n\nExample\n\njulia> using JuMP, MathOptAI, Flux\n\njulia> chain = Flux.Chain(Flux.Dense(1 => 16, Flux.relu), Flux.Dense(16 => 1));\n\njulia> model = Model();\n\njulia> @variable(model, x[1:1]);\n\njulia> y, _ = MathOptAI.add_predictor(model, chain, x; gray_box = true);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Flux[1]\n\njulia> print(model)\nFeasibility\nSubject to\n [x[1], moai_Flux[1]] ∈ VectorNonlinearOracle{Float64}(;\n     dimension = 2,\n     l = [0.0],\n     u = [0.0],\n     ...,\n )\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.LayerNorm","page":"API Reference","title":"MathOptAI.LayerNorm","text":"LayerNorm(\n    shape::NTuple{N,Int};\n    input_size::Tuple{Int,Int,Int},\n    eps::Float64 = 1e-5,\n) where {N}\n\nAn AbstractPredictor that represents the relationship:\n\ny = fracx - ExsqrtVar(x) + eps\n\nwhere E and Var are computed over the first shape dimensions of x.\n\n!!! Note     This layer does not implement the affine scaling seen in some layers.     Apply Scale to the outputs of this predictor instead.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2, 1:3]);\n\njulia> f = MathOptAI.LayerNorm(\n           (2,);\n           eps = 0.0,\n           input_size = (2, 3, 1),\n           weight = [1.0, 2.0],\n           bias = [0.5, 0.6],\n       )\nLayerNorm{Float64, 1}((2, 3, 1), (2,), 0.0, [1.0, 2.0], [0.5, 0.6])\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, vec(x));\n\njulia> y\n6-element Vector{VariableRef}:\n moai_LayerNorm[1]\n moai_LayerNorm[2]\n moai_LayerNorm[3]\n moai_LayerNorm[4]\n moai_LayerNorm[5]\n moai_LayerNorm[6]\n\njulia> formulation\nLayerNorm{Float64, 1}((2, 3, 1), (2,), 0.0, [1.0, 2.0], [0.5, 0.6])\n├ variables [3]\n│ ├ VariableRef[moai_LayerNorm_μ[1], moai_LayerNorm_μ[2], moai_LayerNorm_μ[3]]\n│ ├ VariableRef[moai_LayerNorm_σ[1], moai_LayerNorm_σ[2], moai_LayerNorm_σ[3]]\n│ └ VariableRef[moai_LayerNorm[1], moai_LayerNorm[2], moai_LayerNorm[3], moai_LayerNorm[4], moai_LayerNorm[5], moai_LayerNorm[6]]\n└ constraints [15]\n  ├ moai_LayerNorm_σ[1] ≥ 0\n  ├ moai_LayerNorm_σ[2] ≥ 0\n  ├ moai_LayerNorm_σ[3] ≥ 0\n  ├ -x[1,1] - x[2,1] + 2 moai_LayerNorm_μ[1] = 0\n  ├ -0.5 x[1,1]² + moai_LayerNorm_μ[1]*x[1,1] - 0.5 x[2,1]² + moai_LayerNorm_μ[1]*x[2,1] - moai_LayerNorm_μ[1]² + moai_LayerNorm_σ[1]² = 0\n  ├ moai_LayerNorm_σ[1]*moai_LayerNorm[1] - x[1,1] + moai_LayerNorm_μ[1] - 0.5 moai_LayerNorm_σ[1] = 0\n  ├ moai_LayerNorm_σ[1]*moai_LayerNorm[2] - 2 x[2,1] + 2 moai_LayerNorm_μ[1] - 0.6 moai_LayerNorm_σ[1] = 0\n  ├ -x[1,2] - x[2,2] + 2 moai_LayerNorm_μ[2] = 0\n  ├ -0.5 x[1,2]² + moai_LayerNorm_μ[2]*x[1,2] - 0.5 x[2,2]² + moai_LayerNorm_μ[2]*x[2,2] - moai_LayerNorm_μ[2]² + moai_LayerNorm_σ[2]² = 0\n  ├ moai_LayerNorm_σ[2]*moai_LayerNorm[3] - x[1,2] + moai_LayerNorm_μ[2] - 0.5 moai_LayerNorm_σ[2] = 0\n  ├ moai_LayerNorm_σ[2]*moai_LayerNorm[4] - 2 x[2,2] + 2 moai_LayerNorm_μ[2] - 0.6 moai_LayerNorm_σ[2] = 0\n  ├ -x[1,3] - x[2,3] + 2 moai_LayerNorm_μ[3] = 0\n  ├ -0.5 x[1,3]² + moai_LayerNorm_μ[3]*x[1,3] - 0.5 x[2,3]² + moai_LayerNorm_μ[3]*x[2,3] - moai_LayerNorm_μ[3]² + moai_LayerNorm_σ[3]² = 0\n  ├ moai_LayerNorm_σ[3]*moai_LayerNorm[5] - x[1,3] + moai_LayerNorm_μ[3] - 0.5 moai_LayerNorm_σ[3] = 0\n  └ moai_LayerNorm_σ[3]*moai_LayerNorm[6] - 2 x[2,3] + 2 moai_LayerNorm_μ[3] - 0.6 moai_LayerNorm_σ[3] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.LeakyReLU","page":"API Reference","title":"MathOptAI.LeakyReLU","text":"LeakyReLU(;\n    negative_slope::Float64,\n    relu::AbstractPredictor = ReLU(),\n) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x + eta cdot min0 x\n\nor equivalently:\n\ny = eta cdot x + (1 - eta) cdot max0 x\n\nwhere negative_slope is eta and relu is used to represent max0 x.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.LeakyReLU(; negative_slope = 0.01)\nLeakyReLU{ReLU}(0.01, ReLU())\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_LeakyReLU[1]\n moai_LeakyReLU[2]\n\njulia> formulation\nLeakyReLU{ReLU}(0.01, ReLU())\n├ variables [4]\n│ ├ moai_ReLU[1]\n│ ├ moai_ReLU[2]\n│ ├ moai_LeakyReLU[1]\n│ └ moai_LeakyReLU[2]\n└ constraints [8]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_ReLU[1] - max(0, x[1]) = 0\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  ├ moai_ReLU[2] - max(0, x[2]) = 0\n  ├ -0.01 x[1] + moai_LeakyReLU[1] - 0.99 moai_ReLU[1] = 0\n  └ -0.01 x[2] + moai_LeakyReLU[2] - 0.99 moai_ReLU[2] = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n (0.01 x[1]) + (0.99 * max(0, x[1]))\n (0.01 x[2]) + (0.99 * max(0, x[2]))\n\njulia> formulation\nReducedSpace(LeakyReLU{ReLU}(0.01, ReLU()))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.MaxPool2d","page":"API Reference","title":"MathOptAI.MaxPool2d","text":"MaxPool2d(\n    kernel_size::Tuple{Int,Int};\n    input_size::Tuple{Int,Int,Int},\n    stride::Tuple{Int,Int} = (1, 1),\n    padding::Tuple{Int,Int} = (0, 0),\n) <: AbstractPredictor\n\nAn AbstractPredictor that represents a two-dimensinal max pooling layer.\n\nThe max function is implemented as a non-smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[h in 1:2, w in 1:4])\n2×4 Matrix{VariableRef}:\n x[1,1]  x[1,2]  x[1,3]  x[1,4]\n x[2,1]  x[2,2]  x[2,3]  x[2,4]\n\njulia> predictor = MathOptAI.MaxPool2d((2, 2); input_size = (2, 4, 1))\nMaxPool2d((2, 4, 1), (2, 2), (2, 2), (0, 0))\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, vec(x));\n\njulia> y\n2-element Vector{VariableRef}:\n moai_MaxPool2d[1]\n moai_MaxPool2d[2]\n\njulia> formulation\nMaxPool2d((2, 4, 1), (2, 2), (2, 2), (0, 0))\n├ variables [2]\n│ ├ moai_MaxPool2d[1]\n│ └ moai_MaxPool2d[2]\n└ constraints [2]\n  ├ moai_MaxPool2d[1] - max(max(max(x[1,1], x[2,1]), x[1,2]), x[2,2]) = 0\n  └ moai_MaxPool2d[2] - max(max(max(x[1,3], x[2,3]), x[1,4]), x[2,4]) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, predictor, vec(x); reduced_space = true);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n max(max(max(x[1,1], x[2,1]), x[1,2]), x[2,2])\n max(max(max(x[1,3], x[2,3]), x[1,4]), x[2,4])\n\njulia> formulation\nReducedSpace(MaxPool2d((2, 4, 1), (2, 2), (2, 2), (0, 0)))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.MaxPool2dBigM","page":"API Reference","title":"MathOptAI.MaxPool2dBigM","text":"MaxPool2dBigM(\n    kernel_size::Tuple{Int,Int};\n    M::Float64,\n    input_size::Tuple{Int,Int,Int},\n    stride::Tuple{Int,Int} = kernel_size,\n    padding::Tuple{Int,Int} = (0, 0),\n) <: AbstractPredictor\n\nAn AbstractPredictor that represents a two-dimensinal max pooling layer.\n\nThe max function is implemented as a big-M mixed-integer linear program.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[h in 1:2, w in 1:4])\n2×4 Matrix{VariableRef}:\n x[1,1]  x[1,2]  x[1,3]  x[1,4]\n x[2,1]  x[2,2]  x[2,3]  x[2,4]\n\njulia> predictor =\n           MathOptAI.MaxPool2dBigM((2, 2); input_size = (2, 4, 1), M = 100.0)\nMaxPool2dBigM((2, 4, 1), (2, 2), (2, 2), (0, 0), 100.0)\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, vec(x));\n\njulia> y\n2-element Vector{VariableRef}:\n moai_MaxPool2d[1]\n moai_MaxPool2d[2]\n\njulia> formulation\nMaxPool2dBigM((2, 4, 1), (2, 2), (2, 2), (0, 0), 100.0)\n├ variables [10]\n│ ├ moai_MaxPool2d[1]\n│ ├ moai_MaxPool2d[2]\n│ ├ moai_z[h=1,w=1,c=1][1]\n│ ├ moai_z[h=1,w=1,c=1][2]\n│ ├ moai_z[h=1,w=1,c=1][3]\n│ ├ moai_z[h=1,w=1,c=1][4]\n│ ├ moai_z[h=1,w=2,c=1][1]\n│ ├ moai_z[h=1,w=2,c=1][2]\n│ ├ moai_z[h=1,w=2,c=1][3]\n│ └ moai_z[h=1,w=2,c=1][4]\n└ constraints [18]\n  ├ -x[1,1] + moai_MaxPool2d[1] ≥ 0\n  ├ -x[1,1] + moai_MaxPool2d[1] - 100 moai_z[h=1,w=1,c=1][1] ≤ 0\n  ├ -x[1,2] + moai_MaxPool2d[1] ≥ 0\n  ├ -x[1,2] + moai_MaxPool2d[1] - 100 moai_z[h=1,w=1,c=1][3] ≤ 0\n  ├ -x[2,1] + moai_MaxPool2d[1] ≥ 0\n  ├ -x[2,1] + moai_MaxPool2d[1] - 100 moai_z[h=1,w=1,c=1][2] ≤ 0\n  ├ -x[2,2] + moai_MaxPool2d[1] ≥ 0\n  ├ -x[2,2] + moai_MaxPool2d[1] - 100 moai_z[h=1,w=1,c=1][4] ≤ 0\n  ├ moai_z[h=1,w=1,c=1][1] + moai_z[h=1,w=1,c=1][2] + moai_z[h=1,w=1,c=1][3] + moai_z[h=1,w=1,c=1][4] = 3\n  ├ -x[1,3] + moai_MaxPool2d[2] ≥ 0\n  ├ -x[1,3] + moai_MaxPool2d[2] - 100 moai_z[h=1,w=2,c=1][1] ≤ 0\n  ├ -x[1,4] + moai_MaxPool2d[2] ≥ 0\n  ├ -x[1,4] + moai_MaxPool2d[2] - 100 moai_z[h=1,w=2,c=1][3] ≤ 0\n  ├ -x[2,3] + moai_MaxPool2d[2] ≥ 0\n  ├ -x[2,3] + moai_MaxPool2d[2] - 100 moai_z[h=1,w=2,c=1][2] ≤ 0\n  ├ -x[2,4] + moai_MaxPool2d[2] ≥ 0\n  ├ -x[2,4] + moai_MaxPool2d[2] - 100 moai_z[h=1,w=2,c=1][4] ≤ 0\n  └ moai_z[h=1,w=2,c=1][1] + moai_z[h=1,w=2,c=1][2] + moai_z[h=1,w=2,c=1][3] + moai_z[h=1,w=2,c=1][4] = 3\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Permutation","page":"API Reference","title":"MathOptAI.Permutation","text":"Permutation(p::Vector{Int}) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = xp\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.Permutation([2, 1])\nPermutation([2, 1])\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, f, x; reduced_space = true);\n\njulia> y\n2-element Vector{VariableRef}:\n x[2]\n x[1]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Pipeline","page":"API Reference","title":"MathOptAI.Pipeline","text":"Pipeline(layers::Vector{AbstractPredictor}) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = (l_1 circ ldots circ l_N)(x)\n\nwhere l_i are a list of other AbstractPredictors.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.Pipeline(\n           MathOptAI.Affine([1.0 2.0], [0.0]),\n           MathOptAI.ReLUQuadratic(),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 2, output: 1]\n * ReLUQuadratic(nothing)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_ReLU[1]\n\njulia> formulation\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [1]\n  └ x[1] + 2 x[2] - moai_Affine[1] = 0\nReLUQuadratic(nothing)\n├ variables [2]\n│ ├ moai_ReLU[1]\n│ └ moai_z[1]\n└ constraints [4]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_z[1] ≥ 0\n  ├ moai_Affine[1] - moai_ReLU[1] + moai_z[1] = 0\n  └ moai_ReLU[1]*moai_z[1] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.PytorchModel","page":"API Reference","title":"MathOptAI.PytorchModel","text":"PytorchModel(filename::String)\n\nA wrapper struct for loading a PyTorch model.\n\nThe only supported file extension is .pt, where the .pt file has been created using torch.save(model, filename).\n\nwarning: Warning\nTo use PytorchModel, your code must load the PythonCall package:import PythonCallSee the Python integration section of the documentation for instructions on how to link Python to Julia.\n\nExample\n\njulia> using MathOptAI\n\njulia> using PythonCall  #  This line is important!\n\njulia> predictor = PytorchModel(\"model.pt\");\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Quantile","page":"API Reference","title":"MathOptAI.Quantile","text":"Quantile{D}(distribution::D, quantiles::Vector{Float64}) where {D}\n\nAn AbstractPredictor that represents the quantiles of distribution.\n\nExample\n\njulia> using JuMP, Distributions, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, 1 <= x <= 2);\n\njulia> predictor = MathOptAI.Quantile([0.1, 0.9]) do x\n           return Distributions.Normal(x, 3 - x)\n       end\nQuantile(_, [0.1, 0.9])\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, [x]);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_quantile[1]\n moai_quantile[2]\n\njulia> formulation\nQuantile(_, [0.1, 0.9])\n├ variables [2]\n│ ├ moai_quantile[1]\n│ └ moai_quantile[2]\n└ constraints [2]\n  ├ moai_quantile[1] - op_quantile_0.1(x) = 0\n  └ moai_quantile[2] - op_quantile_0.9(x) = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.ReducedSpace","page":"API Reference","title":"MathOptAI.ReducedSpace","text":"ReducedSpace(predictor::AbstractPredictor)\n\nA wrapper type for other predictors that implement a reduced-space formulation.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> predictor = MathOptAI.ReducedSpace(MathOptAI.ReLU());\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n max(0, x[1])\n max(0, x[2])\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.ReLU","page":"API Reference","title":"MathOptAI.ReLU","text":"ReLU() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nas a non-smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLU()\nReLU()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLU()\n├ variables [2]\n│ ├ moai_ReLU[1]\n│ └ moai_ReLU[2]\n└ constraints [6]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_ReLU[1] - max(0, x[1]) = 0\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  └ moai_ReLU[2] - max(0, x[2]) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n max(0, x[1])\n max(0, x[2])\n\njulia> formulation\nReducedSpace(ReLU())\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.ReLUBigM","page":"API Reference","title":"MathOptAI.ReLUBigM","text":"ReLUBigM(M::Float64) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nvia the big-M MIP reformulation:\n\nbeginaligned\ny ge 0            \ny ge x            \ny le M z          \ny le x + M(1 - z) \nz in0 1\nendaligned\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -3 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLUBigM(100.0)\nReLUBigM(100.0)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLUBigM(100.0)\n├ variables [4]\n│ ├ moai_ReLU[1]\n│ ├ moai_ReLU[2]\n│ ├ moai_z[1]\n│ └ moai_z[2]\n└ constraints [12]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_z[1] binary\n  ├ -x[1] + moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] - moai_z[1] ≤ 0\n  ├ -x[1] + moai_ReLU[1] + 3 moai_z[1] ≤ 3\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  ├ moai_z[2] binary\n  ├ -x[2] + moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] - 2 moai_z[2] ≤ 0\n  └ -x[2] + moai_ReLU[2] + 3 moai_z[2] ≤ 3\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.ReLUQuadratic","page":"API Reference","title":"MathOptAI.ReLUQuadratic","text":"ReLUQuadratic(; relaxation_parameter = nothing) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nby the reformulation:\n\nbeginaligned\nx = y - z \ny cdot z = 0 \ny z ge 0\nendaligned\n\nIf relaxation_parameter is set to a value ϵ, the constraints become:\n\nbeginaligned\nx = y - z \ny cdot z leq epsilon \ny z ge 0\nendaligned\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLUQuadratic()\nReLUQuadratic(nothing)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLUQuadratic(nothing)\n├ variables [4]\n│ ├ moai_ReLU[1]\n│ ├ moai_ReLU[2]\n│ ├ moai_z[1]\n│ └ moai_z[2]\n└ constraints [12]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_z[1] ≥ 0\n  ├ moai_z[1] ≤ 1\n  ├ x[1] - moai_ReLU[1] + moai_z[1] = 0\n  ├ moai_ReLU[1]*moai_z[1] = 0\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  ├ moai_z[2] ≥ 0\n  ├ moai_z[2] ≤ 1\n  ├ x[2] - moai_ReLU[2] + moai_z[2] = 0\n  └ moai_ReLU[2]*moai_z[2] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.ReLUSOS1","page":"API Reference","title":"MathOptAI.ReLUSOS1","text":"ReLUSOS1() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nby the reformulation:\n\nbeginaligned\nx = y - z           \ny z in SOS1    \ny z ge 0\nendaligned\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLUSOS1()\nReLUSOS1()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLUSOS1()\n├ variables [4]\n│ ├ moai_ReLU[1]\n│ ├ moai_ReLU[2]\n│ ├ moai_z[1]\n│ └ moai_z[2]\n└ constraints [12]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_z[1] ≥ 0\n  ├ moai_z[1] ≤ 1\n  ├ x[1] - moai_ReLU[1] + moai_z[1] = 0\n  ├ [moai_ReLU[1], moai_z[1]] ∈ MathOptInterface.SOS1{Float64}([1.0, 2.0])\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  ├ moai_z[2] ≥ 0\n  ├ moai_z[2] ≤ 1\n  ├ x[2] - moai_ReLU[2] + moai_z[2] = 0\n  └ [moai_ReLU[2], moai_z[2]] ∈ MathOptInterface.SOS1{Float64}([1.0, 2.0])\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Scale","page":"API Reference","title":"MathOptAI.Scale","text":"Scale(\n    scale::Vector{T},\n    bias::Vector{T},\n) where {T} <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = Diag(scale)x + bias\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Scale([2.0, 3.0], [4.0, 5.0])\nScale(scale, bias)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_Scale[1]\n moai_Scale[2]\n\njulia> formulation\nScale(scale, bias)\n├ variables [2]\n│ ├ moai_Scale[1]\n│ └ moai_Scale[2]\n└ constraints [6]\n  ├ moai_Scale[1] ≥ 4\n  ├ moai_Scale[1] ≤ 6\n  ├ moai_Scale[2] ≥ 5\n  ├ moai_Scale[2] ≤ 11\n  ├ 2 x[1] - moai_Scale[1] = -4\n  └ 3 x[2] - moai_Scale[2] = -5\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{AffExpr}:\n 2 x[1] + 4\n 3 x[2] + 5\n\njulia> formulation\nReducedSpace(Scale(scale, bias))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Sigmoid","page":"API Reference","title":"MathOptAI.Sigmoid","text":"Sigmoid() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = frac11 + e^-x\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Sigmoid()\nSigmoid()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_Sigmoid[1]\n moai_Sigmoid[2]\n\njulia> formulation\nSigmoid()\n├ variables [2]\n│ ├ moai_Sigmoid[1]\n│ └ moai_Sigmoid[2]\n└ constraints [6]\n  ├ moai_Sigmoid[1] ≥ 0.2689414213699951\n  ├ moai_Sigmoid[1] ≤ 0.7310585786300049\n  ├ moai_Sigmoid[1] - (1 / (1 + exp(-x[1]))) = 0\n  ├ moai_Sigmoid[2] ≥ 0.2689414213699951\n  ├ moai_Sigmoid[2] ≤ 0.8807970779778823\n  └ moai_Sigmoid[2] - (1 / (1 + exp(-x[2]))) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n 1 / (1 + exp(-x[1]))\n 1 / (1 + exp(-x[2]))\n\njulia> formulation\nReducedSpace(Sigmoid())\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.SoftMax","page":"API Reference","title":"MathOptAI.SoftMax","text":"SoftMax() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = frace^xe^x_1\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.SoftMax()\nSoftMax()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_SoftMax[1]\n moai_SoftMax[2]\n\njulia> formulation\nSoftMax()\n├ variables [3]\n│ ├ moai_SoftMax_denom[1]\n│ ├ moai_SoftMax[1]\n│ └ moai_SoftMax[2]\n└ constraints [8]\n  ├ moai_SoftMax_denom[1] ≥ 0\n  ├ moai_SoftMax_denom[1] - (0 + exp(x[2]) + exp(x[1])) = 0\n  ├ moai_SoftMax[1] ≥ 0\n  ├ moai_SoftMax[1] ≤ 1\n  ├ moai_SoftMax[1] - (exp(x[1]) / moai_SoftMax_denom[1]) = 0\n  ├ moai_SoftMax[2] ≥ 0\n  ├ moai_SoftMax[2] ≤ 1\n  └ moai_SoftMax[2] - (exp(x[2]) / moai_SoftMax_denom[1]) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n exp(x[1]) / moai_SoftMax_denom[1]\n exp(x[2]) / moai_SoftMax_denom[1]\n\njulia> formulation\nReducedSpace(SoftMax())\n├ variables [1]\n│ └ moai_SoftMax_denom[1]\n└ constraints [2]\n  ├ moai_SoftMax_denom[1] ≥ 0\n  └ moai_SoftMax_denom[1] - (0 + exp(x[2]) + exp(x[1])) = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.SoftPlus","page":"API Reference","title":"MathOptAI.SoftPlus","text":"SoftPlus(; beta = 1.0) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = frac1beta log(1 + e^beta x)\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.SoftPlus(; beta = 2.0)\nSoftPlus(2.0)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_SoftPlus[1]\n moai_SoftPlus[2]\n\njulia> formulation\nSoftPlus(2.0)\n├ variables [2]\n│ ├ moai_SoftPlus[1]\n│ └ moai_SoftPlus[2]\n└ constraints [6]\n  ├ moai_SoftPlus[1] ≥ 0.0634640055214863\n  ├ moai_SoftPlus[1] ≤ 1.0634640055214863\n  ├ moai_SoftPlus[1] - (log(1 + exp(2 x[1])) / 2) = 0\n  ├ moai_SoftPlus[2] ≥ 0.0634640055214863\n  ├ moai_SoftPlus[2] ≤ 2.0090749639589047\n  └ moai_SoftPlus[2] - (log(1 + exp(2 x[2])) / 2) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n log(1 + exp(2 x[1])) / 2\n log(1 + exp(2 x[2])) / 2\n\njulia> formulation\nReducedSpace(SoftPlus(2.0))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.TAGConv","page":"API Reference","title":"MathOptAI.TAGConv","text":"TAGConv(;\n    weights::Vector{Matrix{T}},\n    bias::Vector{T},\n    edge_index::Vector{Pair{Int,Int}},\n)\n\nAn AbstractPredictor that represents a topology adaptive graph convolutional network operator:\n\nY = sumlimits_k=0^K (D^-12 A D^-12)^k X W_k + b\n\nwhere:\n\nW_k is weights[k+1]\nA is the adjacency matrix constructed from edge_index\nb is the bias vector bias\nD is the diagonal degree matrix.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:3, 1:2]);\n\njulia> f = MathOptAI.TAGConv(;\n           weights = Matrix{Float64}[[1 2]', [3 4]', [5 6]'],\n           bias = [7.0],\n           edge_index = [1 => 2, 2 => 1, 2 => 3, 3 => 2],\n       );\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, vec(x));\n\njulia> y\n3-element Vector{VariableRef}:\n moai_TAGConv[1]\n moai_TAGConv[2]\n moai_TAGConv[3]\n\njulia> formulation\nTAGConv{Float64}([[1.0; 2.0;;], [3.0; 4.0;;], [5.0; 6.0;;]], [7.0], [0.0 0.7071067811865475 0.0; 0.7071067811865475 0.0 0.7071067811865475; 0.0 0.7071067811865475 0.0])\n├ variables [3]\n│ ├ moai_TAGConv[1]\n│ ├ moai_TAGConv[2]\n│ └ moai_TAGConv[3]\n└ constraints [3]\n  ├ -3.4999999999999996 x[1,1] - 2.1213203435596424 x[2,1] - 2.4999999999999996 x[3,1] - 4.999999999999999 x[1,2] - 2.82842712474619 x[2,2] - 2.999999999999999 x[3,2] + moai_TAGConv[1] = 7\n  ├ -2.1213203435596424 x[1,1] - 5.999999999999999 x[2,1] - 2.1213203435596424 x[3,1] - 2.82842712474619 x[1,2] - 7.999999999999998 x[2,2] - 2.82842712474619 x[3,2] + moai_TAGConv[2] = 7\n  └ -2.4999999999999996 x[1,1] - 2.1213203435596424 x[2,1] - 3.4999999999999996 x[3,1] - 2.999999999999999 x[1,2] - 2.82842712474619 x[2,2] - 4.999999999999999 x[3,2] + moai_TAGConv[3] = 7\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Tanh","page":"API Reference","title":"MathOptAI.Tanh","text":"Tanh() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = tanh(x)\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Tanh()\nTanh()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_Tanh[1]\n moai_Tanh[2]\n\njulia> formulation\nTanh()\n├ variables [2]\n│ ├ moai_Tanh[1]\n│ └ moai_Tanh[2]\n└ constraints [6]\n  ├ moai_Tanh[1] ≥ -0.7615941559557649\n  ├ moai_Tanh[1] ≤ 0.7615941559557649\n  ├ moai_Tanh[1] - tanh(x[1]) = 0\n  ├ moai_Tanh[2] ≥ -0.7615941559557649\n  ├ moai_Tanh[2] ≤ 0.9640275800758169\n  └ moai_Tanh[2] - tanh(x[2]) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n tanh(x[1])\n tanh(x[2])\n\njulia> formulation\nReducedSpace(Tanh())\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.AbstractFormulation","page":"API Reference","title":"MathOptAI.AbstractFormulation","text":"abstract type AbstractFormulation end\n\nAn abstract type representing different formulations.\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.Formulation","page":"API Reference","title":"MathOptAI.Formulation","text":"struct Formulation{P<:AbstractPredictor} <: AbstractFormulation\n    predictor::P\n    variables::Vector{Any}\n    constraints::Vector{Any}\nend\n\nFields\n\npredictor: the predictor object used to build the formulation\nvariables: a vector of new decision variables added to the model\nconstraints: a vector of new constraints added to the model\n\nCheck the docstring of the predictor for an explanation of the formulation and the order of the elements in .variables and .constraints.\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.PipelineFormulation","page":"API Reference","title":"MathOptAI.PipelineFormulation","text":"struct PipelineFormulation{P<:AbstractPredictor} <: AbstractFormulation\n    predictor::P\n    layers::Vector{Any}\nend\n\nFields\n\npredictor: the predictor object used to build the formulation\nlayers: the formulation associated with each of the layers in the pipeline\n\n\n\n\n\n","category":"type"},{"location":"api/#MathOptAI.add_predictor-Tuple{JuMP.AbstractModel, MathOptAI.Quantile{<:AbstractGPs.PosteriorGP}, Vector}","page":"API Reference","title":"MathOptAI.add_predictor","text":"MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::MathOptAI.Quantile{<:AbstractGPs.PosteriorGP},\n    x::Vector,\n)\n\nAdd the quantiles of a trained Gaussian Process from AbstractGPs.jl to model.\n\nExample\n\njulia> using JuMP, MathOptAI, AbstractGPs\n\njulia> x_data = 2π .* (0.0:0.1:1.0);\n\njulia> y_data = sin.(x_data);\n\njulia> fx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.1);\n\njulia> p_fx = AbstractGPs.posterior(fx, y_data);\n\njulia> model = Model();\n\njulia> @variable(model, 1 <= x[1:1] <= 6, start = 3);\n\njulia> predictor = MathOptAI.Quantile(p_fx, [0.1, 0.9]);\n\njulia> y, _ = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_quantile[1]\n moai_quantile[2]\n\njulia> @objective(model, Max, y[2] - y[1])\nmoai_quantile[2] - moai_quantile[1]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Tuple{Union{DecisionTree.DecisionTreeClassifier, DecisionTree.Ensemble, DecisionTree.Leaf, DecisionTree.Node, DecisionTree.Root}}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::Union{\n        DecisionTree.Ensemble,\n        DecisionTree.DecisionTreeClassifier,\n        DecisionTree.Leaf,\n        DecisionTree.Node,\n        DecisionTree.Root,\n    },\n)\n\nConvert a binary decision tree from DecisionTree.jl to a BinaryDecisionTree.\n\nExample\n\njulia> using JuMP, MathOptAI, DecisionTree\n\njulia> truth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\ntruth (generic function with 1 method)\n\njulia> features = abs.(sin.((1:10) .* (3:4)'));\n\njulia> size(features)\n(10, 2)\n\njulia> labels = truth.(Vector.(eachrow(features)));\n\njulia> tree = DecisionTree.build_tree(labels, features)\nDecision Tree\nLeaves: 3\nDepth:  2\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[1:2] <= 1);\n\njulia> y, _ = MathOptAI.add_predictor(model, tree, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_BinaryDecisionTree_value[1]\n\njulia> MathOptAI.build_predictor(tree)\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Union{Tuple{EvoTrees.EvoTree{L, 1}}, Tuple{L}} where L","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(predictor::EvoTrees.EvoTree{L,1}) where {L}\n\nConvert a boosted tree from EvoTrees.jl to an AffineCombination of BinaryDecisionTree.\n\nExample\n\njulia> using JuMP, MathOptAI, EvoTrees\n\njulia> truth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\ntruth (generic function with 1 method)\n\njulia> x_train = abs.(sin.((1:10) .* (3:4)'));\n\njulia> size(x_train)\n(10, 2)\n\njulia> y_train = truth.(Vector.(eachrow(x_train)));\n\njulia> config = EvoTrees.EvoTreeRegressor(; nrounds = 3);\n\njulia> tree = EvoTrees.fit(config; x_train, y_train);\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[1:2] <= 1);\n\njulia> y, _ = MathOptAI.add_predictor(model, tree, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_AffineCombination[1]\n\njulia> MathOptAI.build_predictor(tree)\nAffineCombination\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n└ 1.0 * [2.0]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Tuple{Flux.Chain}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::Flux.Chain;\n    config::Dict = Dict{Any,Any}(),\n    gray_box::Bool = false,\n    hessian::Bool = gray_box,\n    input_size::Union{Nothing,NTuple{N,Int}} = nothing,\n)\n\nConvert a trained neural network from Flux.jl to a Pipeline.\n\nSupported layers\n\nFlux.Conv\nFlux.Dense\nFlux.flatten\nFlux.LayerNorm\nFlux.MaxPool\nFlux.MeanPool\nFlux.Scale\nFlux.softmax\n\nSupported activation functions\n\nFlux.relu\nFlux.sigmoid\nFlux.softplus\nFlux.tanh\n\nKeyword arguments\n\nconfig: see the Config section below.\ngray_box: if true, the neural network is added using a GrayBox formulation.\nhessian: if true, the gray_box formulations compute the Hessian of the output using Flux.hessian. The default for hessian is true if gray_box is used.\ninput_size: to disambiguate the input and output sizes of matrix inputs, chains containing Conv, LayerNorm, MaxPool, and MeanPool layers must specify an initial input size.\n\nConfig\n\nThe config dictionary controls how layers in Flux are mapped to AbstractPredictors.\n\nSupported keys and and example key-value pairs are:\n\nFlux.MaxPool => (k; kwargs...) -> MathOptAI.MaxPool2dBigM(k; M = 10.0, kwargs...)\nFlux.relu => MathOptAI.ReLU\nFlux.sigmoid => MathOptAI.Sigmoid\nFlux.softmax => MathOptAI.SoftMax\nFlux.softplus => MathOptAI.SoftPlus\nFlux.tanh => MathOptAI.Tanh\n\nExample\n\njulia> using JuMP, MathOptAI, Flux\n\njulia> chain = Flux.Chain(Flux.Dense(1 => 16, Flux.relu), Flux.Dense(16 => 1));\n\njulia> model = Model();\n\njulia> @variable(model, x[1:1]);\n\njulia> y, _ = MathOptAI.add_predictor(\n           model,\n           chain,\n           x;\n           config = Dict(Flux.relu => MathOptAI.ReLU),\n       );\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> MathOptAI.build_predictor(\n           chain;\n           config = Dict(Flux.relu => MathOptAI.ReLU),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLU()\n * Affine(A, b) [input: 16, output: 1]\n\njulia> MathOptAI.build_predictor(\n           chain;\n           config = Dict(Flux.relu => MathOptAI.ReLUQuadratic),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLUQuadratic(nothing)\n * Affine(A, b) [input: 16, output: 1]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Tuple{GLM.GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Distributions.Bernoulli{Float64}, GLM.LogitLink}}}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::GLM.GeneralizedLinearModel{\n        GLM.GlmResp{Vector{Float64},GLM.Bernoulli{Float64},GLM.LogitLink},\n    };\n    sigmoid::MathOptAI.AbstractPredictor = MathOptAI.Sigmoid(),\n)\n\nConvert a trained logistic model from GLM.jl to a Pipeline layer.\n\nKeyword arguments\n\nsigmoid: the predictor to use for the sigmoid layer.\n\nExample\n\njulia> using JuMP, MathOptAI, GLM\n\njulia> X, Y = rand(10, 2), rand(Bool, 10);\n\njulia> predictor = GLM.glm(X, Y, GLM.Bernoulli());\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> y, _ = MathOptAI.add_predictor(\n           model,\n           predictor,\n           x;\n           sigmoid = MathOptAI.Sigmoid(),\n       );\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Sigmoid[1]\n\njulia> MathOptAI.build_predictor(predictor)\nPipeline with layers:\n * Affine(A, b) [input: 2, output: 1]\n * Sigmoid()\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Tuple{GLM.LinearModel}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(predictor::GLM.LinearModel)\n\nConvert a trained linear model from GLM.jl to an Affine layer.\n\nExample\n\njulia> using JuMP, MathOptAI, GLM\n\njulia> X, Y = rand(10, 2), rand(10);\n\njulia> predictor = GLM.lm(X, Y);\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> y, _ = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> MathOptAI.build_predictor(predictor)\nAffine(A, b) [input: 2, output: 1]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Tuple{Tuple{Lux.Chain, NamedTuple, NamedTuple}}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::Tuple{<:Lux.Chain,<:NamedTuple,<:NamedTuple};\n    config::Dict = Dict{Any,Any}(),\n)\n\nConvert a trained neural network from Lux.jl to a Pipeline.\n\nSupported layers\n\nLux.Dense\nLux.Scale\n\nSupported activation functions\n\nLux.relu\nLux.sigmoid\nLux.softplus\nLux.softmax\nLux.tanh\n\nKeyword arguments\n\nconfig: see the Config section below.\n\nConfig\n\nThe config dictionary controls how layers in Flux are mapped to AbstractPredictors.\n\nSupported keys and and example key-value pairs are:\n\nLux.relu => MathOptAI.ReLU\nLux.sigmoid => MathOptAI.Sigmoid\nLux.sigmoid_fast => MathOptAI.Sigmoid\nLux.softmax => MathOptAI.SoftMax\nLux.softplus => MathOptAI.SoftPlus\nLux.tanh => MathOptAI.Tanh\nLux.tanh_fast => MathOptAI.Tanh\n\nExample\n\njulia> using JuMP, MathOptAI, Lux, Random\n\njulia> rng = Random.MersenneTwister();\n\njulia> chain = Lux.Chain(Lux.Dense(1 => 16, Lux.relu), Lux.Dense(16 => 1))\nChain(\n    layer_1 = Dense(1 => 16, relu),               # 32 parameters\n    layer_2 = Dense(16 => 1),                     # 17 parameters\n)         # Total: 49 parameters,\n          #        plus 0 states.\n\njulia> parameters, state = Lux.setup(rng, chain);\n\njulia> model = Model();\n\njulia> @variable(model, x[1:1]);\n\njulia> y, _ = MathOptAI.add_predictor(\n           model,\n           (chain, parameters, state),\n           x;\n           config = Dict(Lux.relu => MathOptAI.ReLU),\n       );\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> MathOptAI.build_predictor(\n           (chain, parameters, state);\n           config = Dict(Lux.relu => MathOptAI.ReLU),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLU()\n * Affine(A, b) [input: 16, output: 1]\n\njulia> MathOptAI.build_predictor(\n           (chain, parameters, state);\n           config = Dict(Lux.relu => MathOptAI.ReLUQuadratic),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLUQuadratic(nothing)\n * Affine(A, b) [input: 16, output: 1]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.GCNConv-Tuple{PythonCall.Py}","page":"API Reference","title":"MathOptAI.GCNConv","text":"MathOptAI.GCNConv(\n    layer::PythonCall.Py;\n    edge_index::Vector{Pair{Int,Int}},\n)\n\nCreate a GCNConv layer from a torch_geometric.nn.GCNConv layer.\n\nSee the Graph neural networks tutorial for details.\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.TAGConv-Tuple{PythonCall.Py}","page":"API Reference","title":"MathOptAI.TAGConv","text":"MathOptAI.TAGConv(\n    layer::PythonCall.Py;\n    edge_index::Vector{Pair{Int,Int}},\n)\n\nCreate a TAGConv layer from a torch_geometric.nn.TAGConv layer.\n\nSee the Graph neural networks tutorial for details.\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Tuple{MathOptAI.PytorchModel}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::MathOptAI.PytorchModel;\n    config::Dict = Dict{Any,Any}(),\n    gray_box::Bool = false,\n    hessian::Bool = gray_box,\n    device::String = \"cpu\",\n    input_size::Union{Nothing,NTuple{N,Int}} = nothing,\n)\n\nConvert a trained neural network from PyTorch via PythonCall.jl to a Pipeline.\n\nSupported layers\n\nnn.AvgPool2d\nnn.Conv2d\nnn.Dropout\nnn.Flatten\nnn.GELU\nnn.LayerNorm\nnn.LeakyReLU\nnn.Linear\nnn.MaxPool2d\nnn.ReLU\nnn.Sequential\nnn.Sigmoid\nnn.Softmax\nnn.Softplus\nnn.Tanh\n\nNote that nn.Dropout layers are skipped because we assume that the PyTorch model is being evaluated, not trained.\n\nKeyword arguments\n\nconfig: see the Config section below.\ngray_box: if true, the neural network is added using a GrayBox formulation.\nhessian: if true, the gray_box formulation computes the Hessian of the output using torch.func.hessian. The default for hessian is true if gray_box is used.\ndevice: device used to construct PyTorch tensors, for example, \"cuda\" to run on an Nvidia GPU.\ninput_size: to disambiguate the input and output sizes of matrix inputs, models containing AvgPool2d, Conv2d, and MaxPool2d layers must specify an initial input size.\n\nConfig\n\nThe config dictionary controls how layers in PyTorch are mapped to AbstractPredictors.\n\nSupported keys and and example key-value pairs are:\n\n:GELU => MathOptAI.GELU\n:MaxPool2d => (k; kwargs...) -> MathOptAI.MaxPool2dBigM(k; M = 10.0, kwargs...)\n:ReLU => MathOptAI.ReLU\n:Sigmoid => MathOptAI.Sigmoid\n:SoftMax => MathOptAI.SoftMax\n:SoftPlus => (; beta) -> MathOptAI.SoftPlus(; beta)\n:Tanh => MathOptAI.Tanh\n\nNote that :LeakyReLU is not supported. Use :ReLU to control how the inner ReLU is modeled.\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.add_predictor-Tuple{JuMP.AbstractModel, StatsModels.TableRegressionModel, DataFrames.DataFrame}","page":"API Reference","title":"MathOptAI.add_predictor","text":"MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::StatsModels.TableRegressionModel,\n    x::DataFrames.DataFrame;\n    kwargs...,\n)\n\nAdd a trained regression model from StatsModels.jl to model, using the DataFrame x as input.\n\nIn most cases, predictor should be a GLM.jl predictor supported by MathOptAI, but trained using @formula and a DataFrame instead of the raw matrix input.\n\nIn general, x may have some columns that are constant (Float64) and some columns that are JuMP decision variables.\n\nKeyword arguments\n\nAll keyword arguments are passed to the corresponding add_predictor of the GLM extension.\n\nExample\n\njulia> using DataFrames, GLM, JuMP, MathOptAI\n\njulia> train_df = DataFrames.DataFrame(x1 = rand(10), x2 = rand(10));\n\njulia> train_df.y = 1.0 .* train_df.x1 + 2.0 .* train_df.x2 .+ rand(10);\n\njulia> predictor = GLM.lm(GLM.@formula(y ~ x1 + x2), train_df);\n\njulia> model = Model();\n\njulia> test_df = DataFrames.DataFrame(\n           x1 = rand(6),\n           x2 = @variable(model, [1:6]),\n       );\n\njulia> test_df.y, _ = MathOptAI.add_predictor(model, predictor, test_df);\n\njulia> test_df.y\n6-element Vector{VariableRef}:\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.add_variables","page":"API Reference","title":"MathOptAI.add_variables","text":"add_variables(\n    model::JuMP.AbstractModel,\n    x::Vector,\n    n::Int,\n    base_name::String,\n)::Vector\n\nAdd a vector of n variables to model with the base name base_name.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of model  and x as needed.\n\nThe default method is:\n\nfunction add_variables(\n    model::JuMP.AbstractModel,\n    x::Vector,\n    n::Int,\n    base_name::String,\n)\n    return JuMP.@variable(model, [1:n], base_name = base_name)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.get_variable_bounds","page":"API Reference","title":"MathOptAI.get_variable_bounds","text":"get_variable_bounds(x::JuMP.AbstractVariableRef)\n\nReturn a tuple corresponding to the (lower, upper) variable bounds of x.\n\nIf there is no bound, the value returned is missing.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x as needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.set_variable_bounds","page":"API Reference","title":"MathOptAI.set_variable_bounds","text":"set_variable_bounds(\n    cons::Vector{Any},\n    x::JuMP.AbstractVariableRef,\n    l::Any,\n    u::Any;\n    optional::Bool,\n)\n\nSet the bounds on x to l and u, and push! their corresponding constraint references to cons.\n\nIf l or u are missing, do not set the bound.\n\nIf optional = true, you may choose to silently skip setting the bounds because they are not required for correctness.\n\nThe type of l and u depends on get_variable_bounds.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x as needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.get_variable_start","page":"API Reference","title":"MathOptAI.get_variable_start","text":"get_variable_start(x::JuMP.AbstractVariableRef)\n\nGet the primal starting value of x, or return missing if one is not set.\n\nThe return value of this function is propogated through the various AbstractPredictors, and the primal start of new output variables is set using set_variable_start.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x as needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.set_variable_start","page":"API Reference","title":"MathOptAI.set_variable_start","text":"set_variable_start(x::JuMP.AbstractVariableRef, start::Any)\n\nSet the primal starting value of x to start, or do nothing if start is missing.\n\nThe input value start of this function is computed by propogating the primal start of the input variables (obtained with get_variable_start) through the various AbstractPredictors.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x and start as needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.replace_weights_with_variables","page":"API Reference","title":"MathOptAI.replace_weights_with_variables","text":"replace_weights_with_variables(\n    model::JuMP.AbstractModel,\n    predictor::AbstractPredictor,\n)\n\nConvert predictor with trained weights into a predictor in which the weights are JuMP decision variables.\n\nThis function is useful when you wish to use constrained optimization to train small to moderate neural networks.\n\nwarning: Warning\nThis function is experimental and it may change in any future release. If you use this feature, please open a GitHub issue and let us know your thoughts.\n\nExample\n\njulia> using JuMP, Flux, MathOptAI\n\njulia> chain = Flux.Chain(Flux.Dense(2 => 3), Flux.softmax);\n\njulia> model = Model();\n\njulia> @variable(model, x[i in 1:2] == i);\n\njulia> predictor = MathOptAI.build_predictor(chain)\nPipeline with layers:\n * Affine(A, b) [input: 2, output: 3]\n * SoftMax()\n\njulia> predictor = MathOptAI.replace_weights_with_variables(model, predictor)\nPipeline with layers:\n * Affine(A, b) [input: 2, output: 3]\n * SoftMax()\n\njulia> y, _ = MathOptAI.add_predictor(model, predictor, x);\n\n\n\n\n\n","category":"function"},{"location":"manual/AbstractGPs/#AbstractGPs.jl","page":"AbstractGPs.jl","title":"AbstractGPs.jl","text":"AbstractGPs.jl is a library for fitting Gaussian Processes in Julia.","category":"section"},{"location":"manual/AbstractGPs/#Basic-example","page":"AbstractGPs.jl","title":"Basic example","text":"Here is an example:\n\nusing JuMP, MathOptAI, AbstractGPs\nx_data = 2π .* (0.0:0.1:1.0);\ny_data = sin.(x_data);\nfx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.1);\np_fx = AbstractGPs.posterior(fx, y_data);\nmodel = Model();\n@variable(model, 1 <= x[1:1] <= 6, start = 3);\npredictor = MathOptAI.Quantile(p_fx, [0.1, 0.9]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation\n@objective(model, Max, y[2] - y[1])","category":"section"},{"location":"tutorials/pytorch/#Function-fitting-with-PyTorch","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"The purpose of this tutorial is to explain how to embed a neural network model from PyTorch into JuMP.\n\ninfo: Info\nTo use PyTorch from MathOptAI, you must first follow the Python integration instructions.","category":"section"},{"location":"tutorials/pytorch/#Required-packages","page":"Function fitting with PyTorch","title":"Required packages","text":"This tutorial requires the following packages\n\nusing JuMP\nusing Test\nimport Ipopt\nimport MathOptAI\nimport Plots\nimport PythonCall","category":"section"},{"location":"tutorials/pytorch/#Training-a-model","page":"Function fitting with PyTorch","title":"Training a model","text":"The following script builds and trains a simple neural network in PyTorch. For simplicity, we do not evaluate out-of-sample test performance, or use a batched data loader. In general, you should train your model in Python, and then use torch.save(model, filename) to save it to a .pt file for later use in Julia.\n\nThe model is unimportant, but for this example, we are trying to fit noisy observations of the function f(x) = x^2 - 2x.\n\nIn Python, we ran:\n\n#!/usr/bin/python3\nimport torch\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 16),\n    torch.nn.ReLU(),\n    torch.nn.Linear(16, 1),\n)\n\nn = 1024\nx = torch.arange(-2, 2 + 4 / (n - 1), 4 / (n - 1)).reshape(n, 1)\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nfor epoch in range(100):\n    optimizer.zero_grad()\n    N = torch.normal(torch.zeros(n, 1), torch.ones(n, 1))\n    y = x ** 2 -2 * x + 0.1 * N\n    loss = loss_fn(model(x), y)\n    loss.backward()\n    optimizer.step()\n\ntorch.save(model, \"model.pt\")","category":"section"},{"location":"tutorials/pytorch/#JuMP-model","page":"Function fitting with PyTorch","title":"JuMP model","text":"Our goal for this JuMP model is to load the Neural Network from PyTorch into the objective function, and then minimize the objective for different fixed values of x to recreate the function that the Neural Network has learned to approximate.\n\nFirst, create a JuMP model:\n\nmodel = Model(Ipopt.Optimizer)\nset_silent(model)\n@variable(model, x)\n\nThen, load the model from PyTorch using MathOptAI.PytorchModel:\n\npredictor = MathOptAI.PytorchModel(joinpath(@__DIR__, \"model.pt\"))\ny, _ = MathOptAI.add_predictor(model, predictor, [x])\n@objective(model, Min, only(y))\n\nNow, visualize the fitted function y = predictor(x) by repeatedly solving the optimization problem for different fixed values of x:\n\nX, Y = -2:0.1:2, Float64[]\n@constraint(model, c, x == 0.0)\nfor xi in X\n    set_normalized_rhs(c, xi)\n    optimize!(model)\n    @test is_solved_and_feasible(model)\n    push!(Y, objective_value(model))\nend\nPlots.plot(x -> x^2 - 2x, X; label = \"Truth\", linestype = :dot)\nPlots.plot!(X, Y; label = \"Fitted\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"changelog/#Release-notes","page":"Release notes","title":"Release notes","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"section"},{"location":"changelog/#Version-0.2.2-(January-22,-2026)","page":"Release notes","title":"Version 0.2.2 (January 22, 2026)","text":"","category":"section"},{"location":"changelog/#Added","page":"Release notes","title":"Added","text":"Added support for nn.Dropout (#239)\nAdded LayerNorm predictor (#240)\nAdded GCNConv and TAGConv predictors (#244)","category":"section"},{"location":"changelog/#Other","page":"Release notes","title":"Other","text":"Updated printing in docstrings for JuMP@1.29.4 (#241)\nAdded checklists to docs (#242)","category":"section"},{"location":"changelog/#Version-0.2.1-(January-15,-2026)","page":"Release notes","title":"Version 0.2.1 (January 15, 2026)","text":"","category":"section"},{"location":"changelog/#Added-2","page":"Release notes","title":"Added","text":"Added support for custom layers in PyTorch (#237)","category":"section"},{"location":"changelog/#Version-0.2.0-(January-14,-2026)","page":"Release notes","title":"Version 0.2.0 (January 14, 2026)","text":"This release contains a number of breaking changes. See below for details on how to upgrade from v0.1.19.","category":"section"},{"location":"changelog/#Breaking","page":"Release notes","title":"Breaking","text":"Remove ::Matrix method that maps over columns (#223).\nThis change means you cannot do MathOptAI.add_predictor(model, predictor, x) where x::Matrix and have it return a y::Matrix corresponding to mapping the predictor over the columns of x.\nTo upgrade, replace:\ny, formulation = MathOptAI.add_predictor(model, predictor, x::Matrix)\nwith:\nret = map(1:size(x, 2)) do i\n    return MathOptAI.add_predictor(model, predictor, x[:, i])\nend\nformulation = last.(ret)\ny = reduce(hcat, first.(ret))\nand write additional code as needed to handle the formulation objects.\nReplace GrayBox by VectorNonlinearOracle implementation (#230)\nWe have removed the old GrayBox predictor and replaced it by the VectorNonlinearOracle predictor, which has now been renamed to GrayBox. In addition, the ; vector_nonlinear_oracle = true keyword argument has been removed, and the new ; gray_box = true is equivalent to the old ; vector_nonlinear_oracle = true.\nTo upgrade, replace:\nMathOptAI.add_predictor(model, predictor, x; vector_nonlinear_oracle = true)\nwith:\nMathOptAI.add_predictor(model, predictor, x; gray_box = true)\nChange config dictionary to have functions as values (#233)\nWe have changed how the ; config = Dict() keyword works for the Flux, Lux, and PyTorch extensions. Where previously the values were instantiated AbstractPredictor objects, they must now be constructors that, when called, return an AbstractPredictor object.\nAs an example of upgrading, replace:\n; config = Dict(Flux.relu => MathOptAI.ReLU())\nwith:\n; config = Dict(Flux.relu => MathOptAI.ReLU)\n# or, alternatively\n; config = Dict(Flux.relu => () -> MathOptAI.ReLU())","category":"section"},{"location":"changelog/#Added-3","page":"Release notes","title":"Added","text":"Add LeakyReLU predictor (#218)\nAdd AvgPool2d, Conv2d, and MaxPool2d predictors (#220), (#222), (#224)\nAdd support for custom models in Flux (#227)\nAdd MaxPool2dBigM predictor (#231), (#235)","category":"section"},{"location":"changelog/#Fixed","page":"Release notes","title":"Fixed","text":"Simplify test to fix flakey TestPythonCallExt.test_gelu (#228)","category":"section"},{"location":"changelog/#Other-2","page":"Release notes","title":"Other","text":"Remove comment that vector_nonlinear_oracle is experimental (#219)\nAdded this changelog (#234), (#236)","category":"section"},{"location":"changelog/#Version-0.1.19-(December-5,-2025)","page":"Release notes","title":"Version 0.1.19 (December 5, 2025)","text":"","category":"section"},{"location":"changelog/#Added-4","page":"Release notes","title":"Added","text":"Add replace_weights_with_variables (#215)","category":"section"},{"location":"changelog/#Version-0.1.18-(October-24,-2025)","page":"Release notes","title":"Version 0.1.18 (October 24, 2025)","text":"","category":"section"},{"location":"changelog/#Added-5","page":"Release notes","title":"Added","text":"Propagate start values through layers (#210)","category":"section"},{"location":"changelog/#Other-3","page":"Release notes","title":"Other","text":"Update to JuliaFormatter@2 (#211)\nMake some predictors callable (#212)","category":"section"},{"location":"changelog/#Version-0.1.17-(October-23,-2025)","page":"Release notes","title":"Version 0.1.17 (October 23, 2025)","text":"","category":"section"},{"location":"changelog/#Fixed-2","page":"Release notes","title":"Fixed","text":"Iterate over PyTorch Sequential module itself, rather than its children (#207)","category":"section"},{"location":"changelog/#Version-0.1.16-(October-22,-2025)","page":"Release notes","title":"Version 0.1.16 (October 22, 2025)","text":"","category":"section"},{"location":"changelog/#Added-6","page":"Release notes","title":"Added","text":"Add GELU predictor (#202)\nAdd support for MOI.VectorNonlinearOracle (#204)","category":"section"},{"location":"changelog/#Other-4","page":"Release notes","title":"Other","text":"Clarify the problem class of each predictor (#205)","category":"section"},{"location":"changelog/#Version-0.1.15-(September-30,-2025)","page":"Release notes","title":"Version 0.1.15 (September 30, 2025)","text":"","category":"section"},{"location":"changelog/#Added-7","page":"Release notes","title":"Added","text":"Add extension hooks for InfiniteOpt (#200)","category":"section"},{"location":"changelog/#Other-5","page":"Release notes","title":"Other","text":"Refactor different ReLU into separate files (#199)","category":"section"},{"location":"changelog/#Version-0.1.14-(September-8,-2025)","page":"Release notes","title":"Version 0.1.14 (September 8, 2025)","text":"","category":"section"},{"location":"changelog/#Fixed-3","page":"Release notes","title":"Fixed","text":"Fix binary decision tree by making the tolerance default to 1e-6 (#196)\nFix docs because of a change in Lux (#197)","category":"section"},{"location":"changelog/#Other-6","page":"Release notes","title":"Other","text":"Update README.md with MOAI paper (#194)","category":"section"},{"location":"changelog/#Version-0.1.13-(July-8,-2025)","page":"Release notes","title":"Version 0.1.13 (July 8, 2025)","text":"","category":"section"},{"location":"changelog/#Fixed-4","page":"Release notes","title":"Fixed","text":"Add DimensionMismatch checks for Affine and Scale (#192)","category":"section"},{"location":"changelog/#Version-0.1.12-(June-3,-2025)","page":"Release notes","title":"Version 0.1.12 (June 3, 2025)","text":"","category":"section"},{"location":"changelog/#Added-8","page":"Release notes","title":"Added","text":"Add a reduced_space fallback for extensions (#187)","category":"section"},{"location":"changelog/#Other-7","page":"Release notes","title":"Other","text":"Update test for change in MOI@1.40.2 (#186)\nAdd Softmax to the PyTorch manual page (#188)","category":"section"},{"location":"changelog/#Version-0.1.11-(June-2,-2025)","page":"Release notes","title":"Version 0.1.11 (June 2, 2025)","text":"","category":"section"},{"location":"changelog/#Added-9","page":"Release notes","title":"Added","text":"Add LinearCombination predictor (#181)\nAdd EvoTrees.jl extension (#182)","category":"section"},{"location":"changelog/#Other-8","page":"Release notes","title":"Other","text":"Fix docstrings in MathOptAIEvoTreesExt.jl (#183)\nUpdate to Lux@1 (#184)","category":"section"},{"location":"changelog/#Version-0.1.10-(April-14,-2025)","page":"Release notes","title":"Version 0.1.10 (April 14, 2025)","text":"","category":"section"},{"location":"changelog/#Added-10","page":"Release notes","title":"Added","text":"Allow relaxation of ReLUQuadratic (#178)","category":"section"},{"location":"changelog/#Fixed-5","page":"Release notes","title":"Fixed","text":"Fix duplicate test name (#176)","category":"section"},{"location":"changelog/#Version-0.1.9-(April-1,-2025)","page":"Release notes","title":"Version 0.1.9 (April 1, 2025)","text":"","category":"section"},{"location":"changelog/#Added-11","page":"Release notes","title":"Added","text":"Add VectorNonlinearOracle predictor (#172)","category":"section"},{"location":"changelog/#Version-0.1.8-(March-31,-2025)","page":"Release notes","title":"Version 0.1.8 (March 31, 2025)","text":"","category":"section"},{"location":"changelog/#Added-12","page":"Release notes","title":"Added","text":"Set device in output_size (#170)\nSupport SoftMax in PyTorchModel (#173)","category":"section"},{"location":"changelog/#Other-9","page":"Release notes","title":"Other","text":"Add paper to README (#169)","category":"section"},{"location":"changelog/#Version-0.1.7-(January-17,-2025)","page":"Release notes","title":"Version 0.1.7 (January 17, 2025)","text":"","category":"section"},{"location":"changelog/#Fixed-6","page":"Release notes","title":"Fixed","text":"Evaluate torch model to get output dimension (#166)","category":"section"},{"location":"changelog/#Version-0.1.6-(November-7,-2024)","page":"Release notes","title":"Version 0.1.6 (November 7, 2024)","text":"","category":"section"},{"location":"changelog/#Other-10","page":"Release notes","title":"Other","text":"Clarify instructions to load PythonCall (#163)","category":"section"},{"location":"changelog/#Version-0.1.5-(November-6,-2024)","page":"Release notes","title":"Version 0.1.5 (November 6, 2024)","text":"","category":"section"},{"location":"changelog/#Added-13","page":"Release notes","title":"Added","text":"Add gray_box_device option for PyTorchModel (#159)","category":"section"},{"location":"changelog/#Fixed-7","page":"Release notes","title":"Fixed","text":"Fix flakey tests (#161)","category":"section"},{"location":"changelog/#Version-0.1.4-(October-25,-2024)","page":"Release notes","title":"Version 0.1.4 (October 25, 2024)","text":"","category":"section"},{"location":"changelog/#Fixed-8","page":"Release notes","title":"Fixed","text":"Add bound constraints to the Formulation object (#155)","category":"section"},{"location":"changelog/#Other-11","page":"Release notes","title":"Other","text":"Improve the docstrings (#156)\nFix various typos (#157)","category":"section"},{"location":"changelog/#Version-0.1.3-(October-24,-2024)","page":"Release notes","title":"Version 0.1.3 (October 24, 2024)","text":"","category":"section"},{"location":"changelog/#Other-12","page":"Release notes","title":"Other","text":"Delete .github/workflows/documentation-deploy.yml (#150)\nImprove manual for NN extensions (#152)\nFix documentation link in README.md (#153)","category":"section"},{"location":"changelog/#Version-0.1.2-(October-23,-2024)","page":"Release notes","title":"Version 0.1.2 (October 23, 2024)","text":"","category":"section"},{"location":"changelog/#Other-13","page":"Release notes","title":"Other","text":"Add DOCUMENTER_KEY (#148)","category":"section"},{"location":"changelog/#Version-0.1.1-(October-23,-2024)","page":"Release notes","title":"Version 0.1.1 (October 23, 2024)","text":"","category":"section"},{"location":"changelog/#Other-14","page":"Release notes","title":"Other","text":"Update installation instructions for registered package (#141)\nCreate documentation-deploy.yml (#146)","category":"section"},{"location":"changelog/#Version-0.1.0-(October-22,-2024)","page":"Release notes","title":"Version 0.1.0 (October 22, 2024)","text":"Initial release. Too many pull requests to reference.","category":"section"},{"location":"manual/EvoTrees/#EvoTrees.jl","page":"EvoTrees.jl","title":"EvoTrees.jl","text":"EvoTrees.jl is a library for fitting decision trees in Julia.","category":"section"},{"location":"manual/EvoTrees/#Gradient-boosted-tree-regression","page":"EvoTrees.jl","title":"Gradient boosted tree regression","text":"Here is an example:\n\nusing JuMP, MathOptAI, EvoTrees\ntruth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\nx_train = abs.(sin.((1:10) .* (3:4)'));\nsize(x_train)\ny_train = truth.(Vector.(eachrow(x_train)));\nconfig = EvoTrees.EvoTreeRegressor(; nrounds = 3);\npredictor = EvoTrees.fit(config; x_train, y_train);\nmodel = Model();\n@variable(model, 0 <= x[1:2] <= 1);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/Lux/#Lux.jl","page":"Lux.jl","title":"Lux.jl","text":"Lux.jl is a library for machine learning in Julia.\n\nThe upstream documentation is available at https://lux.csail.mit.edu/stable/.","category":"section"},{"location":"manual/Lux/#Supported-layers","page":"Lux.jl","title":"Supported layers","text":"MathOptAI supports embedding a Lux model into JuMP if it is a Lux.Chain composed of:\n\nLux.Dense\nLux.Scale\nLux.relu\nLux.sigmoid\nLux.softmax\nLux.softplus\nLux.tanh","category":"section"},{"location":"manual/Lux/#Basic-example","page":"Lux.jl","title":"Basic example","text":"Use MathOptAI.add_predictor to embed a tuple (containing the Lux.Chain, the parameters, and the state) into a JuMP model:\n\nusing JuMP, Lux, MathOptAI, Random\nrng = Random.MersenneTwister();\nchain = Lux.Chain(Lux.Dense(1 => 2, Lux.relu), Lux.Dense(2 => 1))\nparameters, state = Lux.setup(rng, chain);\npredictor = (chain, parameters, state);\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/Lux/#Reduced-space","page":"Lux.jl","title":"Reduced-space","text":"Use the reduced_space = true keyword to formulate a reduced-space model:\n\nusing JuMP, Lux, MathOptAI, Random\nrng = Random.MersenneTwister();\nchain = Lux.Chain(Lux.Dense(1 => 2, Lux.relu), Lux.Dense(2 => 1))\nparameters, state = Lux.setup(rng, chain);\npredictor = (chain, parameters, state);\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; reduced_space = true);\ny\nformulation","category":"section"},{"location":"manual/Lux/#Gray-box","page":"Lux.jl","title":"Gray-box","text":"The Lux extension does not yet support the gray_box keyword argument.","category":"section"},{"location":"manual/Lux/#Change-how-layers-are-formulated","page":"Lux.jl","title":"Change how layers are formulated","text":"Pass a dictionary to the config keyword that maps Lux activation functions to a MathOptAI predictor:\n\nusing JuMP, Lux, MathOptAI, Random\nrng = Random.MersenneTwister();\nchain = Lux.Chain(Lux.Dense(1 => 2, Lux.relu), Lux.Dense(2 => 1))\nparameters, state = Lux.setup(rng, chain);\npredictor = (chain, parameters, state);\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    config = Dict(Lux.relu => MathOptAI.ReLUSOS1),\n);\ny\nformulation","category":"section"},{"location":"tutorials/decision_trees/#Classification-problems-with-DecisionTree.jl","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The purpose of this tutorial is to explain how to embed a decision tree model from DecisionTree.jl into JuMP.\n\nThe data and example in this tutorial comes from the paper: David Bergman, Teng Huang, Philip Brooks, Andrea Lodi, Arvind U. Raghunathan (2021) JANOS: An Integrated Predictive and Prescriptive Modeling Framework. INFORMS Journal on Computing 34(2):807-816. https://doi.org/10.1287/ijoc.2020.1023","category":"section"},{"location":"tutorials/decision_trees/#Required-packages","page":"Classification problems with DecisionTree.jl","title":"Required packages","text":"This tutorial uses the following packages.\n\nusing JuMP\nimport CSV\nimport DataFrames\nimport Downloads\nimport DecisionTree\nimport HiGHS\nimport MathOptAI\nimport Statistics","category":"section"},{"location":"tutorials/decision_trees/#Data","page":"Classification problems with DecisionTree.jl","title":"Data","text":"Here is a function to load the data directly from the JANOS repository:\n\nfunction read_df(filename)\n    url = \"https://raw.githubusercontent.com/INFORMSJoC/2020.1023/master/data/\"\n    data = Downloads.download(url * filename)\n    return CSV.read(data, DataFrames.DataFrame)\nend\n\nThere are two important files. The first, college_student_enroll-s1-1.csv, contains historical admissions data on anonymized students, their SAT score, their GPA, their merit scholarships, and whether the enrolled in the college.\n\ntrain_df = read_df(\"college_student_enroll-s1-1.csv\")\n\nThe second, college_applications6000.csv, contains the SAT and GPA data of students who are currently applying:\n\nevaluate_df = read_df(\"college_applications6000.csv\")\n\nThere are 6,000 prospective students:\n\nn_students = size(evaluate_df, 1)","category":"section"},{"location":"tutorials/decision_trees/#Prediction-model","page":"Classification problems with DecisionTree.jl","title":"Prediction model","text":"The first step is to train a logistic regression model to predict the Boolean enroll column based on the SAT, GPA, and merit columns.\n\ntrain_features = Matrix(train_df[:, [:SAT, :GPA, :merit]])\ntrain_labels = train_df[:, :enroll]\npredictor = DecisionTree.DecisionTreeClassifier(; max_depth = 3)\nDecisionTree.fit!(predictor, train_features, train_labels)\nDecisionTree.print_tree(predictor)","category":"section"},{"location":"tutorials/decision_trees/#Decision-model","page":"Classification problems with DecisionTree.jl","title":"Decision model","text":"Now that we have a trained decision tree, we want a decision model that chooses the optimal merit scholarship for each student in:\n\nevaluate_df\n\nHere's an empty JuMP model to start:\n\nmodel = Model()\n\nFirst, we add a new column to evaluate_df, with one JuMP decision variable for each row. It is important the .merit column name in evaluate_df matches the name in train_df.\n\nevaluate_df.merit = @variable(model, 0 <= x_merit[1:n_students] <= 2.5);\nevaluate_df\n\nThen, we use MathOptAI.add_predictor to embed model_ml into the JuMP model. MathOptAI.add_predictor returns a vector of variables, one for each row in evaluate_df, corresponding to the output enroll of our logistic regression.\n\nevaluate_features = Matrix(evaluate_df[:, [:SAT, :GPA, :merit]])\nevaluate_df.enroll = mapreduce(vcat, 1:size(evaluate_features, 1)) do i\n    y, _ = MathOptAI.add_predictor(model, predictor, evaluate_features[i, :])\n    return y\nend\nevaluate_df\n\nThe .enroll column name in evaluate_df is just a name. It doesn't have to match the name in train_df.\n\nThe objective of our problem is to maximize the expected number of students who enroll:\n\n@objective(model, Max, sum(evaluate_df.enroll))\n\nSubject to the constraint that we can spend at most 0.2 * n_students on merit scholarships:\n\n@constraint(model, sum(evaluate_df.merit) <= 0.2 * n_students)\n\nBecause logistic regression involves a Sigmoid layer, we need to use a smooth nonlinear optimizer. A common choice is Ipopt. Solve and check the optimizer found a feasible solution:\n\nset_optimizer(model, HiGHS.Optimizer)\nset_silent(model)\noptimize!(model)\n@assert is_solved_and_feasible(model)\n\nLet's store the solution in evaluate_df for analysis:\n\nevaluate_df.merit_sol = value.(evaluate_df.merit);\nevaluate_df.enroll_sol = value.(evaluate_df.enroll);\nevaluate_df","category":"section"},{"location":"tutorials/decision_trees/#Solution-analysis","page":"Classification problems with DecisionTree.jl","title":"Solution analysis","text":"We expect that just under 2,500 students will enroll:\n\nsum(evaluate_df.enroll_sol)\n\nWe awarded merit scholarships to approximately 1 in 6 students:\n\ncount(evaluate_df.merit_sol .> 1e-5)\n\nThe average merit scholarship was worth just under $1,000:\n\n1_000 * Statistics.mean(evaluate_df.merit_sol[evaluate_df.merit_sol .> 1e-5])\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorials/mnist_lux/#Adversarial-machine-learning-with-Lux.jl","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"The purpose of this tutorial is to explain how to embed a neural network model from Lux.jl into JuMP.","category":"section"},{"location":"tutorials/mnist_lux/#Required-packages","page":"Adversarial machine learning with Lux.jl","title":"Required packages","text":"This tutorial requires the following packages\n\nusing JuMP\nimport Lux\nimport Ipopt\nimport MathOptAI\nimport MLDatasets\nimport MLUtils\nimport OneHotArrays\nimport Optimisers\nimport Plots\nimport Random\nimport Zygote","category":"section"},{"location":"tutorials/mnist_lux/#Data","page":"Adversarial machine learning with Lux.jl","title":"Data","text":"This tutorial uses images from the MNIST dataset.\n\nWe load the predefined train and test splits:\n\ntrain_data = MLDatasets.MNIST(; split = :train)\n\ntest_data = MLDatasets.MNIST(; split = :test)\n\nSince the data are images, it is helpful to plot them. (This requires a transpose and reversing the rows to get the orientation correct.)\n\nfunction plot_image(x::Matrix; kwargs...)\n    return Plots.heatmap(\n        x'[size(x, 1):-1:1, :];\n        xlims = (1, size(x, 2)),\n        ylims = (1, size(x, 1)),\n        aspect_ratio = true,\n        legend = false,\n        xaxis = false,\n        yaxis = false,\n        kwargs...,\n    )\nend\n\nfunction plot_image(instance::NamedTuple)\n    return plot_image(instance.features; title = \"Label = $(instance.targets)\")\nend\n\nPlots.plot([plot_image(train_data[i]) for i in 1:6]...; layout = (2, 3))","category":"section"},{"location":"tutorials/mnist_lux/#Training","page":"Adversarial machine learning with Lux.jl","title":"Training","text":"We use a simple neural network with one hidden layer and a sigmoid activation function. (There are better performing networks; try experimenting.)\n\nchain = Lux.Chain(\n    Lux.Dense(28^2 => 32, Lux.sigmoid),\n    Lux.Dense(32 => 10),\n    Lux.softmax,\n)\nrng = Random.MersenneTwister();\nparameters, state = Lux.setup(rng, chain)\npredictor = (chain, parameters, state);\nnothing #hide\n\nHere is a function to load our data into the format that predictor expects:\n\nfunction data_loader(data; batchsize, shuffle = false)\n    x = reshape(data.features, 28^2, :)\n    y = OneHotArrays.onehotbatch(data.targets, 0:9)\n    return MLUtils.DataLoader((x, y); batchsize, shuffle)\nend\n\nand here is a function to score the percentage of correct labels, where we assign a label by choosing the label of the highest softmax in the final layer.\n\nfunction score_model(predictor, data)\n    chain, parameters, state = predictor\n    x, y = only(data_loader(data; batchsize = length(data)))\n    y_hat, _ = chain(x, parameters, state)\n    is_correct = OneHotArrays.onecold(y) .== OneHotArrays.onecold(y_hat)\n    p = round(100 * sum(is_correct) / length(is_correct); digits = 2)\n    println(\"Accuracy = $p %\")\n    return\nend\n\nThe accuracy of our model is only around 10% before training:\n\nscore_model(predictor, train_data)\nscore_model(predictor, test_data)\n\nLet's improve that by training our model.\n\nnote: Note\nIt is not the purpose of this tutorial to explain how Lux works; see the documentation at https://lux.csail.mit.edu for more details. Changing the number of epochs or the learning rate can improve the loss.\n\nbegin\n    train_loader = data_loader(train_data; batchsize = 256, shuffle = true)\n    optimizer_state = Optimisers.setup(Optimisers.Adam(0.0003f0), parameters)\n    for epoch in 1:30\n        loss = 0.0\n        for (x, y) in train_loader\n            global state\n            (loss_batch, state), pullback = Zygote.pullback(parameters) do p\n                y_model, new_state = chain(x, p, state)\n                return Lux.CrossEntropyLoss()(y_model, y), new_state\n            end\n            gradients = only(pullback((one(loss), nothing)))\n            Optimisers.update!(optimizer_state, parameters, gradients)\n            loss += loss_batch\n        end\n        loss = round(loss / length(train_loader); digits = 4)\n        print(\"Epoch $epoch: loss = $loss\\t\")\n        score_model(predictor, test_data)\n    end\nend\n\nHere are the first eight predictions of the test data:\n\nfunction plot_image(predictor, x::Matrix)\n    y, _ = chain(vec(x), parameters, state)\n    score, index = findmax(y)\n    title = \"Predicted: $(index - 1) ($(round(Int, 100 * score))%)\"\n    return plot_image(x; title)\nend\n\nplots = [plot_image(predictor, test_data[i].features) for i in 1:8]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))\n\nWe can also look at the best and worst four predictions:\n\nx, y = only(data_loader(test_data; batchsize = length(test_data)))\ny_model, _ = chain(x, parameters, state)\nlosses = Lux.CrossEntropyLoss(; agg = identity)(y_model, y)\nindices = sortperm(losses; dims = 2)[[1:4; (end-3):end]]\nplots = [plot_image(predictor, test_data[i].features) for i in indices]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))\n\nThere are still some fairly bad mistakes. Can you change the model or training parameters improve to improve things?","category":"section"},{"location":"tutorials/mnist_lux/#JuMP","page":"Adversarial machine learning with Lux.jl","title":"JuMP","text":"Now that we have a trained machine learning model, we can embed it in a JuMP model.\n\nHere's a function which takes a test case and returns an example that maximizes the probability of the adversarial example.\n\nfunction find_adversarial_image(test_case; adversary_label, δ = 0.05)\n    model = Model(Ipopt.Optimizer)\n    set_silent(model)\n    @variable(model, 0 <= x[1:28, 1:28] <= 1)\n    @constraint(model, -δ .<= x .- test_case.features .<= δ)\n    # Note: we need to use `vec` here because `x` is a 28-by-28 Matrix, but our\n    # neural network expects a 28^2 length vector.\n    y, _ = MathOptAI.add_predictor(model, predictor, vec(x))\n    @objective(model, Max, y[adversary_label+1] - y[test_case.targets+1])\n    optimize!(model)\n    @assert is_solved_and_feasible(model)\n    return value.(x)\nend\n\nLet's try finding an adversarial example to the third test image. The image on the left is our input image. The network thinks this is a 1 with probability 99%. The image on the right is the adversarial image. The network thinks this is a 7, although it is less confident.\n\nx_adversary = find_adversarial_image(test_data[3]; adversary_label = 7);\nPlots.plot(\n    plot_image(predictor, test_data[3].features),\n    plot_image(predictor, Float32.(x_adversary)),\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"release_notes/#Release-notes","page":"Release notes","title":"Release notes","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"section"},{"location":"release_notes/#[Version-0.2.2](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.2.2)-(January-22,-2026)","page":"Release notes","title":"Version 0.2.2 (January 22, 2026)","text":"","category":"section"},{"location":"release_notes/#Added","page":"Release notes","title":"Added","text":"Added support for nn.Dropout (#239)\nAdded LayerNorm predictor (#240)\nAdded GCNConv and TAGConv predictors (#244)","category":"section"},{"location":"release_notes/#Other","page":"Release notes","title":"Other","text":"Updated printing in docstrings for JuMP@1.29.4 (#241)\nAdded checklists to docs (#242)","category":"section"},{"location":"release_notes/#[Version-0.2.1](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.2.1)-(January-15,-2026)","page":"Release notes","title":"Version 0.2.1 (January 15, 2026)","text":"","category":"section"},{"location":"release_notes/#Added-2","page":"Release notes","title":"Added","text":"Added support for custom layers in PyTorch (#237)","category":"section"},{"location":"release_notes/#[Version-0.2.0](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.2.0)-(January-14,-2026)","page":"Release notes","title":"Version 0.2.0 (January 14, 2026)","text":"This release contains a number of breaking changes. See below for details on how to upgrade from v0.1.19.","category":"section"},{"location":"release_notes/#Breaking","page":"Release notes","title":"Breaking","text":"Remove ::Matrix method that maps over columns (#223).\nThis change means you cannot do MathOptAI.add_predictor(model, predictor, x) where x::Matrix and have it return a y::Matrix corresponding to mapping the predictor over the columns of x.\nTo upgrade, replace:\ny, formulation = MathOptAI.add_predictor(model, predictor, x::Matrix)\nwith:\nret = map(1:size(x, 2)) do i\n    return MathOptAI.add_predictor(model, predictor, x[:, i])\nend\nformulation = last.(ret)\ny = reduce(hcat, first.(ret))\nand write additional code as needed to handle the formulation objects.\nReplace GrayBox by VectorNonlinearOracle implementation (#230)\nWe have removed the old GrayBox predictor and replaced it by the VectorNonlinearOracle predictor, which has now been renamed to GrayBox. In addition, the ; vector_nonlinear_oracle = true keyword argument has been removed, and the new ; gray_box = true is equivalent to the old ; vector_nonlinear_oracle = true.\nTo upgrade, replace:\nMathOptAI.add_predictor(model, predictor, x; vector_nonlinear_oracle = true)\nwith:\nMathOptAI.add_predictor(model, predictor, x; gray_box = true)\nChange config dictionary to have functions as values (#233)\nWe have changed how the ; config = Dict() keyword works for the Flux, Lux, and PyTorch extensions. Where previously the values were instantiated AbstractPredictor objects, they must now be constructors that, when called, return an AbstractPredictor object.\nAs an example of upgrading, replace:\n; config = Dict(Flux.relu => MathOptAI.ReLU())\nwith:\n; config = Dict(Flux.relu => MathOptAI.ReLU)\n# or, alternatively\n; config = Dict(Flux.relu => () -> MathOptAI.ReLU())","category":"section"},{"location":"release_notes/#Added-3","page":"Release notes","title":"Added","text":"Add LeakyReLU predictor (#218)\nAdd AvgPool2d, Conv2d, and MaxPool2d predictors (#220), (#222), (#224)\nAdd support for custom models in Flux (#227)\nAdd MaxPool2dBigM predictor (#231), (#235)","category":"section"},{"location":"release_notes/#Fixed","page":"Release notes","title":"Fixed","text":"Simplify test to fix flakey TestPythonCallExt.test_gelu (#228)","category":"section"},{"location":"release_notes/#Other-2","page":"Release notes","title":"Other","text":"Remove comment that vector_nonlinear_oracle is experimental (#219)\nAdded this changelog (#234), (#236)","category":"section"},{"location":"release_notes/#[Version-0.1.19](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.19)-(December-5,-2025)","page":"Release notes","title":"Version 0.1.19 (December 5, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-4","page":"Release notes","title":"Added","text":"Add replace_weights_with_variables (#215)","category":"section"},{"location":"release_notes/#[Version-0.1.18](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.18)-(October-24,-2025)","page":"Release notes","title":"Version 0.1.18 (October 24, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-5","page":"Release notes","title":"Added","text":"Propagate start values through layers (#210)","category":"section"},{"location":"release_notes/#Other-3","page":"Release notes","title":"Other","text":"Update to JuliaFormatter@2 (#211)\nMake some predictors callable (#212)","category":"section"},{"location":"release_notes/#[Version-0.1.17](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.17)-(October-23,-2025)","page":"Release notes","title":"Version 0.1.17 (October 23, 2025)","text":"","category":"section"},{"location":"release_notes/#Fixed-2","page":"Release notes","title":"Fixed","text":"Iterate over PyTorch Sequential module itself, rather than its children (#207)","category":"section"},{"location":"release_notes/#[Version-0.1.16](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.16)-(October-22,-2025)","page":"Release notes","title":"Version 0.1.16 (October 22, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-6","page":"Release notes","title":"Added","text":"Add GELU predictor (#202)\nAdd support for MOI.VectorNonlinearOracle (#204)","category":"section"},{"location":"release_notes/#Other-4","page":"Release notes","title":"Other","text":"Clarify the problem class of each predictor (#205)","category":"section"},{"location":"release_notes/#[Version-0.1.15](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.15)-(September-30,-2025)","page":"Release notes","title":"Version 0.1.15 (September 30, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-7","page":"Release notes","title":"Added","text":"Add extension hooks for InfiniteOpt (#200)","category":"section"},{"location":"release_notes/#Other-5","page":"Release notes","title":"Other","text":"Refactor different ReLU into separate files (#199)","category":"section"},{"location":"release_notes/#[Version-0.1.14](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.14)-(September-8,-2025)","page":"Release notes","title":"Version 0.1.14 (September 8, 2025)","text":"","category":"section"},{"location":"release_notes/#Fixed-3","page":"Release notes","title":"Fixed","text":"Fix binary decision tree by making the tolerance default to 1e-6 (#196)\nFix docs because of a change in Lux (#197)","category":"section"},{"location":"release_notes/#Other-6","page":"Release notes","title":"Other","text":"Update README.md with MOAI paper (#194)","category":"section"},{"location":"release_notes/#[Version-0.1.13](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.13)-(July-8,-2025)","page":"Release notes","title":"Version 0.1.13 (July 8, 2025)","text":"","category":"section"},{"location":"release_notes/#Fixed-4","page":"Release notes","title":"Fixed","text":"Add DimensionMismatch checks for Affine and Scale (#192)","category":"section"},{"location":"release_notes/#[Version-0.1.12](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.12)-(June-3,-2025)","page":"Release notes","title":"Version 0.1.12 (June 3, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-8","page":"Release notes","title":"Added","text":"Add a reduced_space fallback for extensions (#187)","category":"section"},{"location":"release_notes/#Other-7","page":"Release notes","title":"Other","text":"Update test for change in MOI@1.40.2 (#186)\nAdd Softmax to the PyTorch manual page (#188)","category":"section"},{"location":"release_notes/#[Version-0.1.11](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.11)-(June-2,-2025)","page":"Release notes","title":"Version 0.1.11 (June 2, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-9","page":"Release notes","title":"Added","text":"Add LinearCombination predictor (#181)\nAdd EvoTrees.jl extension (#182)","category":"section"},{"location":"release_notes/#Other-8","page":"Release notes","title":"Other","text":"Fix docstrings in MathOptAIEvoTreesExt.jl (#183)\nUpdate to Lux@1 (#184)","category":"section"},{"location":"release_notes/#[Version-0.1.10](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.10)-(April-14,-2025)","page":"Release notes","title":"Version 0.1.10 (April 14, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-10","page":"Release notes","title":"Added","text":"Allow relaxation of ReLUQuadratic (#178)","category":"section"},{"location":"release_notes/#Fixed-5","page":"Release notes","title":"Fixed","text":"Fix duplicate test name (#176)","category":"section"},{"location":"release_notes/#[Version-0.1.9](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.9)-(April-1,-2025)","page":"Release notes","title":"Version 0.1.9 (April 1, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-11","page":"Release notes","title":"Added","text":"Add VectorNonlinearOracle predictor (#172)","category":"section"},{"location":"release_notes/#[Version-0.1.8](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.8)-(March-31,-2025)","page":"Release notes","title":"Version 0.1.8 (March 31, 2025)","text":"","category":"section"},{"location":"release_notes/#Added-12","page":"Release notes","title":"Added","text":"Set device in output_size (#170)\nSupport SoftMax in PyTorchModel (#173)","category":"section"},{"location":"release_notes/#Other-9","page":"Release notes","title":"Other","text":"Add paper to README (#169)","category":"section"},{"location":"release_notes/#[Version-0.1.7](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.7)-(January-17,-2025)","page":"Release notes","title":"Version 0.1.7 (January 17, 2025)","text":"","category":"section"},{"location":"release_notes/#Fixed-6","page":"Release notes","title":"Fixed","text":"Evaluate torch model to get output dimension (#166)","category":"section"},{"location":"release_notes/#[Version-0.1.6](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.6)-(November-7,-2024)","page":"Release notes","title":"Version 0.1.6 (November 7, 2024)","text":"","category":"section"},{"location":"release_notes/#Other-10","page":"Release notes","title":"Other","text":"Clarify instructions to load PythonCall (#163)","category":"section"},{"location":"release_notes/#[Version-0.1.5](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.5)-(November-6,-2024)","page":"Release notes","title":"Version 0.1.5 (November 6, 2024)","text":"","category":"section"},{"location":"release_notes/#Added-13","page":"Release notes","title":"Added","text":"Add gray_box_device option for PyTorchModel (#159)","category":"section"},{"location":"release_notes/#Fixed-7","page":"Release notes","title":"Fixed","text":"Fix flakey tests (#161)","category":"section"},{"location":"release_notes/#[Version-0.1.4](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.4)-(October-25,-2024)","page":"Release notes","title":"Version 0.1.4 (October 25, 2024)","text":"","category":"section"},{"location":"release_notes/#Fixed-8","page":"Release notes","title":"Fixed","text":"Add bound constraints to the Formulation object (#155)","category":"section"},{"location":"release_notes/#Other-11","page":"Release notes","title":"Other","text":"Improve the docstrings (#156)\nFix various typos (#157)","category":"section"},{"location":"release_notes/#[Version-0.1.3](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.3)-(October-24,-2024)","page":"Release notes","title":"Version 0.1.3 (October 24, 2024)","text":"","category":"section"},{"location":"release_notes/#Other-12","page":"Release notes","title":"Other","text":"Delete .github/workflows/documentation-deploy.yml (#150)\nImprove manual for NN extensions (#152)\nFix documentation link in README.md (#153)","category":"section"},{"location":"release_notes/#[Version-0.1.2](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.2)-(October-23,-2024)","page":"Release notes","title":"Version 0.1.2 (October 23, 2024)","text":"","category":"section"},{"location":"release_notes/#Other-13","page":"Release notes","title":"Other","text":"Add DOCUMENTER_KEY (#148)","category":"section"},{"location":"release_notes/#[Version-0.1.1](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.1)-(October-23,-2024)","page":"Release notes","title":"Version 0.1.1 (October 23, 2024)","text":"","category":"section"},{"location":"release_notes/#Other-14","page":"Release notes","title":"Other","text":"Update installation instructions for registered package (#141)\nCreate documentation-deploy.yml (#146)","category":"section"},{"location":"release_notes/#[Version-0.1.0](https://github.com/lanl-ansi/MathOptAI.jl/releases/tag/v0.1.0)-(October-22,-2024)","page":"Release notes","title":"Version 0.1.0 (October 22, 2024)","text":"Initial release. Too many pull requests to reference.","category":"section"},{"location":"manual/PyTorch/#PyTorch","page":"PyTorch","title":"PyTorch","text":"PyTorch is a library for machine learning in Python.\n\nThe upstream documentation is available at https://pytorch.org/docs/stable/.\n\ninfo: Info\nTo use PyTorch from MathOptAI, you must first follow the Python integration instructions.","category":"section"},{"location":"manual/PyTorch/#Supported-layers","page":"PyTorch","title":"Supported layers","text":"MathOptAI supports embedding a PyTorch models into JuMP if it is a nn.Sequential composed of:\n\nnn.AvgPool2d\nnn.Conv2d\nnn.Flatten\nnn.GELU\nnn.LayerNorm\nnn.LeakyReLU\nnn.Linear\nnn.MaxPool2d\nnn.ReLU\nnn.Sigmoid\nnn.Softmax\nnn.Softplus\nnn.Tanh","category":"section"},{"location":"manual/PyTorch/#File-format","page":"PyTorch","title":"File format","text":"Use torch.save to save a trained PyTorch model to a .pt file:\n\n#!/usr/bin/python3\nimport torch\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2, 1),\n)\ntorch.save(model, \"saved_pytorch_model.pt\")","category":"section"},{"location":"manual/PyTorch/#Basic-example","page":"PyTorch","title":"Basic example","text":"Use MathOptAI.add_predictor to embed a PyTorch model into a JuMP model:\n\nusing JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"section"},{"location":"manual/PyTorch/#Reduced-space","page":"PyTorch","title":"Reduced-space","text":"Use the reduced_space = true keyword to formulate a reduced-space model:\n\nusing JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; reduced_space = true);\ny\nformulation","category":"section"},{"location":"manual/PyTorch/#Gray-box","page":"PyTorch","title":"Gray-box","text":"Use the gray_box = true keyword to embed the network as a vector nonlinear operator:\n\nusing JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; gray_box = true);\ny\nformulation","category":"section"},{"location":"manual/PyTorch/#Change-how-layers-are-formulated","page":"PyTorch","title":"Change how layers are formulated","text":"Pass a dictionary to the config keyword that maps the Symbol name of each PyTorch layer to a MathOptAI predictor:\n\nusing JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    config = Dict(:ReLU => MathOptAI.ReLUSOS1),\n);\ny\nformulation","category":"section"},{"location":"manual/PyTorch/#Custom-layers","page":"PyTorch","title":"Custom layers","text":"If your PyTorch model contains a custom layer, define a new AbstractPredictor and pass a config dictionary that maps the Class object to a callback that builds the new predictor.\n\nThe callback must have the signature (layer::PythonCall.Py; kwargs...). Valid keyword arguments are currently:\n\ninput_size: the input size of they layer\nconfig: the config dictionary, if needed to convert layers inside the custom layer\nnn: a reference to torch.nn\n\nYou must always have kwargs... so that future versions of MathOptAI can add new keywords in a non-breaking way.\n\nusing JuMP, PythonCall, MathOptAI\ndir = mktempdir()\nwrite(\n    joinpath(dir, \"custom_model.py\"),\n    \"\"\"\n    import torch\n    class Skip(torch.nn.Module):\n        def __init__(self, inner):\n            super().__init__()\n            self.inner = inner\n        def forward(self, x):\n            return self.inner(x) + x\n    \"\"\",\n)\nfilename = joinpath(dir, \"custom_model.pt\")\nPythonCall.@pyexec(\n    (dir, filename) =>\n        \"\"\"\n        import sys\n        sys.path.insert(0, dir)\n        import torch\n        from custom_model import Skip\n        inner = torch.nn.Sequential(torch.nn.Linear(3, 3), torch.nn.ReLU())\n        model = Skip(inner)\n        torch.save(model, filename)\n        \"\"\" => Skip,\n)\nstruct CustomPredictor <: MathOptAI.AbstractPredictor\n    p::MathOptAI.Pipeline\nend\nfunction MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::CustomPredictor,\n    x::Vector;\n    kwargs...,\n)\n    y, formulation = MathOptAI.add_predictor(model, predictor.p, x; kwargs...)\n    @assert length(x) == length(y)\n    return y .+ x, formulation\nend\nmodel = Model();\n@variable(model, x[i in 1:3]);\npredictor = MathOptAI.PytorchModel(filename)\nfunction skip_callback(layer::PythonCall.Py; input_size, kwargs...)\n    return CustomPredictor(MathOptAI.build_predictor(layer.inner))\nend\nconfig = Dict(Skip => skip_callback)\ny, formulation = MathOptAI.add_predictor(model, predictor, x; config);\ny\nformulation","category":"section"},{"location":"developers/checklists/#Checklists","page":"Checklists","title":"Checklists","text":"The purpose of this page is to collate a series of checklists for commonly performed changes to the source code of MathOptAI.\n\nIn each case, copy the checklist into the description of the pull request.","category":"section"},{"location":"developers/checklists/#Making-a-release","page":"Checklists","title":"Making a release","text":"In preparation for a release, use the following checklist. These steps can be done in the same commit, or separately. The last commit should have the message \"Prep for vX.Y.Z.\"\n\n## Pre-release\n\n - [ ] Update `docs/src/changelog.md`\n - [ ] Change the version number in `Project.toml`\n - [ ] The commit messages in this PR do not contain `[ci skip]`\n\n## The release\n\n - [ ] After merging this pull request, comment `[at]JuliaRegistrator register`\n       in the GitHub commit.","category":"section"},{"location":"developers/checklists/#Adding-a-new-predictor","page":"Checklists","title":"Adding a new predictor","text":"Use this checklist when adding a new predictor.\n\n## The predictor\n\n - [ ] Create a new file in `src/predictors`\n - [ ] Add the default copyright header\n - [ ] Make a new `<: AbstractPredictor`  type\n - [ ] Add a docstring to the predictor\n - [ ] Add the predictor to `docs/src/api.md`\n - [ ] Add the predictor to `docs/src/manual/predictors.md`\n - [ ] Implement `output_size`\n - [ ] Implement `add_predictor(model, ::NewPredictor, x)`\n - [ ] Optionally implement `add_predictor(model, ::ReducedSpace{NewPredictor}, x)`\n - [ ] Add a test to `test/test_predictors.jl`\n\n## Extensions\n\nFor each revelevant extension:\n\n - [ ] Add support for the new predictor\n - [ ] Add tests\n - [ ] Mention the predictor in the docstring of the relevant `build_predictor`\n - [ ] Mention the predictor in the relevant `docs/src/manual/<ext>.md`","category":"section"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"(Image: )","category":"section"},{"location":"#MathOptAI.jl","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"MathOptAI.jl is a JuMP extension for embedding trained AI, machine learning, and statistical learning models into a JuMP optimization model.","category":"section"},{"location":"#License","page":"MathOptAI.jl","title":"License","text":"MathOptAI.jl is provided under a BSD-3 license as part of the Optimization and Machine Learning Toolbox project, O4806.\n\nSee LICENSE.md for details.\n\nDespite the name similarity, this project is not affiliated with OMLT, the Optimization and Machine Learning Toolkit.","category":"section"},{"location":"#Installation","page":"MathOptAI.jl","title":"Installation","text":"Install MathOptAI.jl using the Julia package manager:\n\nimport Pkg\nPkg.add(\"MathOptAI\")","category":"section"},{"location":"#Getting-started","page":"MathOptAI.jl","title":"Getting started","text":"Here's an example of using MathOptAI to embed a trained neural network from Flux into a JuMP model. The vector of JuMP variables x is fed as input to the neural network. The output y is a vector of JuMP variables that represents the output layer of the neural network. The formulation object stores the additional variables and constraints that were added to model.\n\njulia> using JuMP, MathOptAI, Flux\n\njulia> predictor = Flux.Chain(\n           Flux.Dense(28^2 => 32, Flux.sigmoid),\n           Flux.Dense(32 => 10),\n           Flux.softmax,\n       );\n\njulia> #= Train the Flux model. Code not shown for simplicity =#\n\njulia> model = JuMP.Model();\n\njulia> JuMP.@variable(model, 0 <= x[1:28^2] <= 1);\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n10-element Vector{VariableRef}:\n moai_SoftMax[1]\n moai_SoftMax[2]\n moai_SoftMax[3]\n moai_SoftMax[4]\n moai_SoftMax[5]\n moai_SoftMax[6]\n moai_SoftMax[7]\n moai_SoftMax[8]\n moai_SoftMax[9]\n moai_SoftMax[10]","category":"section"},{"location":"#Getting-help","page":"MathOptAI.jl","title":"Getting help","text":"This package is under active development. For help, questions, comments, and suggestions, please open a GitHub issue.","category":"section"},{"location":"#Inspiration","page":"MathOptAI.jl","title":"Inspiration","text":"This project is mainly inspired by two existing projects:\n\nOMLT\ngurobi-machinelearning\n\nOther works, from which we took less inspiration, include:\n\nJANOS\nMeLOn\nENTMOOT\nreluMIP\nOptiCL\nPySCIPOpt-ML\n\nThe 2024 paper of López-Flores et al. is an excellent summary of the state of the field at the time that we started development of MathOptAI.\n\nLópez-Flores, F.J., Ramírez-Márquez, C., Ponce-Ortega J.M. (2024). Process Systems Engineering Tools for Optimization of Trained Machine Learning Models: Comparative and Perspective. Industrial & Engineering Chemistry Research, 63(32), 13966-13979. DOI: 10.1021/acs.iecr.4c00632","category":"section"},{"location":"tutorials/gaussian/#Function-fitting-with-AbstractGPs","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"The purpose of this tutorial is to explain how to embed a Gaussian Process from AbstractGPs into JuMP.","category":"section"},{"location":"tutorials/gaussian/#Required-packages","page":"Function fitting with AbstractGPs","title":"Required packages","text":"This tutorial requires the following packages\n\nusing JuMP\nusing Test\nimport AbstractGPs\nimport Ipopt\nimport MathOptAI\nimport Plots","category":"section"},{"location":"tutorials/gaussian/#Prediction-model","page":"Function fitting with AbstractGPs","title":"Prediction model","text":"Assume that we have some true underlying univariate function:\n\nx_domain = 0:0.01:2π\ntrue_function(x) = sin(x)\nPlots.plot(x_domain, true_function.(x_domain); label = \"truth\")\n\nWe don't know the function, but we have access to a limited set of noisy sample points:\n\nN = 20\nx_data = rand(x_domain, N)\nnoisy_sampler(x) = true_function(x) + 0.25 * (2rand() - 1)\ny_data = noisy_sampler.(x_data)\nPlots.scatter!(x_data, y_data; label = \"data\")\n\nUsing the data, we want to build a predictor y = predictor(x). One choice is a Gaussian Process:\n\nfx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.4)\np_fx = AbstractGPs.posterior(fx, y_data)\nPlots.plot!(x_domain, p_fx; label = \"GP\", fillalpha = 0.1)\n\nGaussian Processes fit a mean and variance function:\n\nAbstractGPs.mean_and_var(p_fx, [π / 2])","category":"section"},{"location":"tutorials/gaussian/#Decision-model","page":"Function fitting with AbstractGPs","title":"Decision model","text":"Our goal for this JuMP model is to embed the Gaussian Process from AbstractGPs into the model and then solve for different fixed values of x to recreate the function that the Gaussian Process has learned to approximate.\n\nFirst, create a JuMP model:\n\nmodel = Model(Ipopt.Optimizer)\nset_silent(model)\n@variable(model, x)\n\nSince a Gaussian Process is a infinite dimensional object (its prediction is a distribution), we need some way of converting the Gaussian Process into a finite set of scalar values. For this, we use the Quantile predictor:\n\npredictor = MathOptAI.Quantile(p_fx, [0.25, 0.75]);\ny, _ = MathOptAI.add_predictor(model, predictor, [x])\n\nNow, visualize the fitted function y = predictor(x) by repeatedly solving the optimization problem for different fixed values of x. Each value of y has two elements: one for the 25th percentile and one for the 75th.\n\nX, Y = range(0, 2π; length = 20), Any[]\n@constraint(model, c, x == 0.0)\nfor xi in X\n    set_normalized_rhs(c, xi)\n    optimize!(model)\n    if is_solved_and_feasible(model)\n        push!(Y, value.(y))\n    else\n        push!(Y, [NaN, NaN])\n    end\nend\nPlots.plot!(X, reduce(hcat, Y)'; label = [\"P25\" \"P75\"], linewidth = 3)\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
