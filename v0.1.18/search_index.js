var documenterSearchIndex = {"docs":
[{"location":"manual/Flux/#Flux.jl","page":"Flux.jl","title":"Flux.jl","text":"","category":"section"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"Flux.jl is a library for machine learning in Julia.","category":"page"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"The upstream documentation is available at https://fluxml.ai/Flux.jl/stable/.","category":"page"},{"location":"manual/Flux/#Supported-layers","page":"Flux.jl","title":"Supported layers","text":"","category":"section"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"MathOptAI supports embedding a Flux model into JuMP if it is a Flux.Chain composed of:","category":"page"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"Flux.Dense\nFlux.Scale\nFlux.relu\nFlux.sigmoid\nFlux.softmax\nFlux.softplus\nFlux.tanh","category":"page"},{"location":"manual/Flux/#Basic-example","page":"Flux.jl","title":"Basic example","text":"","category":"section"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"Use MathOptAI.add_predictor to embed a Flux.Chain into a JuMP model:","category":"page"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"using JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"manual/Flux/#Reduced-space","page":"Flux.jl","title":"Reduced-space","text":"","category":"section"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"Use the reduced_space = true keyword to formulate a reduced-space model:","category":"page"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"using JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; reduced_space = true);\ny\nformulation","category":"page"},{"location":"manual/Flux/#Gray-box","page":"Flux.jl","title":"Gray-box","text":"","category":"section"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"Use the gray_box = true keyword to embed the network as a nonlinear operator:","category":"page"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"using JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; gray_box = true);\ny\nformulation","category":"page"},{"location":"manual/Flux/#flux-vector-nonlinear-oracle","page":"Flux.jl","title":"VectorNonlinearOracle","text":"","category":"section"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"Use the vector_nonlinear_oracle = true keyword to embed the network as a vector nonlinear operator:","category":"page"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"using JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    vector_nonlinear_oracle = true,\n);\ny\nformulation","category":"page"},{"location":"manual/Flux/#Change-how-layers-are-formulated","page":"Flux.jl","title":"Change how layers are formulated","text":"","category":"section"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"Pass a dictionary to the config keyword that maps Flux activation functions to a MathOptAI predictor:","category":"page"},{"location":"manual/Flux/","page":"Flux.jl","title":"Flux.jl","text":"using JuMP, Flux, MathOptAI\npredictor = Flux.Chain(Flux.Dense(1 => 2, Flux.relu), Flux.Dense(2 => 1));\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    config = Dict(Flux.relu => MathOptAI.ReLUSOS1()),\n);\ny\nformulation","category":"page"},{"location":"tutorials/mnist/#Adversarial-machine-learning-with-Flux.jl","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"","category":"section"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"The purpose of this tutorial is to explain how to embed a neural network model from Flux.jl into JuMP.","category":"page"},{"location":"tutorials/mnist/#Required-packages","page":"Adversarial machine learning with Flux.jl","title":"Required packages","text":"","category":"section"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"This tutorial requires the following packages","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"using JuMP\nimport Flux\nimport Ipopt\nimport MathOptAI\nimport MLDatasets\nimport Plots","category":"page"},{"location":"tutorials/mnist/#Data","page":"Adversarial machine learning with Flux.jl","title":"Data","text":"","category":"section"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"This tutorial uses images from the MNIST dataset.","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"We load the predefined train and test splits:","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"train_data = MLDatasets.MNIST(; split = :train)","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"test_data = MLDatasets.MNIST(; split = :test)","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"Since the data are images, it is helpful to plot them. (This requires a transpose and reversing the rows to get the orientation correct.)","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"function plot_image(x::Matrix; kwargs...)\n    return Plots.heatmap(\n        x'[size(x, 1):-1:1, :];\n        xlims = (1, size(x, 2)),\n        ylims = (1, size(x, 1)),\n        aspect_ratio = true,\n        legend = false,\n        xaxis = false,\n        yaxis = false,\n        kwargs...,\n    )\nend\n\nfunction plot_image(instance::NamedTuple)\n    return plot_image(instance.features; title = \"Label = $(instance.targets)\")\nend\n\nPlots.plot([plot_image(train_data[i]) for i in 1:6]...; layout = (2, 3))","category":"page"},{"location":"tutorials/mnist/#Training","page":"Adversarial machine learning with Flux.jl","title":"Training","text":"","category":"section"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"We use a simple neural network with one hidden layer and a sigmoid activation function. (There are better performing networks; try experimenting.)","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"predictor = Flux.Chain(\n    Flux.Dense(28^2 => 32, Flux.sigmoid),\n    Flux.Dense(32 => 10),\n    Flux.softmax,\n)","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"Here is a function to load our data into the format that predictor expects:","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"function data_loader(data; batchsize, shuffle = false)\n    x = reshape(data.features, 28^2, :)\n    y = Flux.onehotbatch(data.targets, 0:9)\n    return Flux.DataLoader((x, y); batchsize, shuffle)\nend","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"and here is a function to score the percentage of correct labels, where we assign a label by choosing the label of the highest softmax in the final layer.","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"function score_model(predictor, data)\n    x, y = only(data_loader(data; batchsize = length(data)))\n    y_hat = predictor(x)\n    is_correct = Flux.onecold(y) .== Flux.onecold(y_hat)\n    p = round(100 * sum(is_correct) / length(is_correct); digits = 2)\n    println(\"Accuracy = $p %\")\n    return\nend","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"The accuracy of our model is only around 10% before training:","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"score_model(predictor, train_data)\nscore_model(predictor, test_data)","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"Let's improve that by training our model.","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"note: Note\nIt is not the purpose of this tutorial to explain how Flux works; see the documentation at https://fluxml.ai for more details. Changing the number of epochs or the learning rate can improve the loss.","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"begin\n    train_loader = data_loader(train_data; batchsize = 256, shuffle = true)\n    optimizer_state = Flux.setup(Flux.Adam(3e-4), predictor)\n    for epoch in 1:30\n        loss = 0.0\n        for (x, y) in train_loader\n            loss_batch, gradient = Flux.withgradient(predictor) do model\n                return Flux.crossentropy(model(x), y)\n            end\n            Flux.update!(optimizer_state, predictor, only(gradient))\n            loss += loss_batch\n        end\n        loss = round(loss / length(train_loader); digits = 4)\n        print(\"Epoch $epoch: loss = $loss\\t\")\n        score_model(predictor, test_data)\n    end\nend","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"Here are the first eight predictions of the test data:","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"function plot_image(predictor, x::Matrix)\n    score, index = findmax(predictor(vec(x)))\n    title = \"Predicted: $(index - 1) ($(round(Int, 100 * score))%)\"\n    return plot_image(x; title)\nend\n\nplots = [plot_image(predictor, test_data[i].features) for i in 1:8]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"We can also look at the best and worst four predictions:","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"x, y = only(data_loader(test_data; batchsize = length(test_data)))\nlosses = Flux.crossentropy(predictor(x), y; agg = identity)\nindices = sortperm(losses; dims = 2)[[1:4; (end-3):end]]\nplots = [plot_image(predictor, test_data[i].features) for i in indices]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"There are still some fairly bad mistakes. Can you change the model or training parameters improve to improve things?","category":"page"},{"location":"tutorials/mnist/#JuMP","page":"Adversarial machine learning with Flux.jl","title":"JuMP","text":"","category":"section"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"Now that we have a trained machine learning model, we can embed it in a JuMP model.","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"Here's a function which takes a test case and returns an example that maximizes the probability of the adversarial example.","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"function find_adversarial_image(test_case; adversary_label, δ = 0.05)\n    model = Model(Ipopt.Optimizer)\n    set_silent(model)\n    @variable(model, 0 <= x[1:28, 1:28] <= 1)\n    @constraint(model, -δ .<= x .- test_case.features .<= δ)\n    # Note: we need to use `vec` here because `x` is a 28-by-28 Matrix, but our\n    # neural network expects a 28^2 length vector.\n    y, _ = MathOptAI.add_predictor(model, predictor, vec(x))\n    @objective(model, Max, y[adversary_label+1] - y[test_case.targets+1])\n    optimize!(model)\n    @assert is_solved_and_feasible(model)\n    return value.(x)\nend","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"Let's try finding an adversarial example to the third test image. The image on the left is our input image. The network thinks this is a 1 with probability 99%. The image on the right is the adversarial image. The network thinks this is a 7, although it is less confident.","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"x_adversary = find_adversarial_image(test_data[3]; adversary_label = 7);\nPlots.plot(\n    plot_image(predictor, test_data[3].features),\n    plot_image(predictor, Float32.(x_adversary)),\n)","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"","category":"page"},{"location":"tutorials/mnist/","page":"Adversarial machine learning with Flux.jl","title":"Adversarial machine learning with Flux.jl","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/student_enrollment/#Logistic-regression-with-GLM.jl","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"","category":"section"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The purpose of this tutorial is to explain how to embed a logistic regression model from GLM.jl into JuMP.","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The data and example in this tutorial comes from the paper: David Bergman, Teng Huang, Philip Brooks, Andrea Lodi, Arvind U. Raghunathan (2021) JANOS: An Integrated Predictive and Prescriptive Modeling Framework. INFORMS Journal on Computing 34(2):807-816. https://doi.org/10.1287/ijoc.2020.1023","category":"page"},{"location":"tutorials/student_enrollment/#Required-packages","page":"Logistic regression with GLM.jl","title":"Required packages","text":"","category":"section"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"This tutorial uses the following packages.","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"using JuMP\nimport CSV\nimport DataFrames\nimport Downloads\nimport GLM\nimport Ipopt\nimport MathOptAI\nimport Statistics","category":"page"},{"location":"tutorials/student_enrollment/#Data","page":"Logistic regression with GLM.jl","title":"Data","text":"","category":"section"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"Here is a function to load the data directly from the JANOS repository:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"function read_df(filename)\n    url = \"https://raw.githubusercontent.com/INFORMSJoC/2020.1023/master/data/\"\n    data = Downloads.download(url * filename)\n    return CSV.read(data, DataFrames.DataFrame)\nend","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"There are two important files. The first, college_student_enroll-s1-1.csv, contains historical admissions data on anonymized students, their SAT score, their GPA, their merit scholarships, and whether the enrolled in the college.","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"train_df = read_df(\"college_student_enroll-s1-1.csv\")","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The second, college_applications6000.csv, contains the SAT and GPA data of students who are currently applying:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"evaluate_df = read_df(\"college_applications6000.csv\")","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"There are 6,000 prospective students:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"n_students = size(evaluate_df, 1)","category":"page"},{"location":"tutorials/student_enrollment/#Prediction-model","page":"Logistic regression with GLM.jl","title":"Prediction model","text":"","category":"section"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The first step is to train a logistic regression model to predict the Boolean enroll column based on the SAT, GPA, and merit columns.","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"predictor = GLM.glm(\n    GLM.@formula(enroll ~ 0 + SAT + GPA + merit),\n    train_df,\n    GLM.Bernoulli(),\n)","category":"page"},{"location":"tutorials/student_enrollment/#Decision-model","page":"Logistic regression with GLM.jl","title":"Decision model","text":"","category":"section"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"Now that we have a trained logistic regression model, we want a decision model that chooses the optimal merit scholarship for each student in","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"evaluate_df","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"Here's an empty JuMP model to start:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"model = Model()","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"First, we add a new column to evaluate_df, with one JuMP decision variable for each row. It is important the .merit column name in evaluate_df matches the name in train_df.","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"evaluate_df.merit = @variable(model, 0 <= x_merit[1:n_students] <= 2.5);\nevaluate_df","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"Then, we use MathOptAI.add_predictor to embed predictor into the JuMP model. MathOptAI.add_predictor returns a vector of variables, one for each row inn evaluate_df, corresponding to the output enroll of our logistic regression.","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"evaluate_df.enroll, _ = MathOptAI.add_predictor(model, predictor, evaluate_df);\nevaluate_df","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The .enroll column name in evaluate_df is just a name. It doesn't have to match the name in train_df.","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The objective of our problem is to maximize the expected number of students who enroll:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"@objective(model, Max, sum(evaluate_df.enroll))","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"Subject to the constraint that we can spend at most 0.2 * n_students on merit scholarships:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"@constraint(model, sum(evaluate_df.merit) <= 0.2 * n_students)","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"Because logistic regression involves a Sigmoid layer, we need to use a smooth nonlinear optimizer. A common choice is Ipopt. Solve and check the optimizer found a feasible solution:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"set_optimizer(model, Ipopt.Optimizer)\nset_silent(model)\noptimize!(model)\n@assert is_solved_and_feasible(model)\nsolution_summary(model)","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"Let's store the solution in evaluate_df for analysis:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"evaluate_df.merit_sol = value.(evaluate_df.merit);\nevaluate_df.enroll_sol = value.(evaluate_df.enroll);\nevaluate_df","category":"page"},{"location":"tutorials/student_enrollment/#Solution-analysis","page":"Logistic regression with GLM.jl","title":"Solution analysis","text":"","category":"section"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"We expect that just under 2,500 students will enroll:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"sum(evaluate_df.enroll_sol)","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"We awarded merit scholarships to approximately 1 in 6 students:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"count(evaluate_df.merit_sol .> 1e-5)","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"The average merit scholarship was worth just over $1,000:","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"1_000 * Statistics.mean(evaluate_df.merit_sol[evaluate_df.merit_sol .> 1e-5])","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"","category":"page"},{"location":"tutorials/student_enrollment/","page":"Logistic regression with GLM.jl","title":"Logistic regression with GLM.jl","text":"This page was generated using Literate.jl.","category":"page"},{"location":"manual/GLM/#GLM.jl","page":"GLM.jl","title":"GLM.jl","text":"","category":"section"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"GLM.jl is a library for fitting generalized linear models in Julia.","category":"page"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"MathOptAI.jl supports embedding two types of regression models from GLM:","category":"page"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"GLM.lm(X, Y)\nGLM.glm(X, Y, GLM.Bernoulli())","category":"page"},{"location":"manual/GLM/#Linear-regression","page":"GLM.jl","title":"Linear regression","text":"","category":"section"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"The input x to add_predictor must be a vector with the same number of elements as columns in the training matrix. The return is a vector of JuMP variables with a single element.","category":"page"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"using GLM, JuMP, MathOptAI\nX, Y = rand(10, 2), rand(10);\npredictor = GLM.lm(X, Y);\nmodel = Model();\n@variable(model, x[1:2]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"manual/GLM/#Logistic-regression","page":"GLM.jl","title":"Logistic regression","text":"","category":"section"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"The input x to add_predictor must be a vector with the same number of elements as columns in the training matrix. The return is a vector of JuMP variables with a single element.","category":"page"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"using GLM, JuMP, MathOptAI\nX, Y = rand(10, 2), rand(Bool, 10);\npredictor = GLM.glm(X, Y, GLM.Bernoulli());\nmodel = Model();\n@variable(model, x[1:2]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"manual/GLM/#DataFrames","page":"GLM.jl","title":"DataFrames","text":"","category":"section"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"DataFrames.jl can be used with GLM.jl.","category":"page"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"The input x to add_predictor must be a DataFrame with the same feature columns as the training DataFrame. The return is a vector of JuMP variables, with one element for each row in the DataFrame.","category":"page"},{"location":"manual/GLM/","page":"GLM.jl","title":"GLM.jl","text":"using DataFrames, GLM, JuMP, MathOptAI\ntrain_df = DataFrames.DataFrame(x1 = rand(10), x2 = rand(10));\ntrain_df.y = 1.0 .* train_df.x1 + 2.0 .* train_df.x2 .+ rand(10);\npredictor = GLM.lm(GLM.@formula(y ~ x1 + x2), train_df);\nmodel = Model();\ntest_df = DataFrames.DataFrame(\n    x1 = rand(6),\n    x2 = @variable(model, [1:6]),\n);\ntest_df.y, _ = MathOptAI.add_predictor(model, predictor, test_df);\ntest_df.y","category":"page"},{"location":"developers/design_principles/#Design-principles","page":"Design principles","title":"Design principles","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"This project is inspired by two existing projects:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"OMLT\ngurobi-machinelearning","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"OMLT is a framework built around Pyomo, and gurobi-machinelearning is a framework build around gurobipy.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"These projects served as inspiration, but we also departed from them in some carefully considered ways.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"All of our design decisions were guided by two principles:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"To be simple\nTo leverage Julia's Pkg extensions and multiple dispatch.","category":"page"},{"location":"developers/design_principles/#Terminology","page":"Design principles","title":"Terminology","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Because the field is relatively new, there is no settled choice of terminology.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"MathOptAI chooses to use \"predictor\" as the synonym for the machine learning model. Hence, we have AbstractPredictor, add_predictor, and build_predictor.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"In contrast, gurobi-machinelearning tends to use \"regression model\" and OMLT uses \"formulation.\"","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"We choose \"predictor\" because all models we implement are of the form y = f(x).","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"We do not use \"machine learning model\" because we have support for the linear and logistic regression models of classical statistical fitting. We could have used \"regression model,\" but we find that models like neural networks and binary decision trees are not commonly thought of as regression models.","category":"page"},{"location":"developers/design_principles/#Inputs-are-vectors","page":"Design principles","title":"Inputs are vectors","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"MathOptAI assumes that all inputs x and outputs y to y = predictor(x) are Base.Vectors.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"We make this choice for simplicity.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"In our opinion, Julia libraries often take a laissez-faire approach to the types that they support. In the optimistic case, this can lead to novel behavior by combining two packages that the package author had previously not considered or tested. In the pessimistic case, this can lead to incorrect results or cryptic error messages.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Exceptions to the Vector rule will be carefully considered and tested.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Currently, there are two exceptions:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"If x is a Matrix, then the columns of x are interpreted as independent observations, and the output y will be a Matrix with the same number of columns\nThe StatsModels extension allows x to be a DataFrames.DataFrame, if the predictor is a StatsModels.TableRegressionModel.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Exceptions 1 and 2 are combined in the StatsModels exception, so that the predictor is mapped over the rows of the DataFrames.DataFrame (which we assume will be a common use-case).","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"We choose to interpret the rows as input variables and columns as independent observations (rather than the more traditional table-based approach where columns are the input variables and rows are observations) because Julia uses column-major ordering in Matrix. Another justification follows from the Affine predictor, f(x) = Ax + b, where passing in a Matrix as x with column observations naturally leads to a Matrix output for y of the appropriate dimensions.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"We choose to make y a Vector, even for scalar outputs, to simplify code that works generically for many different predictors. Without this principle, there will inevitably be cases where a scalar and length-1 vector are confused.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"If you want to use a predictor that does not take Vector input (for example, it is an image as input to a neural network), the first preprocessing step should be to vec the input into a single Vector.","category":"page"},{"location":"developers/design_principles/#Inputs-are-provided,-outputs-are-returned","page":"Design principles","title":"Inputs are provided, outputs are returned","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The main job of MathOptAI is to embed models of the form y = predictor(x) into a JuMP model. A key design decision is how to represent the input x and output y.","category":"page"},{"location":"developers/design_principles/#gurobi-machinelearning","page":"Design principles","title":"gurobi-machinelearning","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"gurobi-machinelearning implements an API of the following general form:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"pred_constr = add_predictor_constr(model, predictor, x, y)","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Here, both the input x and the output y must be created and provided by the user, and a new object pred_constr is returned.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The benefit of this design is that pred_constr can contain statistics about the reformulation (for example, the number of variables that were added), and it can be used to delete a predictor from model.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The downside is that the user must ensure that the shape and size of y is correct.","category":"page"},{"location":"developers/design_principles/#OMLT","page":"Design principles","title":"OMLT","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"OMLT implements an API of the following general form:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"model.pred_constr = OmltBlock()\nmodel.pred_constr.build_formulation(predictor)\nx, y = model.pred_constr.inputs, model.pred_constr.outputs","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"First, a new OmltBlock() is created. Then the formulation is built inside the block, and both the input and output are provided to the user.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The benefit of this design is that pred_constr can contain statistics about the reformulation (for example, the number of variables that were added), and it can be used to delete a predictor from model.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The downside is that the user must often write additional constraints to connect the input and output of the OmltBlock to their existing decision variables:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"#connect pyomo model input and output to the neural network\n@model.Constraint()\ndef connect_input(mdl):\n    return mdl.input == mdl.nn.inputs[0]\n\n@model.Constraint()\ndef connect_output(mdl):\n    return mdl.output == mdl.nn.outputs[0]","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"A second downside is that the predictor must describe the input and output dimension; these cannot be inferred automatically. As one example, this means that it cannot do the following:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"# Not possible because dimension not given\nmodel.pred_constr.build_formulation(ReLU())","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"In the context of MathOptAI, something like ReLU() is useful so that we can map generic layers like Flux.relu => MathOptAI.ReLU(), and so that we do not duplicate required dimension information in input and predictor (see the MathOptAI section below).","category":"page"},{"location":"developers/design_principles/#MathOptAI","page":"Design principles","title":"MathOptAI","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The main entry-point to MathOptAI is add_predictor:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"y, formulation = MathOptAI.add_predictor(model, predictor, x)","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The user provides the input x, and the output y is returned.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The main benefit of this approach is simplicity.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"First, the user probably already has the input x as decision variables or an expression in the model, so we do not need the connect_input constraint, and because we use a full-space formulation by default, the output y will always be a vector of decision variables, which avoids the need for a connect_output constraint.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Second, predictors do not need to store dimension information, so we can have:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"y, formulation = MathOptAI.add_predictor(model, MathOptAI.ReLU(), x)","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"for any size of x.","category":"page"},{"location":"developers/design_principles/#Activations-are-predictors","page":"Design principles","title":"Activations are predictors","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"OMLT makes a distinction between layers, like full_space_dense_layer, and elementwise activation functions, like sigmoid_activation_function.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The downside to this approach is that it treats activation functions as special, leading to issues such as OMLT#125.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"In contrast, MathOptAI treats activation functions as a vector-valued predictor like any other:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"y, formulation = MathOptAI.add_predictor(model, MathOptAI.ReLU(), x)","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"This means that we can pipeline them to create predictors such as:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"function LogisticRegression(A)\n    return MathOptAI.Pipeline(MathOptAI.Affine(A), MathOptAI.Sigmoid())\nend","category":"page"},{"location":"developers/design_principles/#Controlling-transformations","page":"Design principles","title":"Controlling transformations","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Many predictors have multiple ways that they can be formulated in an optimization model. For example, ReLU implements the non-smooth nonlinear formulation y = maxx 0, while ReLUQuadratic implements a the complementarity formulation x = y - slack y slack ge 0 y * slack = 0.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Choosing the appropriate formulation for the combination of model and solver can have a large impact on the performance.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Because gurobi-machinelearning is specific to the Gurobi solver, they have a limited ability for the user to choose and implement different formulations.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"OMLT is more general, in that it has multiple ways of formulating layers such as ReLU. However, these are hard-coded into complete formulations such as omlt.neuralnet.nn_formulation.ReluBigMFormulation or omlt.neuralnet.nn_formulation.ReluComplementarityFormulation.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"In contrast, MathOptAI tries to take a maximally modular approach, where the user can control how the layers are formulated at runtime, including using a custom formulation that is not defined in MathOptAI.jl.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Currently, we achieve this with a config dictionary, which maps the various neural network layers to an AbstractPredictor. For example:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"chain = Flux.Chain(Flux.Dense(1 => 16, Flux.relu), Flux.Dense(16 => 1));\nconfig = Dict(Flux.relu => MathOptAI.ReLU())\npredictor = MathOptAI.build_predictor(chain; config)","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"Please open a GitHub issue if you have a suggestion for a better API.","category":"page"},{"location":"developers/design_principles/#Full-space-or-reduced-space","page":"Design principles","title":"Full-space or reduced-space","text":"","category":"section"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"OMLT has two ways that it can formulate neural networks: full-space and reduced-space.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"The full-space formulations add intermediate variables to represent the output of all layers.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"For example, in a Flux.Dense(2, 3, Flux.relu) layer, a full-space formulation will add an intermediate y_tmp variable to represent the output of the affine layer prior to the ReLU:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"layer = Flux.Dense(2, 3, Flux.relu)\nmodel_full_space = Model()\n@variable(model_full_space, x[1:2])\n@variable(model_full_space, y_tmp[1:3])\n@variable(model_full_space, y[1:3])\n@constraint(model_full_space, y_tmp == layer.A * x + layer.b)\n@constraint(model_full_space, y .== max.(0, y_tmp))","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"In contrast, a reduced-space formulation encodes the input-output relationship as a single nonlinear constraint:","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"layer = Flux.Dense(2, 3, Flux.relu)\nmodel_reduced_space = Model()\n@variable(model_reduced_space, x[1:2])\n@variable(model_reduced_space, y[1:3])\n@constraint(model_reduced_space, y .== max.(0, layer.A * x + layer.b))","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"In general, the full-space formulations have more variables and constraints but simpler nonlinear expressions, whereas the reduced-space formulations have fewer variables and constraints but more complicated nonlinear expressions.","category":"page"},{"location":"developers/design_principles/","page":"Design principles","title":"Design principles","text":"MathOptAI.jl implements the full-space formulation by default, but some layers support the reduced-space formulation with the ReducedSpace wrapper.","category":"page"},{"location":"manual/DecisionTree/#DecisionTree.jl","page":"DecisionTree.jl","title":"DecisionTree.jl","text":"","category":"section"},{"location":"manual/DecisionTree/","page":"DecisionTree.jl","title":"DecisionTree.jl","text":"DecisionTree.jl is a library for fitting decision trees in Julia.","category":"page"},{"location":"manual/DecisionTree/#Binary-decision-tree-regression","page":"DecisionTree.jl","title":"Binary decision tree regression","text":"","category":"section"},{"location":"manual/DecisionTree/","page":"DecisionTree.jl","title":"DecisionTree.jl","text":"Here is an example:","category":"page"},{"location":"manual/DecisionTree/","page":"DecisionTree.jl","title":"DecisionTree.jl","text":"using JuMP, MathOptAI, DecisionTree\ntruth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\nfeatures = abs.(sin.((1:10) .* (3:4)'));\nsize(features)\nlabels = truth.(Vector.(eachrow(features)));\npredictor = DecisionTree.build_tree(labels, features)\nmodel = Model();\n@variable(model, 0 <= x[1:2] <= 1);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"manual/DecisionTree/#Random-forest-regression","page":"DecisionTree.jl","title":"Random forest regression","text":"","category":"section"},{"location":"manual/DecisionTree/","page":"DecisionTree.jl","title":"DecisionTree.jl","text":"using JuMP, MathOptAI, DecisionTree\ntruth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\nfeatures = abs.(sin.((1:10) .* (3:4)'));\nsize(features)\nlabels = truth.(Vector.(eachrow(features)));\npredictor = DecisionTree.build_forest(labels, features)\nmodel = Model();\n@variable(model, 0 <= x[1:2] <= 1);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"manual/predictors/#Predictors","page":"Predictors","title":"Predictors","text":"","category":"section"},{"location":"manual/predictors/","page":"Predictors","title":"Predictors","text":"The main entry point for embedding prediction models into JuMP is add_predictor.","category":"page"},{"location":"manual/predictors/","page":"Predictors","title":"Predictors","text":"All methods use the form y, formulation = MathOptAI.add_predictor(model, predictor, x) to add the relationship y = predictor(x) to model.","category":"page"},{"location":"manual/predictors/#Supported-predictors","page":"Predictors","title":"Supported predictors","text":"","category":"section"},{"location":"manual/predictors/","page":"Predictors","title":"Predictors","text":"The following predictors are supported. See their docstrings for details:","category":"page"},{"location":"manual/predictors/","page":"Predictors","title":"Predictors","text":"Predictor Problem class Dimensions\nAffine Linear M rightarrow N\nAffineCombination Linear M rightarrow N\nBinaryDecisionTree Mixed-integer linear M rightarrow 1\nGELU Global nonlinear M rightarrow M\nGrayBox Local nonlinear M rightarrow N\nPipeline  M rightarrow N\nQuantile Local nonlinear M rightarrow N\nReLU Global nonlinear M rightarrow M\nReLUBigM Mixed-integer linear M rightarrow M\nReLUQuadratic Non-convex quadratic M rightarrow M\nReLUSOS1 Mixed-integer linear M rightarrow M\nScale Linear M rightarrow M\nSigmoid Global nonlinear M rightarrow M\nSoftMax Global nonlinear M rightarrow M\nSoftPlus Global nonlinear M rightarrow M\nTanh Global nonlinear M rightarrow M","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"This page lists the public API of MathOptAI.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"info: Info\nThis page is an unstructured list of the MathOptAI API. For a more structured overview, read the Manual or Tutorial parts of this documentation.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Load all of the public the API into the current scope with:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"using MathOptAI","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Alternatively, load only the module with:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"import MathOptAI","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"and then prefix all calls with MathOptAI. to create MathOptAI.<NAME>.","category":"page"},{"location":"api/#AbstractPredictor","page":"API Reference","title":"AbstractPredictor","text":"","category":"section"},{"location":"api/#MathOptAI.AbstractPredictor","page":"API Reference","title":"MathOptAI.AbstractPredictor","text":"abstract type AbstractPredictor end\n\nAn abstract type representing different types of prediction models.\n\nMethods\n\nAll subtypes must implement:\n\nadd_predictor\nbuild_predictor\n\n\n\n\n\n","category":"type"},{"location":"api/#add_predictor","page":"API Reference","title":"add_predictor","text":"","category":"section"},{"location":"api/#MathOptAI.add_predictor","page":"API Reference","title":"MathOptAI.add_predictor","text":"MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::MathOptAI.Quantile{<:AbstractGPs.PosteriorGP},\n    x::Vector,\n)\n\nAdd the quantiles of a trained Gaussian Process from AbstractGPs.jl to model.\n\nExample\n\njulia> using JuMP, MathOptAI, AbstractGPs\n\njulia> x_data = 2π .* (0.0:0.1:1.0);\n\njulia> y_data = sin.(x_data);\n\njulia> fx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.1);\n\njulia> p_fx = AbstractGPs.posterior(fx, y_data);\n\njulia> model = Model();\n\njulia> @variable(model, 1 <= x[1:1] <= 6, start = 3);\n\njulia> predictor = MathOptAI.Quantile(p_fx, [0.1, 0.9]);\n\njulia> y, _ = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_quantile[1]\n moai_quantile[2]\n\njulia> @objective(model, Max, y[2] - y[1])\nmoai_quantile[2] - moai_quantile[1]\n\n\n\n\n\nMathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::StatsModels.TableRegressionModel,\n    x::DataFrames.DataFrame;\n    kwargs...,\n)\n\nAdd a trained regression model from StatsModels.jl to model, using the DataFrame x as input.\n\nIn most cases, predictor should be a GLM.jl predictor supported by MathOptAI, but trained using @formula and a DataFrame instead of the raw matrix input.\n\nIn general, x may have some columns that are constant (Float64) and some columns that are JuMP decision variables.\n\nKeyword arguments\n\nAll keyword arguments are passed to the corresponding add_predictor of the GLM extension.\n\nExample\n\njulia> using DataFrames, GLM, JuMP, MathOptAI\n\njulia> train_df = DataFrames.DataFrame(x1 = rand(10), x2 = rand(10));\n\njulia> train_df.y = 1.0 .* train_df.x1 + 2.0 .* train_df.x2 .+ rand(10);\n\njulia> predictor = GLM.lm(GLM.@formula(y ~ x1 + x2), train_df);\n\njulia> model = Model();\n\njulia> test_df = DataFrames.DataFrame(\n           x1 = rand(6),\n           x2 = @variable(model, [1:6]),\n       );\n\njulia> test_df.y, _ = MathOptAI.add_predictor(model, predictor, test_df);\n\njulia> test_df.y\n6-element Vector{VariableRef}:\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n\n\n\n\n\nadd_predictor(\n    model::JuMP.AbstractModel,\n    predictor::Any,\n    x::Vector;\n    reduced_space::Bool = false,\n    kwargs...,\n)::Tuple{<:Vector,<:AbstractFormulation}\n\nReturn a Vector representing y such that y = predictor(x) and an AbstractFormulation containing the variables and constraints that were added to the model.\n\nThe element type of x is deliberately unspecified. The vector x may contain any mix of scalar constants, JuMP decision variables, and scalar JuMP functions like AffExpr, QuadExpr, or NonlinearExpr.\n\nKeyword arguments\n\nreduced_space: if true, wrap predictor in ReducedSpace before adding to the model.\n\nAll other keyword arguments are passed to build_predictor.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.Affine([2.0, 3.0])\nAffine(A, b) [input: 2, output: 1]\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> formulation\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [1]\n  └ 2 x[1] + 3 x[2] - moai_Affine[1] = 0\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x; reduced_space = true);\n\njulia> y\n1-element Vector{AffExpr}:\n 2 x[1] + 3 x[2]\n\njulia> formulation\nReducedSpace(Affine(A, b) [input: 2, output: 1])\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\nadd_predictor(model::JuMP.AbstractModel, predictor, x::Matrix)\n\nReturn a Matrix, representing y such that y[:, i] = predictor(x[:, i]) for each column i.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2, 1:3]);\n\njulia> f = MathOptAI.Affine([2.0, 3.0])\nAffine(A, b) [input: 2, output: 1]\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1×3 Matrix{VariableRef}:\n moai_Affine[1]  moai_Affine[1]  moai_Affine[1]\n\njulia> formulation\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [1]\n  └ 2 x[1,1] + 3 x[2,1] - moai_Affine[1] = 0\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [1]\n  └ 2 x[1,2] + 3 x[2,2] - moai_Affine[1] = 0\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [1]\n  └ 2 x[1,3] + 3 x[2,3] - moai_Affine[1] = 0\n\n\n\n\n\n","category":"function"},{"location":"api/#build_predictor","page":"API Reference","title":"build_predictor","text":"","category":"section"},{"location":"api/#MathOptAI.build_predictor-Tuple{MathOptAI.AbstractPredictor}","page":"API Reference","title":"MathOptAI.build_predictor","text":"build_predictor(extension; kwargs...)::AbstractPredictor\n\nA uniform interface to convert various extension types to an AbstractPredictor.\n\nSee the various extension docstrings for details.\n\n\n\n\n\n","category":"method"},{"location":"api/#Affine","page":"API Reference","title":"Affine","text":"","category":"section"},{"location":"api/#MathOptAI.Affine","page":"API Reference","title":"MathOptAI.Affine","text":"Affine(\n    A::Matrix{T},\n    b::Vector{T} = zeros(T, size(A, 1)),\n) where {T} <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = A x + b\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Affine([2.0 3.0], [4.0])\nAffine(A, b) [input: 2, output: 1]\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> formulation\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [3]\n  ├ moai_Affine[1] ≥ 4\n  ├ moai_Affine[1] ≤ 12\n  └ 2 x[1] + 3 x[2] - moai_Affine[1] = -4\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n1-element Vector{AffExpr}:\n 2 x[1] + 3 x[2] + 4\n\njulia> formulation\nReducedSpace(Affine(A, b) [input: 2, output: 1])\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#AffineCombination","page":"API Reference","title":"AffineCombination","text":"","category":"section"},{"location":"api/#MathOptAI.AffineCombination","page":"API Reference","title":"MathOptAI.AffineCombination","text":"AffineCombination(\n    predictors::Vector{<:AbstractPredictor},\n    weights::Vector{Float64},\n    constant::Vector{Float64},\n)\n\nAn AbstractPredictor that represents the linear combination of other predictors.\n\nThe main purpose of this predictor is to model random forests and gradient boosted trees.\n\nA random forest is the mean a set of BinaryDecisionTree\nA gradient boosted tree is the sum of a set of BinaryDecisionTree\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> rhs = MathOptAI.BinaryDecisionTree(1, 1.0, 0, 1)\nBinaryDecisionTree{Float64,Int64} [leaves=2, depth=2]\n\njulia> lhs = MathOptAI.BinaryDecisionTree(1, -0.1, -1, 0)\nBinaryDecisionTree{Float64,Int64} [leaves=2, depth=2]\n\njulia> tree_1 = MathOptAI.BinaryDecisionTree(1, 0.0, -1, rhs);\n\njulia> tree_2 = MathOptAI.BinaryDecisionTree(1, 0.9, lhs, 1);\n\njulia> random_forest = MathOptAI.AffineCombination(\n           [tree_1, tree_2],\n           [0.5, 0.5],\n           [0.0],\n       )\nAffineCombination\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n└ 1.0 * [0.0]\n\njulia> model = Model();\n\njulia> @variable(model, -3 <= x[1:1] <= 5);\n\njulia> y, formulation = MathOptAI.add_predictor(model, random_forest, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_AffineCombination[1]\n\njulia> formulation\nAffineCombination\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ 0.5 * BinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n└ 1.0 * [0.0]\n├ variables [1]\n│ └ moai_AffineCombination[1]\n└ constraints [1]\n  └ 0.5 moai_BinaryDecisionTree_value[1] + 0.5 moai_BinaryDecisionTree_value[1] - moai_AffineCombination[1] = 0\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ variables [4]\n│ ├ moai_BinaryDecisionTree_value[1]\n│ ├ moai_BinaryDecisionTree_z[1]\n│ ├ moai_BinaryDecisionTree_z[2]\n│ └ moai_BinaryDecisionTree_z[3]\n└ constraints [7]\n  ├ moai_BinaryDecisionTree_z[1] + moai_BinaryDecisionTree_z[2] + moai_BinaryDecisionTree_z[3] = 1\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ -1.0e-6}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≤ 0.999999}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 1}\n  └ moai_BinaryDecisionTree_z[1] - moai_BinaryDecisionTree_z[3] + moai_BinaryDecisionTree_value[1] = 0\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ variables [4]\n│ ├ moai_BinaryDecisionTree_value[1]\n│ ├ moai_BinaryDecisionTree_z[1]\n│ ├ moai_BinaryDecisionTree_z[2]\n│ └ moai_BinaryDecisionTree_z[3]\n└ constraints [7]\n  ├ moai_BinaryDecisionTree_z[1] + moai_BinaryDecisionTree_z[2] + moai_BinaryDecisionTree_z[3] = 1\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ 0.899999}\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ -0.100001}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≤ 0.899999}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≥ -0.1}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 0.9}\n  └ moai_BinaryDecisionTree_z[1] - moai_BinaryDecisionTree_z[3] + moai_BinaryDecisionTree_value[1] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#BinaryDecisionTree","page":"API Reference","title":"BinaryDecisionTree","text":"","category":"section"},{"location":"api/#MathOptAI.BinaryDecisionTree","page":"API Reference","title":"MathOptAI.BinaryDecisionTree","text":"BinaryDecisionTree{K,V}(\n    feat_id::Int,\n    feat_value::K,\n    lhs::Union{V,BinaryDecisionTree{K,V}},\n    rhs::Union{V,BinaryDecisionTree{K,V}},\n    atol::Float64 = 1e-6,\n)\n\nAn AbstractPredictor that represents a binary decision tree.\n\nIf x[feat_id] <= feat_value - atol, then return lhs\nIf x[feat_id] >= feat_value, then return rhs\n\nExample\n\nTo represent the tree x[1] <= 0.0 ? -1 : (x[1] <= 1.0 ? 0 : 1), do:\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:1]);\n\njulia> f = MathOptAI.BinaryDecisionTree{Float64,Int}(\n           1,\n           0.0,\n           -1,\n           MathOptAI.BinaryDecisionTree{Float64,Int}(1, 1.0, 0, 1),\n       )\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_BinaryDecisionTree_value[1]\n\njulia> formulation\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n├ variables [4]\n│ ├ moai_BinaryDecisionTree_value[1]\n│ ├ moai_BinaryDecisionTree_z[1]\n│ ├ moai_BinaryDecisionTree_z[2]\n│ └ moai_BinaryDecisionTree_z[3]\n└ constraints [7]\n  ├ moai_BinaryDecisionTree_z[1] + moai_BinaryDecisionTree_z[2] + moai_BinaryDecisionTree_z[3] = 1\n  ├ moai_BinaryDecisionTree_z[1] --> {x[1] ≤ -1.0e-6}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[2] --> {x[1] ≤ 0.999999}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 0}\n  ├ moai_BinaryDecisionTree_z[3] --> {x[1] ≥ 1}\n  └ moai_BinaryDecisionTree_z[1] - moai_BinaryDecisionTree_z[3] + moai_BinaryDecisionTree_value[1] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#GELU","page":"API Reference","title":"GELU","text":"","category":"section"},{"location":"api/#MathOptAI.GELU","page":"API Reference","title":"MathOptAI.GELU","text":"GeLU() <: AbstractPredictor\n\nAn AbstractPredictor representing the Gaussian Error Linear Units function:\n\ny approx x * (1 + tanh(sqrt(2  pi) * (x + 0044715 x^3)))  2\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.GELU()\nGELU()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_GELU[1]\n moai_GELU[2]\n\njulia> formulation\nGELU()\n├ variables [2]\n│ ├ moai_GELU[1]\n│ └ moai_GELU[2]\n└ constraints [6]\n  ├ moai_GELU[1] ≥ -0.17\n  ├ moai_GELU[1] ≤ 0.8411919906082768\n  ├ moai_GELU[1] - ((0.5 x[1]) * (1.0 + tanh(0.7978845608028654 * (x[1] + (0.044715 * (x[1] ^ 3.0)))))) = 0\n  ├ moai_GELU[2] ≥ -0.17\n  ├ moai_GELU[2] ≤ 1.954597694087775\n  └ moai_GELU[2] - ((0.5 x[2]) * (1.0 + tanh(0.7978845608028654 * (x[2] + (0.044715 * (x[2] ^ 3.0)))))) = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#GrayBox","page":"API Reference","title":"GrayBox","text":"","category":"section"},{"location":"api/#MathOptAI.GrayBox","page":"API Reference","title":"MathOptAI.GrayBox","text":"GrayBox(\n    output_size::Function,\n    callback::Function;\n    has_hessian::Bool = false,\n) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = f(x)\n\nas a user-defined nonlinear operator.\n\nArguments\n\noutput_size(x::Vector):Int: given an input vector x, return the dimension of the output vector\ncallback(x::Vector)::NamedTuple -> (;value, jacobian[, hessian]): given an input vector x, return a NamedTuple that computes the primal value and Jacobian of the output value with respect to the input. jacobian[j, i] is the partial derivative of value[j] with respect to x[i].\nhas_hessian: if true, the callback additionally contains a field hessian, which is an N × N × M matrix, where hessian[i, j, k] is the partial derivative of value[k] with respect to x[i] and x[j].\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.GrayBox(\n           x -> 2,\n           x -> (value = x.^2, jacobian = [2 * x[1] 0.0; 0.0 2 * x[2]]),\n       );\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_GrayBox[1]\n moai_GrayBox[2]\n\njulia> formulation\nGrayBox\n├ variables [2]\n│ ├ moai_GrayBox[1]\n│ └ moai_GrayBox[2]\n└ constraints [2]\n  ├ op_##330(x[1], x[2]) - moai_GrayBox[1] = 0\n  └ op_##331(x[1], x[2]) - moai_GrayBox[2] = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n op_##332(x[1], x[2])\n op_##333(x[1], x[2])\n\njulia> formulation\nReducedSpace(GrayBox)\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#Pipeline","page":"API Reference","title":"Pipeline","text":"","category":"section"},{"location":"api/#MathOptAI.Pipeline","page":"API Reference","title":"MathOptAI.Pipeline","text":"Pipeline(layers::Vector{AbstractPredictor}) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = (l_1 circ ldots circ l_N)(x)\n\nwhere l_i are a list of other AbstractPredictors.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.Pipeline(\n           MathOptAI.Affine([1.0 2.0], [0.0]),\n           MathOptAI.ReLUQuadratic(),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 2, output: 1]\n * ReLUQuadratic(nothing)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_ReLU[1]\n\njulia> formulation\nAffine(A, b) [input: 2, output: 1]\n├ variables [1]\n│ └ moai_Affine[1]\n└ constraints [1]\n  └ x[1] + 2 x[2] - moai_Affine[1] = 0\nReLUQuadratic(nothing)\n├ variables [2]\n│ ├ moai_ReLU[1]\n│ └ moai_z[1]\n└ constraints [4]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_z[1] ≥ 0\n  ├ moai_Affine[1] - moai_ReLU[1] + moai_z[1] = 0\n  └ moai_ReLU[1]*moai_z[1] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#PytorchModel","page":"API Reference","title":"PytorchModel","text":"","category":"section"},{"location":"api/#MathOptAI.PytorchModel","page":"API Reference","title":"MathOptAI.PytorchModel","text":"PytorchModel(filename::String)\n\nA wrapper struct for loading a PyTorch model.\n\nThe only supported file extension is .pt, where the .pt file has been created using torch.save(model, filename).\n\nwarning: Warning\nTo use PytorchModel, your code must load the PythonCall package:import PythonCall\n\nExample\n\njulia> using MathOptAI\n\njulia> using PythonCall  #  This line is important!\n\njulia> predictor = PytorchModel(\"model.pt\");\n\n\n\n\n\n","category":"type"},{"location":"api/#Quantile","page":"API Reference","title":"Quantile","text":"","category":"section"},{"location":"api/#MathOptAI.Quantile","page":"API Reference","title":"MathOptAI.Quantile","text":"Quantile{D}(distribution::D, quantiles::Vector{Float64}) where {D}\n\nAn AbstractPredictor that represents the quantiles of distribution.\n\nExample\n\njulia> using JuMP, Distributions, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, 1 <= x <= 2);\n\njulia> predictor = MathOptAI.Quantile([0.1, 0.9]) do x\n           return Distributions.Normal(x, 3 - x)\n       end\nQuantile(_, [0.1, 0.9])\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, [x]);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_quantile[1]\n moai_quantile[2]\n\njulia> formulation\nQuantile(_, [0.1, 0.9])\n├ variables [2]\n│ ├ moai_quantile[1]\n│ └ moai_quantile[2]\n└ constraints [2]\n  ├ moai_quantile[1] - op_quantile_0.1(x) = 0\n  └ moai_quantile[2] - op_quantile_0.9(x) = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#ReducedSpace","page":"API Reference","title":"ReducedSpace","text":"","category":"section"},{"location":"api/#MathOptAI.ReducedSpace","page":"API Reference","title":"MathOptAI.ReducedSpace","text":"ReducedSpace(predictor::AbstractPredictor)\n\nA wrapper type for other predictors that implement a reduced-space formulation.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> predictor = MathOptAI.ReducedSpace(MathOptAI.ReLU());\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n max(0.0, x[1])\n max(0.0, x[2])\n\n\n\n\n\n","category":"type"},{"location":"api/#ReLU","page":"API Reference","title":"ReLU","text":"","category":"section"},{"location":"api/#MathOptAI.ReLU","page":"API Reference","title":"MathOptAI.ReLU","text":"ReLU() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nas a non-smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLU()\nReLU()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLU()\n├ variables [2]\n│ ├ moai_ReLU[1]\n│ └ moai_ReLU[2]\n└ constraints [6]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_ReLU[1] - max(0.0, x[1]) = 0\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  └ moai_ReLU[2] - max(0.0, x[2]) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n max(0.0, x[1])\n max(0.0, x[2])\n\njulia> formulation\nReducedSpace(ReLU())\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#ReLUBigM","page":"API Reference","title":"ReLUBigM","text":"","category":"section"},{"location":"api/#MathOptAI.ReLUBigM","page":"API Reference","title":"MathOptAI.ReLUBigM","text":"ReLUBigM(M::Float64) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nvia the big-M MIP reformulation:\n\nbeginaligned\ny ge 0            \ny ge x            \ny le M z          \ny le x + M(1 - z) \nz in0 1\nendaligned\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -3 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLUBigM(100.0)\nReLUBigM(100.0)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLUBigM(100.0)\n├ variables [4]\n│ ├ moai_ReLU[1]\n│ ├ moai_ReLU[2]\n│ ├ moai_z[1]\n│ └ moai_z[2]\n└ constraints [12]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_z[1] binary\n  ├ -x[1] + moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] - moai_z[1] ≤ 0\n  ├ -x[1] + moai_ReLU[1] + 3 moai_z[1] ≤ 3\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  ├ moai_z[2] binary\n  ├ -x[2] + moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] - 2 moai_z[2] ≤ 0\n  └ -x[2] + moai_ReLU[2] + 3 moai_z[2] ≤ 3\n\n\n\n\n\n","category":"type"},{"location":"api/#ReLUQuadratic","page":"API Reference","title":"ReLUQuadratic","text":"","category":"section"},{"location":"api/#MathOptAI.ReLUQuadratic","page":"API Reference","title":"MathOptAI.ReLUQuadratic","text":"ReLUQuadratic(; relaxation_parameter = nothing) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nby the reformulation:\n\nbeginaligned\nx = y - z \ny cdot z = 0 \ny z ge 0\nendaligned\n\nIf relaxation_parameter is set to a value ϵ, the constraints become:\n\nbeginaligned\nx = y - z \ny cdot z leq epsilon \ny z ge 0\nendaligned\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLUQuadratic()\nReLUQuadratic(nothing)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLUQuadratic(nothing)\n├ variables [4]\n│ ├ moai_ReLU[1]\n│ ├ moai_ReLU[2]\n│ ├ moai_z[1]\n│ └ moai_z[2]\n└ constraints [12]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_z[1] ≥ 0\n  ├ moai_z[1] ≤ 1\n  ├ x[1] - moai_ReLU[1] + moai_z[1] = 0\n  ├ moai_ReLU[1]*moai_z[1] = 0\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  ├ moai_z[2] ≥ 0\n  ├ moai_z[2] ≤ 1\n  ├ x[2] - moai_ReLU[2] + moai_z[2] = 0\n  └ moai_ReLU[2]*moai_z[2] = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#ReLUSOS1","page":"API Reference","title":"ReLUSOS1","text":"","category":"section"},{"location":"api/#MathOptAI.ReLUSOS1","page":"API Reference","title":"MathOptAI.ReLUSOS1","text":"ReLUSOS1() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = max0 x\n\nby the reformulation:\n\nbeginaligned\nx = y - z           \ny z in SOS1    \ny z ge 0\nendaligned\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.ReLUSOS1()\nReLUSOS1()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_ReLU[1]\n moai_ReLU[2]\n\njulia> formulation\nReLUSOS1()\n├ variables [4]\n│ ├ moai_ReLU[1]\n│ ├ moai_ReLU[2]\n│ ├ moai_z[1]\n│ └ moai_z[2]\n└ constraints [12]\n  ├ moai_ReLU[1] ≥ 0\n  ├ moai_ReLU[1] ≤ 1\n  ├ moai_z[1] ≥ 0\n  ├ moai_z[1] ≤ 1\n  ├ x[1] - moai_ReLU[1] + moai_z[1] = 0\n  ├ [moai_ReLU[1], moai_z[1]] ∈ MathOptInterface.SOS1{Float64}([1.0, 2.0])\n  ├ moai_ReLU[2] ≥ 0\n  ├ moai_ReLU[2] ≤ 2\n  ├ moai_z[2] ≥ 0\n  ├ moai_z[2] ≤ 1\n  ├ x[2] - moai_ReLU[2] + moai_z[2] = 0\n  └ [moai_ReLU[2], moai_z[2]] ∈ MathOptInterface.SOS1{Float64}([1.0, 2.0])\n\n\n\n\n\n","category":"type"},{"location":"api/#Scale","page":"API Reference","title":"Scale","text":"","category":"section"},{"location":"api/#MathOptAI.Scale","page":"API Reference","title":"MathOptAI.Scale","text":"Scale(\n    scale::Vector{T},\n    bias::Vector{T},\n) where {T} <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = Diag(scale)x + bias\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Scale([2.0, 3.0], [4.0, 5.0])\nScale(scale, bias)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_Scale[1]\n moai_Scale[2]\n\njulia> formulation\nScale(scale, bias)\n├ variables [2]\n│ ├ moai_Scale[1]\n│ └ moai_Scale[2]\n└ constraints [6]\n  ├ moai_Scale[1] ≥ 4\n  ├ moai_Scale[1] ≤ 6\n  ├ moai_Scale[2] ≥ 5\n  ├ moai_Scale[2] ≤ 11\n  ├ 2 x[1] - moai_Scale[1] = -4\n  └ 3 x[2] - moai_Scale[2] = -5\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{AffExpr}:\n 2 x[1] + 4\n 3 x[2] + 5\n\njulia> formulation\nReducedSpace(Scale(scale, bias))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#Sigmoid","page":"API Reference","title":"Sigmoid","text":"","category":"section"},{"location":"api/#MathOptAI.Sigmoid","page":"API Reference","title":"MathOptAI.Sigmoid","text":"Sigmoid() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = frac11 + e^-x\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Sigmoid()\nSigmoid()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_Sigmoid[1]\n moai_Sigmoid[2]\n\njulia> formulation\nSigmoid()\n├ variables [2]\n│ ├ moai_Sigmoid[1]\n│ └ moai_Sigmoid[2]\n└ constraints [6]\n  ├ moai_Sigmoid[1] ≥ 0.2689414213699951\n  ├ moai_Sigmoid[1] ≤ 0.7310585786300049\n  ├ moai_Sigmoid[1] - (1.0 / (1.0 + exp(-x[1]))) = 0\n  ├ moai_Sigmoid[2] ≥ 0.2689414213699951\n  ├ moai_Sigmoid[2] ≤ 0.8807970779778823\n  └ moai_Sigmoid[2] - (1.0 / (1.0 + exp(-x[2]))) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n 1.0 / (1.0 + exp(-x[1]))\n 1.0 / (1.0 + exp(-x[2]))\n\njulia> formulation\nReducedSpace(Sigmoid())\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#SoftMax","page":"API Reference","title":"SoftMax","text":"","category":"section"},{"location":"api/#MathOptAI.SoftMax","page":"API Reference","title":"MathOptAI.SoftMax","text":"SoftMax() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = frace^xe^x_1\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> f = MathOptAI.SoftMax()\nSoftMax()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_SoftMax[1]\n moai_SoftMax[2]\n\njulia> formulation\nSoftMax()\n├ variables [3]\n│ ├ moai_SoftMax_denom[1]\n│ ├ moai_SoftMax[1]\n│ └ moai_SoftMax[2]\n└ constraints [8]\n  ├ moai_SoftMax_denom[1] ≥ 0\n  ├ moai_SoftMax_denom[1] - (0.0 + exp(x[2]) + exp(x[1])) = 0\n  ├ moai_SoftMax[1] ≥ 0\n  ├ moai_SoftMax[1] ≤ 1\n  ├ moai_SoftMax[1] - (exp(x[1]) / moai_SoftMax_denom[1]) = 0\n  ├ moai_SoftMax[2] ≥ 0\n  ├ moai_SoftMax[2] ≤ 1\n  └ moai_SoftMax[2] - (exp(x[2]) / moai_SoftMax_denom[1]) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n exp(x[1]) / moai_SoftMax_denom[1]\n exp(x[2]) / moai_SoftMax_denom[1]\n\njulia> formulation\nReducedSpace(SoftMax())\n├ variables [1]\n│ └ moai_SoftMax_denom[1]\n└ constraints [2]\n  ├ moai_SoftMax_denom[1] ≥ 0\n  └ moai_SoftMax_denom[1] - (0.0 + exp(x[2]) + exp(x[1])) = 0\n\n\n\n\n\n","category":"type"},{"location":"api/#SoftPlus","page":"API Reference","title":"SoftPlus","text":"","category":"section"},{"location":"api/#MathOptAI.SoftPlus","page":"API Reference","title":"MathOptAI.SoftPlus","text":"SoftPlus(; beta = 1.0) <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = frac1beta log(1 + e^beta x)\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.SoftPlus(; beta = 2.0)\nSoftPlus(2.0)\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_SoftPlus[1]\n moai_SoftPlus[2]\n\njulia> formulation\nSoftPlus(2.0)\n├ variables [2]\n│ ├ moai_SoftPlus[1]\n│ └ moai_SoftPlus[2]\n└ constraints [6]\n  ├ moai_SoftPlus[1] ≥ 0.0634640055214863\n  ├ moai_SoftPlus[1] ≤ 1.0634640055214863\n  ├ moai_SoftPlus[1] - (log(1.0 + exp(2 x[1])) / 2.0) = 0\n  ├ moai_SoftPlus[2] ≥ 0.0634640055214863\n  ├ moai_SoftPlus[2] ≤ 2.0090749639589047\n  └ moai_SoftPlus[2] - (log(1.0 + exp(2 x[2])) / 2.0) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n log(1.0 + exp(2 x[1])) / 2.0\n log(1.0 + exp(2 x[2])) / 2.0\n\njulia> formulation\nReducedSpace(SoftPlus(2.0))\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#Tanh","page":"API Reference","title":"Tanh","text":"","category":"section"},{"location":"api/#MathOptAI.Tanh","page":"API Reference","title":"MathOptAI.Tanh","text":"Tanh() <: AbstractPredictor\n\nAn AbstractPredictor that represents the relationship:\n\ny = tanh(x)\n\nas a smooth nonlinear constraint.\n\nExample\n\njulia> using JuMP, MathOptAI\n\njulia> model = Model();\n\njulia> @variable(model, -1 <= x[i in 1:2] <= i);\n\njulia> f = MathOptAI.Tanh()\nTanh()\n\njulia> y, formulation = MathOptAI.add_predictor(model, f, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_Tanh[1]\n moai_Tanh[2]\n\njulia> formulation\nTanh()\n├ variables [2]\n│ ├ moai_Tanh[1]\n│ └ moai_Tanh[2]\n└ constraints [6]\n  ├ moai_Tanh[1] ≥ -0.7615941559557649\n  ├ moai_Tanh[1] ≤ 0.7615941559557649\n  ├ moai_Tanh[1] - tanh(x[1]) = 0\n  ├ moai_Tanh[2] ≥ -0.7615941559557649\n  ├ moai_Tanh[2] ≤ 0.9640275800758169\n  └ moai_Tanh[2] - tanh(x[2]) = 0\n\njulia> y, formulation =\n           MathOptAI.add_predictor(model, MathOptAI.ReducedSpace(f), x);\n\njulia> y\n2-element Vector{NonlinearExpr}:\n tanh(x[1])\n tanh(x[2])\n\njulia> formulation\nReducedSpace(Tanh())\n├ variables [0]\n└ constraints [0]\n\n\n\n\n\n","category":"type"},{"location":"api/#VectorNonlinearOracle","page":"API Reference","title":"VectorNonlinearOracle","text":"","category":"section"},{"location":"api/#MathOptAI.VectorNonlinearOracle","page":"API Reference","title":"MathOptAI.VectorNonlinearOracle","text":"VectorNonlinearOracle(x)\n\nA wrapper struct for creating an MOI.VectorNonlinearOracle.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractFormulation","page":"API Reference","title":"AbstractFormulation","text":"","category":"section"},{"location":"api/#MathOptAI.AbstractFormulation","page":"API Reference","title":"MathOptAI.AbstractFormulation","text":"abstract type AbstractFormulation end\n\nAn abstract type representing different formulations.\n\n\n\n\n\n","category":"type"},{"location":"api/#Formulation","page":"API Reference","title":"Formulation","text":"","category":"section"},{"location":"api/#MathOptAI.Formulation","page":"API Reference","title":"MathOptAI.Formulation","text":"struct Formulation{P<:AbstractPredictor} <: AbstractFormulation\n    predictor::P\n    variables::Vector{Any}\n    constraints::Vector{Any}\nend\n\nFields\n\npredictor: the predictor object used to build the formulation\nvariables: a vector of new decision variables added to the model\nconstraints: a vector of new constraints added to the model\n\nCheck the docstring of the predictor for an explanation of the formulation and the order of the elements in .variables and .constraints.\n\n\n\n\n\n","category":"type"},{"location":"api/#PipelineFormulation","page":"API Reference","title":"PipelineFormulation","text":"","category":"section"},{"location":"api/#MathOptAI.PipelineFormulation","page":"API Reference","title":"MathOptAI.PipelineFormulation","text":"struct PipelineFormulation{P<:AbstractPredictor} <: AbstractFormulation\n    predictor::P\n    layers::Vector{Any}\nend\n\nFields\n\npredictor: the predictor object used to build the formulation\nlayers: the formulation associated with each of the layers in the pipeline\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs","page":"API Reference","title":"AbstractGPs","text":"","category":"section"},{"location":"api/#MathOptAI.add_predictor-Tuple{JuMP.AbstractModel, MathOptAI.Quantile{<:AbstractGPs.PosteriorGP}, Vector}","page":"API Reference","title":"MathOptAI.add_predictor","text":"MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::MathOptAI.Quantile{<:AbstractGPs.PosteriorGP},\n    x::Vector,\n)\n\nAdd the quantiles of a trained Gaussian Process from AbstractGPs.jl to model.\n\nExample\n\njulia> using JuMP, MathOptAI, AbstractGPs\n\njulia> x_data = 2π .* (0.0:0.1:1.0);\n\njulia> y_data = sin.(x_data);\n\njulia> fx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.1);\n\njulia> p_fx = AbstractGPs.posterior(fx, y_data);\n\njulia> model = Model();\n\njulia> @variable(model, 1 <= x[1:1] <= 6, start = 3);\n\njulia> predictor = MathOptAI.Quantile(p_fx, [0.1, 0.9]);\n\njulia> y, _ = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n2-element Vector{VariableRef}:\n moai_quantile[1]\n moai_quantile[2]\n\njulia> @objective(model, Max, y[2] - y[1])\nmoai_quantile[2] - moai_quantile[1]\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionTree","page":"API Reference","title":"DecisionTree","text":"","category":"section"},{"location":"api/#MathOptAI.build_predictor-Tuple{Union{DecisionTree.DecisionTreeClassifier, DecisionTree.Ensemble, DecisionTree.Leaf, DecisionTree.Node, DecisionTree.Root}}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::Union{\n        DecisionTree.Ensemble,\n        DecisionTree.DecisionTreeClassifier,\n        DecisionTree.Leaf,\n        DecisionTree.Node,\n        DecisionTree.Root,\n    },\n)\n\nConvert a binary decision tree from DecisionTree.jl to a BinaryDecisionTree.\n\nExample\n\njulia> using JuMP, MathOptAI, DecisionTree\n\njulia> truth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\ntruth (generic function with 1 method)\n\njulia> features = abs.(sin.((1:10) .* (3:4)'));\n\njulia> size(features)\n(10, 2)\n\njulia> labels = truth.(Vector.(eachrow(features)));\n\njulia> tree = DecisionTree.build_tree(labels, features)\nDecision Tree\nLeaves: 3\nDepth:  2\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[1:2] <= 1);\n\njulia> y, _ = MathOptAI.add_predictor(model, tree, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_BinaryDecisionTree_value[1]\n\njulia> MathOptAI.build_predictor(tree)\nBinaryDecisionTree{Float64,Int64} [leaves=3, depth=2]\n\n\n\n\n\n","category":"method"},{"location":"api/#EvoTrees","page":"API Reference","title":"EvoTrees","text":"","category":"section"},{"location":"api/#MathOptAI.build_predictor-Union{Tuple{EvoTrees.EvoTree{L, 1}}, Tuple{L}} where L","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(predictor::EvoTrees.EvoTree{L,1}) where {L}\n\nConvert a boosted tree from EvoTrees.jl to an AffineCombination of BinaryDecisionTree.\n\nExample\n\njulia> using JuMP, MathOptAI, EvoTrees\n\njulia> truth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\ntruth (generic function with 1 method)\n\njulia> x_train = abs.(sin.((1:10) .* (3:4)'));\n\njulia> size(x_train)\n(10, 2)\n\njulia> y_train = truth.(Vector.(eachrow(x_train)));\n\njulia> config = EvoTrees.EvoTreeRegressor(; nrounds = 3);\n\njulia> tree = EvoTrees.fit(config; x_train, y_train);\n\njulia> model = Model();\n\njulia> @variable(model, 0 <= x[1:2] <= 1);\n\njulia> y, _ = MathOptAI.add_predictor(model, tree, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_AffineCombination[1]\n\njulia> MathOptAI.build_predictor(tree)\nAffineCombination\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n├ 1.0 * BinaryDecisionTree{Float64,Float64} [leaves=3, depth=2]\n└ 1.0 * [2.0]\n\n\n\n\n\n","category":"method"},{"location":"api/#Flux","page":"API Reference","title":"Flux","text":"","category":"section"},{"location":"api/#MathOptAI.build_predictor-Tuple{Flux.Chain}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::Flux.Chain;\n    config::Dict = Dict{Any,Any}(),\n    gray_box::Bool = false,\n    vector_nonlinear_oracle::Bool = false,\n    hessian::Bool = vector_nonlinear_oracle,\n)\n\nConvert a trained neural network from Flux.jl to a Pipeline.\n\nSupported layers\n\nFlux.Dense\nFlux.Scale\nFlux.softmax\n\nSupported activation functions\n\nFlux.relu\nFlux.sigmoid\nFlux.softplus\nFlux.tanh\n\nKeyword arguments\n\nconfig: a dictionary that maps supported Flux activation functions to AbstractPredictors that control how the activation functions are reformulated. For example, Flux.sigmoid => MathOptAI.Sigmoid() or Flux.relu => MathOptAI.QuadraticReLU().\ngray_box: if true, the neural network is added using a GrayBox formulation.\nvector_nonlinear_oracle: if true, the neural network is added using a VectorNonlinearOracle formulation.\nhessian: if true, the gray_box and vector_nonlinear_oracle formulations compute the Hessian of the output using Flux.hessian. The default for hessian is false if gray_box is used, and true if vector_nonlinear_oracle is used.\n\nCompatibility\n\nThe vector_nonlinear_oracle feature is experimental. It relies on a private API feature of Ipopt.jl that will change in a future release.\n\nIf you use this feature, you must pin the version of Ipopt.jl in your Project.toml to ensure that future updates to Ipopt.jl do not break your existing code.\n\nA known good version of Ipopt.jl is v1.8.0. Pin the version using:\n\n[compat]\nIpopt = \"=1.8.0\"\n\nExample\n\njulia> using JuMP, MathOptAI, Flux\n\njulia> chain = Flux.Chain(Flux.Dense(1 => 16, Flux.relu), Flux.Dense(16 => 1));\n\njulia> model = Model();\n\njulia> @variable(model, x[1:1]);\n\njulia> y, _ = MathOptAI.add_predictor(\n           model,\n           chain,\n           x;\n           config = Dict(Flux.relu => MathOptAI.ReLU()),\n       );\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> MathOptAI.build_predictor(\n           chain;\n           config = Dict(Flux.relu => MathOptAI.ReLU()),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLU()\n * Affine(A, b) [input: 16, output: 1]\n\njulia> MathOptAI.build_predictor(\n           chain;\n           config = Dict(Flux.relu => MathOptAI.ReLUQuadratic()),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLUQuadratic(nothing)\n * Affine(A, b) [input: 16, output: 1]\n\n\n\n\n\n","category":"method"},{"location":"api/#GLM","page":"API Reference","title":"GLM","text":"","category":"section"},{"location":"api/#MathOptAI.build_predictor-Tuple{GLM.GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Distributions.Bernoulli{Float64}, GLM.LogitLink}}}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::GLM.GeneralizedLinearModel{\n        GLM.GlmResp{Vector{Float64},GLM.Bernoulli{Float64},GLM.LogitLink},\n    };\n    sigmoid::MathOptAI.AbstractPredictor = MathOptAI.Sigmoid(),\n)\n\nConvert a trained logistic model from GLM.jl to a Pipeline layer.\n\nKeyword arguments\n\nsigmoid: the predictor to use for the sigmoid layer.\n\nExample\n\njulia> using JuMP, MathOptAI, GLM\n\njulia> X, Y = rand(10, 2), rand(Bool, 10);\n\njulia> predictor = GLM.glm(X, Y, GLM.Bernoulli());\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> y, _ = MathOptAI.add_predictor(\n           model,\n           predictor,\n           x;\n           sigmoid = MathOptAI.Sigmoid(),\n       );\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Sigmoid[1]\n\njulia> MathOptAI.build_predictor(predictor)\nPipeline with layers:\n * Affine(A, b) [input: 2, output: 1]\n * Sigmoid()\n\n\n\n\n\n","category":"method"},{"location":"api/#MathOptAI.build_predictor-Tuple{GLM.LinearModel}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(predictor::GLM.LinearModel)\n\nConvert a trained linear model from GLM.jl to an Affine layer.\n\nExample\n\njulia> using JuMP, MathOptAI, GLM\n\njulia> X, Y = rand(10, 2), rand(10);\n\njulia> predictor = GLM.lm(X, Y);\n\njulia> model = Model();\n\njulia> @variable(model, x[1:2]);\n\njulia> y, _ = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> MathOptAI.build_predictor(predictor)\nAffine(A, b) [input: 2, output: 1]\n\n\n\n\n\n","category":"method"},{"location":"api/#Lux","page":"API Reference","title":"Lux","text":"","category":"section"},{"location":"api/#MathOptAI.build_predictor-Tuple{Tuple{Lux.Chain, NamedTuple, NamedTuple}}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::Tuple{<:Lux.Chain,<:NamedTuple,<:NamedTuple};\n    config::Dict = Dict{Any,Any}(),\n)\n\nConvert a trained neural network from Lux.jl to a Pipeline.\n\nSupported layers\n\nLux.Dense\nLux.Scale\n\nSupported activation functions\n\nLux.relu\nLux.sigmoid\nLux.softplus\nLux.softmax\nLux.tanh\n\nKeyword arguments\n\nconfig: a dictionary that maps supported Lux activation functions to AbstractPredictors that control how the activation functions are reformulated. For example, Lux.sigmoid => MathOptAI.Sigmoid() or Lux.relu => MathOptAI.QuadraticReLU().\n\nExample\n\njulia> using JuMP, MathOptAI, Lux, Random\n\njulia> rng = Random.MersenneTwister();\n\njulia> chain = Lux.Chain(Lux.Dense(1 => 16, Lux.relu), Lux.Dense(16 => 1))\nChain(\n    layer_1 = Dense(1 => 16, relu),               # 32 parameters\n    layer_2 = Dense(16 => 1),                     # 17 parameters\n)         # Total: 49 parameters,\n          #        plus 0 states.\n\njulia> parameters, state = Lux.setup(rng, chain);\n\njulia> model = Model();\n\njulia> @variable(model, x[1:1]);\n\njulia> y, _ = MathOptAI.add_predictor(\n           model,\n           (chain, parameters, state),\n           x;\n           config = Dict(Lux.relu => MathOptAI.ReLU()),\n       );\n\njulia> y\n1-element Vector{VariableRef}:\n moai_Affine[1]\n\njulia> MathOptAI.build_predictor(\n           (chain, parameters, state);\n           config = Dict(Lux.relu => MathOptAI.ReLU()),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLU()\n * Affine(A, b) [input: 16, output: 1]\n\njulia> MathOptAI.build_predictor(\n           (chain, parameters, state);\n           config = Dict(Lux.relu => MathOptAI.ReLUQuadratic()),\n       )\nPipeline with layers:\n * Affine(A, b) [input: 1, output: 16]\n * ReLUQuadratic(nothing)\n * Affine(A, b) [input: 16, output: 1]\n\n\n\n\n\n","category":"method"},{"location":"api/#PythonCall","page":"API Reference","title":"PythonCall","text":"","category":"section"},{"location":"api/#MathOptAI.build_predictor-Tuple{MathOptAI.PytorchModel}","page":"API Reference","title":"MathOptAI.build_predictor","text":"MathOptAI.build_predictor(\n    predictor::MathOptAI.PytorchModel;\n    config::Dict = Dict{Any,Any}(),\n    gray_box::Bool = false,\n    vector_nonlinear_oracle::Bool = false,\n    hessian::Bool = vector_nonlinear_oracle,\n    device::String = \"cpu\",\n)\n\nConvert a trained neural network from PyTorch via PythonCall.jl to a Pipeline.\n\nSupported layers\n\nnn.GELU\nnn.Linear\nnn.ReLU\nnn.Sequential\nnn.Sigmoid\nnn.Softmax\nnn.Softplus\nnn.Tanh\n\nKeyword arguments\n\nconfig: a dictionary that maps Symbols to AbstractPredictors that control how the activation functions are reformulated. For example, :Sigmoid => MathOptAI.Sigmoid() or :ReLU => MathOptAI.QuadraticReLU(). The supported Symbols are :ReLU, :Sigmoid, :SoftMax, :SoftPlus, and :Tanh.\ngray_box: if true, the neural network is added using a GrayBox formulation.\nvector_nonlinear_oracle: if true, the neural network is added using a VectorNonlinearOracle formulation.\nhessian: if true, the gray_box and vector_nonlinear_oracle formulations compute the Hessian of the output using torch.func.hessian. The default for hessian is false if gray_box is used, and true if vector_nonlinear_oracle is used.\ndevice: device used to construct PyTorch tensors, for example, \"cuda\" to run on an Nvidia GPU.\n\nCompatibility\n\nThe vector_nonlinear_oracle feature is experimental. It relies on a private API feature of Ipopt.jl that will change in a future release.\n\nIf you use this feature, you must pin the version of Ipopt.jl in your Project.toml to ensure that future updates to Ipopt.jl do not break your existing code.\n\nA known good version of Ipopt.jl is v1.8.0. Pin the version using:\n\n[compat]\nIpopt = \"=1.8.0\"\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsModels","page":"API Reference","title":"StatsModels","text":"","category":"section"},{"location":"api/#MathOptAI.add_predictor-Tuple{JuMP.AbstractModel, StatsModels.TableRegressionModel, DataFrames.DataFrame}","page":"API Reference","title":"MathOptAI.add_predictor","text":"MathOptAI.add_predictor(\n    model::JuMP.AbstractModel,\n    predictor::StatsModels.TableRegressionModel,\n    x::DataFrames.DataFrame;\n    kwargs...,\n)\n\nAdd a trained regression model from StatsModels.jl to model, using the DataFrame x as input.\n\nIn most cases, predictor should be a GLM.jl predictor supported by MathOptAI, but trained using @formula and a DataFrame instead of the raw matrix input.\n\nIn general, x may have some columns that are constant (Float64) and some columns that are JuMP decision variables.\n\nKeyword arguments\n\nAll keyword arguments are passed to the corresponding add_predictor of the GLM extension.\n\nExample\n\njulia> using DataFrames, GLM, JuMP, MathOptAI\n\njulia> train_df = DataFrames.DataFrame(x1 = rand(10), x2 = rand(10));\n\njulia> train_df.y = 1.0 .* train_df.x1 + 2.0 .* train_df.x2 .+ rand(10);\n\njulia> predictor = GLM.lm(GLM.@formula(y ~ x1 + x2), train_df);\n\njulia> model = Model();\n\njulia> test_df = DataFrames.DataFrame(\n           x1 = rand(6),\n           x2 = @variable(model, [1:6]),\n       );\n\njulia> test_df.y, _ = MathOptAI.add_predictor(model, predictor, test_df);\n\njulia> test_df.y\n6-element Vector{VariableRef}:\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n moai_Affine[1]\n\n\n\n\n\n","category":"method"},{"location":"api/#Extensions","page":"API Reference","title":"Extensions","text":"","category":"section"},{"location":"api/#MathOptAI.add_variables","page":"API Reference","title":"MathOptAI.add_variables","text":"add_variables(\n    model::JuMP.AbstractModel,\n    x::Vector,\n    n::Int,\n    base_name::String,\n)::Vector\n\nAdd a vector of n variables to model with the base name base_name.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of model  and x as needed.\n\nThe default method is:\n\nfunction add_variables(\n    model::JuMP.AbstractModel,\n    x::Vector,\n    n::Int,\n    base_name::String,\n)\n    return JuMP.@variable(model, [1:n], base_name = base_name)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.get_variable_bounds","page":"API Reference","title":"MathOptAI.get_variable_bounds","text":"get_variable_bounds(x::JuMP.AbstractVariableRef)\n\nReturn a tuple corresponding to the (lower, upper) variable bounds of x.\n\nIf there is no bound, the value returned is missing.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x as needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.set_variable_bounds","page":"API Reference","title":"MathOptAI.set_variable_bounds","text":"set_variable_bounds(\n    cons::Vector{Any},\n    x::JuMP.AbstractVariableRef,\n    l::Any,\n    u::Any;\n    optional::Bool,\n)\n\nSet the bounds on x to l and u, and push! their corresponding constraint references to cons.\n\nIf l or u are missing, do not set the bound.\n\nIf optional = true, you may choose to silently skip setting the bounds because they are not required for correctness.\n\nThe type of l and u depends on get_variable_bounds.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x as needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.get_variable_start","page":"API Reference","title":"MathOptAI.get_variable_start","text":"get_variable_start(x::JuMP.AbstractVariableRef)\n\nGet the primal starting value of x, or return missing if one is not set.\n\nThe return value of this function is propogated through the various AbstractPredictors, and the primal start of new output variables is set using set_variable_start.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x as needed.\n\n\n\n\n\n","category":"function"},{"location":"api/#MathOptAI.set_variable_start","page":"API Reference","title":"MathOptAI.set_variable_start","text":"set_variable_start(x::JuMP.AbstractVariableRef, start::Any)\n\nSet the primal starting value of x to start, or do nothing if start is missing.\n\nThe input value start of this function is computed by propogating the primal start of the input variables (obtained with get_variable_start) through the various AbstractPredictors.\n\nExtensions\n\nThis function is a hook for JuMP extensions to interact with MathOptAI.\n\nImplement this method for subtypes of x and start as needed.\n\n\n\n\n\n","category":"function"},{"location":"manual/AbstractGPs/#AbstractGPs.jl","page":"AbstractGPs.jl","title":"AbstractGPs.jl","text":"","category":"section"},{"location":"manual/AbstractGPs/","page":"AbstractGPs.jl","title":"AbstractGPs.jl","text":"AbstractGPs.jl is a library for fitting Gaussian Processes in Julia.","category":"page"},{"location":"manual/AbstractGPs/#Basic-example","page":"AbstractGPs.jl","title":"Basic example","text":"","category":"section"},{"location":"manual/AbstractGPs/","page":"AbstractGPs.jl","title":"AbstractGPs.jl","text":"Here is an example:","category":"page"},{"location":"manual/AbstractGPs/","page":"AbstractGPs.jl","title":"AbstractGPs.jl","text":"using JuMP, MathOptAI, AbstractGPs\nx_data = 2π .* (0.0:0.1:1.0);\ny_data = sin.(x_data);\nfx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.1);\np_fx = AbstractGPs.posterior(fx, y_data);\nmodel = Model();\n@variable(model, 1 <= x[1:1] <= 6, start = 3);\npredictor = MathOptAI.Quantile(p_fx, [0.1, 0.9]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation\n@objective(model, Max, y[2] - y[1])","category":"page"},{"location":"tutorials/pytorch/#Function-fitting-with-PyTorch","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"","category":"section"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"The purpose of this tutorial is to explain how to embed a neural network model from PyTorch into JuMP.","category":"page"},{"location":"tutorials/pytorch/#Python-integration","page":"Function fitting with PyTorch","title":"Python integration","text":"","category":"section"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"MathOptAI uses PythonCall.jl to call from Julia into Python.","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"To use PytorchModel your code must load the PythonCall package:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"import PythonCall","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"PythonCall uses CondaPkg.jl to manage Python dependencies. See CondaPkg.jl for more control over how to link Julia to an existing Python environment. For example, if you have an existing Python installation (with PyTorch installed), and it is available in the current Conda environment, do:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"ENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Current\"\nimport PythonCall","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"If the Python installation can be found on the path and it is not in a Conda environment, do:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"ENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Null\"\nimport PythonCall","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"If python is not on your path, you may additionally need to set JULIA_PYTHONCALL_EXE, for example, do:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"ENV[\"JULIA_PYTHONCALL_EXE\"] = \"python3\"\nENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Null\"\nimport PythonCall","category":"page"},{"location":"tutorials/pytorch/#Required-packages","page":"Function fitting with PyTorch","title":"Required packages","text":"","category":"section"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"This tutorial requires the following packages","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"using JuMP\nusing Test\nimport Ipopt\nimport MathOptAI\nimport Plots\nimport PythonCall","category":"page"},{"location":"tutorials/pytorch/#Training-a-model","page":"Function fitting with PyTorch","title":"Training a model","text":"","category":"section"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"The following script builds and trains a simple neural network in PyTorch. For simplicity, we do not evaluate out-of-sample test performance, or use a batched data loader. In general, you should train your model in Python, and then use torch.save(model, filename) to save it to a .pt file for later use in Julia.","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"The model is unimportant, but for this example, we are trying to fit noisy observations of the function f(x) = x^2 - 2x.","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"In Python, we ran:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"#!/usr/bin/python3\nimport torch\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 16),\n    torch.nn.ReLU(),\n    torch.nn.Linear(16, 1),\n)\n\nn = 1024\nx = torch.arange(-2, 2 + 4 / (n - 1), 4 / (n - 1)).reshape(n, 1)\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nfor epoch in range(100):\n    optimizer.zero_grad()\n    N = torch.normal(torch.zeros(n, 1), torch.ones(n, 1))\n    y = x ** 2 -2 * x + 0.1 * N\n    loss = loss_fn(model(x), y)\n    loss.backward()\n    optimizer.step()\n\ntorch.save(model, \"model.pt\")","category":"page"},{"location":"tutorials/pytorch/#JuMP-model","page":"Function fitting with PyTorch","title":"JuMP model","text":"","category":"section"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"Our goal for this JuMP model is to load the Neural Network from PyTorch into the objective function, and then minimize the objective for different fixed values of x to recreate the function that the Neural Network has learned to approximate.","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"First, create a JuMP model:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"model = Model(Ipopt.Optimizer)\nset_silent(model)\n@variable(model, x)","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"Then, load the model from PyTorch using MathOptAI.PytorchModel:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"predictor = MathOptAI.PytorchModel(joinpath(@__DIR__, \"model.pt\"))\ny, _ = MathOptAI.add_predictor(model, predictor, [x])\n@objective(model, Min, only(y))","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"Now, visualize the fitted function y = predictor(x) by repeatedly solving the optimization problem for different fixed values of x:","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"X, Y = -2:0.1:2, Float64[]\n@constraint(model, c, x == 0.0)\nfor xi in X\n    set_normalized_rhs(c, xi)\n    optimize!(model)\n    @test is_solved_and_feasible(model)\n    push!(Y, objective_value(model))\nend\nPlots.plot(x -> x^2 - 2x, X; label = \"Truth\", linestype = :dot)\nPlots.plot!(X, Y; label = \"Fitted\")","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"","category":"page"},{"location":"tutorials/pytorch/","page":"Function fitting with PyTorch","title":"Function fitting with PyTorch","text":"This page was generated using Literate.jl.","category":"page"},{"location":"manual/EvoTrees/#EvoTrees.jl","page":"EvoTrees.jl","title":"EvoTrees.jl","text":"","category":"section"},{"location":"manual/EvoTrees/","page":"EvoTrees.jl","title":"EvoTrees.jl","text":"EvoTrees.jl is a library for fitting decision trees in Julia.","category":"page"},{"location":"manual/EvoTrees/#Gradient-boosted-tree-regression","page":"EvoTrees.jl","title":"Gradient boosted tree regression","text":"","category":"section"},{"location":"manual/EvoTrees/","page":"EvoTrees.jl","title":"EvoTrees.jl","text":"Here is an example:","category":"page"},{"location":"manual/EvoTrees/","page":"EvoTrees.jl","title":"EvoTrees.jl","text":"using JuMP, MathOptAI, EvoTrees\ntruth(x::Vector) = x[1] <= 0.5 ? -2 : (x[2] <= 0.3 ? 3 : 4)\nx_train = abs.(sin.((1:10) .* (3:4)'));\nsize(x_train)\ny_train = truth.(Vector.(eachrow(x_train)));\nconfig = EvoTrees.EvoTreeRegressor(; nrounds = 3);\npredictor = EvoTrees.fit(config; x_train, y_train);\nmodel = Model();\n@variable(model, 0 <= x[1:2] <= 1);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"tutorials/decision_trees/#Classification-problems-with-DecisionTree.jl","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"","category":"section"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The purpose of this tutorial is to explain how to embed a decision tree model from DecisionTree.jl into JuMP.","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The data and example in this tutorial comes from the paper: David Bergman, Teng Huang, Philip Brooks, Andrea Lodi, Arvind U. Raghunathan (2021) JANOS: An Integrated Predictive and Prescriptive Modeling Framework. INFORMS Journal on Computing 34(2):807-816. https://doi.org/10.1287/ijoc.2020.1023","category":"page"},{"location":"tutorials/decision_trees/#Required-packages","page":"Classification problems with DecisionTree.jl","title":"Required packages","text":"","category":"section"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"This tutorial uses the following packages.","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"using JuMP\nimport CSV\nimport DataFrames\nimport Downloads\nimport DecisionTree\nimport HiGHS\nimport MathOptAI\nimport Statistics","category":"page"},{"location":"tutorials/decision_trees/#Data","page":"Classification problems with DecisionTree.jl","title":"Data","text":"","category":"section"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"Here is a function to load the data directly from the JANOS repository:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"function read_df(filename)\n    url = \"https://raw.githubusercontent.com/INFORMSJoC/2020.1023/master/data/\"\n    data = Downloads.download(url * filename)\n    return CSV.read(data, DataFrames.DataFrame)\nend","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"There are two important files. The first, college_student_enroll-s1-1.csv, contains historical admissions data on anonymized students, their SAT score, their GPA, their merit scholarships, and whether the enrolled in the college.","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"train_df = read_df(\"college_student_enroll-s1-1.csv\")","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The second, college_applications6000.csv, contains the SAT and GPA data of students who are currently applying:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"evaluate_df = read_df(\"college_applications6000.csv\")","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"There are 6,000 prospective students:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"n_students = size(evaluate_df, 1)","category":"page"},{"location":"tutorials/decision_trees/#Prediction-model","page":"Classification problems with DecisionTree.jl","title":"Prediction model","text":"","category":"section"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The first step is to train a logistic regression model to predict the Boolean enroll column based on the SAT, GPA, and merit columns.","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"train_features = Matrix(train_df[:, [:SAT, :GPA, :merit]])\ntrain_labels = train_df[:, :enroll]\npredictor = DecisionTree.DecisionTreeClassifier(; max_depth = 3)\nDecisionTree.fit!(predictor, train_features, train_labels)\nDecisionTree.print_tree(predictor)","category":"page"},{"location":"tutorials/decision_trees/#Decision-model","page":"Classification problems with DecisionTree.jl","title":"Decision model","text":"","category":"section"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"Now that we have a trained decision tree, we want a decision model that chooses the optimal merit scholarship for each student in:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"evaluate_df","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"Here's an empty JuMP model to start:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"model = Model()","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"First, we add a new column to evaluate_df, with one JuMP decision variable for each row. It is important the .merit column name in evaluate_df matches the name in train_df.","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"evaluate_df.merit = @variable(model, 0 <= x_merit[1:n_students] <= 2.5);\nevaluate_df","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"Then, we use MathOptAI.add_predictor to embed model_ml into the JuMP model. MathOptAI.add_predictor returns a vector of variables, one for each row in evaluate_df, corresponding to the output enroll of our logistic regression.","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"evaluate_features = Matrix(evaluate_df[:, [:SAT, :GPA, :merit]])\nevaluate_df.enroll = mapreduce(vcat, 1:size(evaluate_features, 1)) do i\n    y, _ = MathOptAI.add_predictor(model, predictor, evaluate_features[i, :])\n    return y\nend\nevaluate_df","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The .enroll column name in evaluate_df is just a name. It doesn't have to match the name in train_df.","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The objective of our problem is to maximize the expected number of students who enroll:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"@objective(model, Max, sum(evaluate_df.enroll))","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"Subject to the constraint that we can spend at most 0.2 * n_students on merit scholarships:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"@constraint(model, sum(evaluate_df.merit) <= 0.2 * n_students)","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"Because logistic regression involves a Sigmoid layer, we need to use a smooth nonlinear optimizer. A common choice is Ipopt. Solve and check the optimizer found a feasible solution:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"set_optimizer(model, HiGHS.Optimizer)\nset_silent(model)\noptimize!(model)\n@assert is_solved_and_feasible(model)","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"Let's store the solution in evaluate_df for analysis:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"evaluate_df.merit_sol = value.(evaluate_df.merit);\nevaluate_df.enroll_sol = value.(evaluate_df.enroll);\nevaluate_df","category":"page"},{"location":"tutorials/decision_trees/#Solution-analysis","page":"Classification problems with DecisionTree.jl","title":"Solution analysis","text":"","category":"section"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"We expect that just under 2,500 students will enroll:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"sum(evaluate_df.enroll_sol)","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"We awarded merit scholarships to approximately 1 in 6 students:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"count(evaluate_df.merit_sol .> 1e-5)","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"The average merit scholarship was worth just under $1,000:","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"1_000 * Statistics.mean(evaluate_df.merit_sol[evaluate_df.merit_sol .> 1e-5])","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"","category":"page"},{"location":"tutorials/decision_trees/","page":"Classification problems with DecisionTree.jl","title":"Classification problems with DecisionTree.jl","text":"This page was generated using Literate.jl.","category":"page"},{"location":"manual/Lux/#Lux.jl","page":"Lux.jl","title":"Lux.jl","text":"","category":"section"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"Lux.jl is a library for machine learning in Julia.","category":"page"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"The upstream documentation is available at https://lux.csail.mit.edu/stable/.","category":"page"},{"location":"manual/Lux/#Supported-layers","page":"Lux.jl","title":"Supported layers","text":"","category":"section"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"MathOptAI supports embedding a Lux model into JuMP if it is a Lux.Chain composed of:","category":"page"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"Lux.Dense\nLux.Scale\nLux.relu\nLux.sigmoid\nLux.softmax\nLux.softplus\nLux.tanh","category":"page"},{"location":"manual/Lux/#Basic-example","page":"Lux.jl","title":"Basic example","text":"","category":"section"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"Use MathOptAI.add_predictor to embed a tuple (containing the Lux.Chain, the parameters, and the state) into a JuMP model:","category":"page"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"using JuMP, Lux, MathOptAI, Random\nrng = Random.MersenneTwister();\nchain = Lux.Chain(Lux.Dense(1 => 2, Lux.relu), Lux.Dense(2 => 1))\nparameters, state = Lux.setup(rng, chain);\npredictor = (chain, parameters, state);\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"manual/Lux/#Reduced-space","page":"Lux.jl","title":"Reduced-space","text":"","category":"section"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"Use the reduced_space = true keyword to formulate a reduced-space model:","category":"page"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"using JuMP, Lux, MathOptAI, Random\nrng = Random.MersenneTwister();\nchain = Lux.Chain(Lux.Dense(1 => 2, Lux.relu), Lux.Dense(2 => 1))\nparameters, state = Lux.setup(rng, chain);\npredictor = (chain, parameters, state);\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; reduced_space = true);\ny\nformulation","category":"page"},{"location":"manual/Lux/#Gray-box","page":"Lux.jl","title":"Gray-box","text":"","category":"section"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"The Lux extension does not yet support the gray_box keyword argument.","category":"page"},{"location":"manual/Lux/#Change-how-layers-are-formulated","page":"Lux.jl","title":"Change how layers are formulated","text":"","category":"section"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"Pass a dictionary to the config keyword that maps Lux activation functions to a MathOptAI predictor:","category":"page"},{"location":"manual/Lux/","page":"Lux.jl","title":"Lux.jl","text":"using JuMP, Lux, MathOptAI, Random\nrng = Random.MersenneTwister();\nchain = Lux.Chain(Lux.Dense(1 => 2, Lux.relu), Lux.Dense(2 => 1))\nparameters, state = Lux.setup(rng, chain);\npredictor = (chain, parameters, state);\nmodel = Model();\n@variable(model, x[1:1]);\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    config = Dict(Lux.relu => MathOptAI.ReLUSOS1()),\n);\ny\nformulation","category":"page"},{"location":"tutorials/mnist_lux/#Adversarial-machine-learning-with-Lux.jl","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"","category":"section"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"The purpose of this tutorial is to explain how to embed a neural network model from Lux.jl into JuMP.","category":"page"},{"location":"tutorials/mnist_lux/#Required-packages","page":"Adversarial machine learning with Lux.jl","title":"Required packages","text":"","category":"section"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"This tutorial requires the following packages","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"using JuMP\nimport Lux\nimport Ipopt\nimport MathOptAI\nimport MLDatasets\nimport MLUtils\nimport OneHotArrays\nimport Optimisers\nimport Plots\nimport Random\nimport Zygote","category":"page"},{"location":"tutorials/mnist_lux/#Data","page":"Adversarial machine learning with Lux.jl","title":"Data","text":"","category":"section"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"This tutorial uses images from the MNIST dataset.","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"We load the predefined train and test splits:","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"train_data = MLDatasets.MNIST(; split = :train)","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"test_data = MLDatasets.MNIST(; split = :test)","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"Since the data are images, it is helpful to plot them. (This requires a transpose and reversing the rows to get the orientation correct.)","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"function plot_image(x::Matrix; kwargs...)\n    return Plots.heatmap(\n        x'[size(x, 1):-1:1, :];\n        xlims = (1, size(x, 2)),\n        ylims = (1, size(x, 1)),\n        aspect_ratio = true,\n        legend = false,\n        xaxis = false,\n        yaxis = false,\n        kwargs...,\n    )\nend\n\nfunction plot_image(instance::NamedTuple)\n    return plot_image(instance.features; title = \"Label = $(instance.targets)\")\nend\n\nPlots.plot([plot_image(train_data[i]) for i in 1:6]...; layout = (2, 3))","category":"page"},{"location":"tutorials/mnist_lux/#Training","page":"Adversarial machine learning with Lux.jl","title":"Training","text":"","category":"section"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"We use a simple neural network with one hidden layer and a sigmoid activation function. (There are better performing networks; try experimenting.)","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"chain = Lux.Chain(\n    Lux.Dense(28^2 => 32, Lux.sigmoid),\n    Lux.Dense(32 => 10),\n    Lux.softmax,\n)\nrng = Random.MersenneTwister();\nparameters, state = Lux.setup(rng, chain)\npredictor = (chain, parameters, state);\nnothing #hide","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"Here is a function to load our data into the format that predictor expects:","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"function data_loader(data; batchsize, shuffle = false)\n    x = reshape(data.features, 28^2, :)\n    y = OneHotArrays.onehotbatch(data.targets, 0:9)\n    return MLUtils.DataLoader((x, y); batchsize, shuffle)\nend","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"and here is a function to score the percentage of correct labels, where we assign a label by choosing the label of the highest softmax in the final layer.","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"function score_model(predictor, data)\n    chain, parameters, state = predictor\n    x, y = only(data_loader(data; batchsize = length(data)))\n    y_hat, _ = chain(x, parameters, state)\n    is_correct = OneHotArrays.onecold(y) .== OneHotArrays.onecold(y_hat)\n    p = round(100 * sum(is_correct) / length(is_correct); digits = 2)\n    println(\"Accuracy = $p %\")\n    return\nend","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"The accuracy of our model is only around 10% before training:","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"score_model(predictor, train_data)\nscore_model(predictor, test_data)","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"Let's improve that by training our model.","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"note: Note\nIt is not the purpose of this tutorial to explain how Lux works; see the documentation at https://lux.csail.mit.edu for more details. Changing the number of epochs or the learning rate can improve the loss.","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"begin\n    train_loader = data_loader(train_data; batchsize = 256, shuffle = true)\n    optimizer_state = Optimisers.setup(Optimisers.Adam(0.0003f0), parameters)\n    for epoch in 1:30\n        loss = 0.0\n        for (x, y) in train_loader\n            global state\n            (loss_batch, state), pullback = Zygote.pullback(parameters) do p\n                y_model, new_state = chain(x, p, state)\n                return Lux.CrossEntropyLoss()(y_model, y), new_state\n            end\n            gradients = only(pullback((one(loss), nothing)))\n            Optimisers.update!(optimizer_state, parameters, gradients)\n            loss += loss_batch\n        end\n        loss = round(loss / length(train_loader); digits = 4)\n        print(\"Epoch $epoch: loss = $loss\\t\")\n        score_model(predictor, test_data)\n    end\nend","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"Here are the first eight predictions of the test data:","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"function plot_image(predictor, x::Matrix)\n    y, _ = chain(vec(x), parameters, state)\n    score, index = findmax(y)\n    title = \"Predicted: $(index - 1) ($(round(Int, 100 * score))%)\"\n    return plot_image(x; title)\nend\n\nplots = [plot_image(predictor, test_data[i].features) for i in 1:8]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"We can also look at the best and worst four predictions:","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"x, y = only(data_loader(test_data; batchsize = length(test_data)))\ny_model, _ = chain(x, parameters, state)\nlosses = Lux.CrossEntropyLoss(; agg = identity)(y_model, y)\nindices = sortperm(losses; dims = 2)[[1:4; (end-3):end]]\nplots = [plot_image(predictor, test_data[i].features) for i in indices]\nPlots.plot(plots...; size = (1200, 600), layout = (2, 4))","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"There are still some fairly bad mistakes. Can you change the model or training parameters improve to improve things?","category":"page"},{"location":"tutorials/mnist_lux/#JuMP","page":"Adversarial machine learning with Lux.jl","title":"JuMP","text":"","category":"section"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"Now that we have a trained machine learning model, we can embed it in a JuMP model.","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"Here's a function which takes a test case and returns an example that maximizes the probability of the adversarial example.","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"function find_adversarial_image(test_case; adversary_label, δ = 0.05)\n    model = Model(Ipopt.Optimizer)\n    set_silent(model)\n    @variable(model, 0 <= x[1:28, 1:28] <= 1)\n    @constraint(model, -δ .<= x .- test_case.features .<= δ)\n    # Note: we need to use `vec` here because `x` is a 28-by-28 Matrix, but our\n    # neural network expects a 28^2 length vector.\n    y, _ = MathOptAI.add_predictor(model, predictor, vec(x))\n    @objective(model, Max, y[adversary_label+1] - y[test_case.targets+1])\n    optimize!(model)\n    @assert is_solved_and_feasible(model)\n    return value.(x)\nend","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"Let's try finding an adversarial example to the third test image. The image on the left is our input image. The network thinks this is a 1 with probability 99%. The image on the right is the adversarial image. The network thinks this is a 7, although it is less confident.","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"x_adversary = find_adversarial_image(test_data[3]; adversary_label = 7);\nPlots.plot(\n    plot_image(predictor, test_data[3].features),\n    plot_image(predictor, Float32.(x_adversary)),\n)","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"","category":"page"},{"location":"tutorials/mnist_lux/","page":"Adversarial machine learning with Lux.jl","title":"Adversarial machine learning with Lux.jl","text":"This page was generated using Literate.jl.","category":"page"},{"location":"manual/PyTorch/#PyTorch","page":"PyTorch","title":"PyTorch","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"PyTorch is a library for machine learning in Python.","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"The upstream documentation is available at https://pytorch.org/docs/stable/.","category":"page"},{"location":"manual/PyTorch/#Supported-layers","page":"PyTorch","title":"Supported layers","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"MathOptAI supports embedding a PyTorch models into JuMP if it is a nn.Sequential composed of:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"nn.Linear\nnn.ReLU\nnn.Sigmoid\nnn.Softmax\nnn.Softplus\nnn.Tanh","category":"page"},{"location":"manual/PyTorch/#File-format","page":"PyTorch","title":"File format","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"Use torch.save to save a trained PyTorch model to a .pt file:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"#!/usr/bin/python3\nimport torch\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2, 1),\n)\ntorch.save(model, \"saved_pytorch_model.pt\")","category":"page"},{"location":"manual/PyTorch/#Python-integration","page":"PyTorch","title":"Python integration","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"MathOptAI uses PythonCall.jl to call from Julia into Python.","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"To use PytorchModel your code must load the PythonCall package:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"import PythonCall","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"PythonCall uses CondaPkg.jl to manage Python dependencies. See CondaPkg.jl for more control over how to link Julia to an existing Python environment. For example, if you have an existing Python installation (with PyTorch installed), and it is available in the current Conda environment, do:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"ENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Current\"\nimport PythonCall","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"If the Python installation can be found on the path and it is not in a Conda environment, do:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"ENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Null\"\nimport PythonCall","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"If python is not on your path, you may additionally need to set JULIA_PYTHONCALL_EXE, for example, do:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"ENV[\"JULIA_PYTHONCALL_EXE\"] = \"python3\"\nENV[\"JULIA_CONDAPKG_BACKEND\"] = \"Null\"\nimport PythonCall","category":"page"},{"location":"manual/PyTorch/#Basic-example","page":"PyTorch","title":"Basic example","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"Use MathOptAI.add_predictor to embed a PyTorch model into a JuMP model:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"using JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation = MathOptAI.add_predictor(model, predictor, x);\ny\nformulation","category":"page"},{"location":"manual/PyTorch/#Reduced-space","page":"PyTorch","title":"Reduced-space","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"Use the reduced_space = true keyword to formulate a reduced-space model:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"using JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; reduced_space = true);\ny\nformulation","category":"page"},{"location":"manual/PyTorch/#Gray-box","page":"PyTorch","title":"Gray-box","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"Use the gray_box = true keyword to embed the network as a nonlinear operator:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"using JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation =\n    MathOptAI.add_predictor(model, predictor, x; gray_box = true);\ny\nformulation","category":"page"},{"location":"manual/PyTorch/#pytorch-vector-nonlinear-oracle","page":"PyTorch","title":"VectorNonlinearOracle","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"Use the vector_nonlinear_oracle = true keyword to embed the network as a vector nonlinear operator:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"using JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    vector_nonlinear_oracle = true,\n);\ny\nformulation","category":"page"},{"location":"manual/PyTorch/#Change-how-layers-are-formulated","page":"PyTorch","title":"Change how layers are formulated","text":"","category":"section"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"Pass a dictionary to the config keyword that maps the Symbol name of each PyTorch layer to a MathOptAI predictor:","category":"page"},{"location":"manual/PyTorch/","page":"PyTorch","title":"PyTorch","text":"using JuMP, MathOptAI, PythonCall\nmodel = Model();\n@variable(model, x[1:1]);\npredictor = MathOptAI.PytorchModel(\"saved_pytorch_model.pt\");\ny, formulation = MathOptAI.add_predictor(\n    model,\n    predictor,\n    x;\n    config = Dict(:ReLU => MathOptAI.ReLUSOS1()),\n);\ny\nformulation","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"(Image: )","category":"page"},{"location":"#MathOptAI.jl","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"","category":"section"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"MathOptAI.jl is a JuMP extension for embedding trained AI, machine learning, and statistical learning models into a JuMP optimization model.","category":"page"},{"location":"#License","page":"MathOptAI.jl","title":"License","text":"","category":"section"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"MathOptAI.jl is provided under a BSD-3 license as part of the Optimization and Machine Learning Toolbox project, O4806.","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"See LICENSE.md for details.","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"Despite the name similarity, this project is not affiliated with OMLT, the Optimization and Machine Learning Toolkit.","category":"page"},{"location":"#Installation","page":"MathOptAI.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"Install MathOptAI.jl using the Julia package manager:","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"import Pkg\nPkg.add(\"MathOptAI\")","category":"page"},{"location":"#Getting-started","page":"MathOptAI.jl","title":"Getting started","text":"","category":"section"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"Here's an example of using MathOptAI to embed a trained neural network from Flux into a JuMP model. The vector of JuMP variables x is fed as input to the neural network. The output y is a vector of JuMP variables that represents the output layer of the neural network. The formulation object stores the additional variables and constraints that were added to model.","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"julia> using JuMP, MathOptAI, Flux\n\njulia> predictor = Flux.Chain(\n           Flux.Dense(28^2 => 32, Flux.sigmoid),\n           Flux.Dense(32 => 10),\n           Flux.softmax,\n       );\n\njulia> #= Train the Flux model. Code not shown for simplicity =#\n\njulia> model = JuMP.Model();\n\njulia> JuMP.@variable(model, 0 <= x[1:28^2] <= 1);\n\njulia> y, formulation = MathOptAI.add_predictor(model, predictor, x);\n\njulia> y\n10-element Vector{VariableRef}:\n moai_SoftMax[1]\n moai_SoftMax[2]\n moai_SoftMax[3]\n moai_SoftMax[4]\n moai_SoftMax[5]\n moai_SoftMax[6]\n moai_SoftMax[7]\n moai_SoftMax[8]\n moai_SoftMax[9]\n moai_SoftMax[10]","category":"page"},{"location":"#Getting-help","page":"MathOptAI.jl","title":"Getting help","text":"","category":"section"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"This package is under active development. For help, questions, comments, and suggestions, please open a GitHub issue.","category":"page"},{"location":"#Inspiration","page":"MathOptAI.jl","title":"Inspiration","text":"","category":"section"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"This project is mainly inspired by two existing projects:","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"OMLT\ngurobi-machinelearning","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"Other works, from which we took less inspiration, include:","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"JANOS\nMeLOn\nENTMOOT\nreluMIP\nOptiCL\nPySCIPOpt-ML","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"The 2024 paper of López-Flores et al. is an excellent summary of the state of the field at the time that we started development of MathOptAI.","category":"page"},{"location":"","page":"MathOptAI.jl","title":"MathOptAI.jl","text":"López-Flores, F.J., Ramírez-Márquez, C., Ponce-Ortega J.M. (2024). Process Systems Engineering Tools for Optimization of Trained Machine Learning Models: Comparative and Perspective. Industrial & Engineering Chemistry Research, 63(32), 13966-13979. DOI: 10.1021/acs.iecr.4c00632","category":"page"},{"location":"tutorials/gaussian/#Function-fitting-with-AbstractGPs","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"","category":"section"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"The purpose of this tutorial is to explain how to embed a Gaussian Process from AbstractGPs into JuMP.","category":"page"},{"location":"tutorials/gaussian/#Required-packages","page":"Function fitting with AbstractGPs","title":"Required packages","text":"","category":"section"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"This tutorial requires the following packages","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"using JuMP\nusing Test\nimport AbstractGPs\nimport Ipopt\nimport MathOptAI\nimport Plots","category":"page"},{"location":"tutorials/gaussian/#Prediction-model","page":"Function fitting with AbstractGPs","title":"Prediction model","text":"","category":"section"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"Assume that we have some true underlying univariate function:","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"x_domain = 0:0.01:2π\ntrue_function(x) = sin(x)\nPlots.plot(x_domain, true_function.(x_domain); label = \"truth\")","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"We don't know the function, but we have access to a limited set of noisy sample points:","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"N = 20\nx_data = rand(x_domain, N)\nnoisy_sampler(x) = true_function(x) + 0.25 * (2rand() - 1)\ny_data = noisy_sampler.(x_data)\nPlots.scatter!(x_data, y_data; label = \"data\")","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"Using the data, we want to build a predictor y = predictor(x). One choice is a Gaussian Process:","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"fx = AbstractGPs.GP(AbstractGPs.Matern32Kernel())(x_data, 0.4)\np_fx = AbstractGPs.posterior(fx, y_data)\nPlots.plot!(x_domain, p_fx; label = \"GP\", fillalpha = 0.1)","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"Gaussian Processes fit a mean and variance function:","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"AbstractGPs.mean_and_var(p_fx, [π / 2])","category":"page"},{"location":"tutorials/gaussian/#Decision-model","page":"Function fitting with AbstractGPs","title":"Decision model","text":"","category":"section"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"Our goal for this JuMP model is to embed the Gaussian Process from AbstractGPs into the model and then solve for different fixed values of x to recreate the function that the Gaussian Process has learned to approximate.","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"First, create a JuMP model:","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"model = Model(Ipopt.Optimizer)\nset_silent(model)\n@variable(model, x)","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"Since a Gaussian Process is a infinite dimensional object (its prediction is a distribution), we need some way of converting the Gaussian Process into a finite set of scalar values. For this, we use the Quantile predictor:","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"predictor = MathOptAI.Quantile(p_fx, [0.25, 0.75]);\ny, _ = MathOptAI.add_predictor(model, predictor, [x])","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"Now, visualize the fitted function y = predictor(x) by repeatedly solving the optimization problem for different fixed values of x. Each value of y has two elements: one for the 25th percentile and one for the 75th.","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"X, Y = range(0, 2π; length = 20), Any[]\n@constraint(model, c, x == 0.0)\nfor xi in X\n    set_normalized_rhs(c, xi)\n    optimize!(model)\n    if is_solved_and_feasible(model)\n        push!(Y, value.(y))\n    else\n        push!(Y, [NaN, NaN])\n    end\nend\nPlots.plot!(X, reduce(hcat, Y)'; label = [\"P25\" \"P75\"], linewidth = 3)","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"","category":"page"},{"location":"tutorials/gaussian/","page":"Function fitting with AbstractGPs","title":"Function fitting with AbstractGPs","text":"This page was generated using Literate.jl.","category":"page"}]
}
